{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPulZkM2wy1G6U/ysNFwF/D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kodai-utsunomiya/memorization-and-generalization/blob/main/Disentangling_feature_and_lazy_training_in_deep_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# アーキテクチャ"
      ],
      "metadata": {
        "id": "wYZDDacUMJwF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EiVUBkeAKWyg"
      },
      "outputs": [],
      "source": [
        "# pylint: disable=E1101, C, arguments-differ\n",
        "\"\"\"\n",
        "Defines three architectures:\n",
        "- Fully connecetd `FC`\n",
        "- Convolutional `CV`\n",
        "- And a resnet `Wide_ResNet`\n",
        "\"\"\"\n",
        "import functools\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class FC(nn.Module):\n",
        "    def __init__(self, d, h, L, act, bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        hh = d\n",
        "        for i in range(L):\n",
        "            W = torch.randn(h, hh)\n",
        "\n",
        "            # next two line are here to avoid memory issue when computing the kernel\n",
        "            n = max(1, 128 * 256 // hh)\n",
        "            W = nn.ParameterList([nn.Parameter(W[j: j+n]) for j in range(0, len(W), n)])\n",
        "\n",
        "            setattr(self, \"W{}\".format(i), W)\n",
        "            if bias:\n",
        "                self.register_parameter(\"B{}\".format(i), nn.Parameter(torch.zeros(h)))\n",
        "            hh = h\n",
        "\n",
        "        self.register_parameter(\"W{}\".format(L), nn.Parameter(torch.randn(1, hh)))\n",
        "        if bias:\n",
        "            self.register_parameter(\"B{}\".format(L), nn.Parameter(torch.zeros(1)))\n",
        "\n",
        "        self.L = L\n",
        "        self.act = act\n",
        "        self.bias = bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self.L + 1):\n",
        "            W = getattr(self, \"W{}\".format(i))\n",
        "\n",
        "            if isinstance(W, nn.ParameterList):\n",
        "                W = torch.cat(list(W))\n",
        "\n",
        "            if self.bias:\n",
        "                B = self.bias * getattr(self, \"B{}\".format(i))\n",
        "            else:\n",
        "                B = 0\n",
        "\n",
        "            h = x.size(1)\n",
        "\n",
        "            if i < self.L:\n",
        "                x = x @ (W.t() / h ** 0.5)\n",
        "                x = self.act(x + B)\n",
        "            else:\n",
        "                x = x @ (W.t() / h) + B\n",
        "\n",
        "        return x.view(-1)\n",
        "\n",
        "\n",
        "class CV(nn.Module):\n",
        "    def __init__(self, d, h, L1, L2, act, h_base, fsz, pad, stride_first):\n",
        "        super().__init__()\n",
        "\n",
        "        h1 = d\n",
        "        for i in range(L1):\n",
        "            h2 = round(h * h_base ** i)\n",
        "            for j in range(L2):\n",
        "                W = nn.ParameterList([nn.Parameter(torch.randn(h1, fsz, fsz)) for _ in range(h2)])\n",
        "                setattr(self, \"W{}_{}\".format(i, j), W)\n",
        "                h1 = h2\n",
        "\n",
        "        self.W = nn.Parameter(torch.randn(h1))\n",
        "\n",
        "        self.L1 = L1\n",
        "        self.L2 = L2\n",
        "        self.act = act\n",
        "        self.pad = pad\n",
        "        self.stride_first = stride_first\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self.L1):\n",
        "            for j in range(self.L2):\n",
        "                assert x.size(2) >= 5 and x.size(3) >= 5\n",
        "                W = getattr(self, \"W{}_{}\".format(i, j))\n",
        "                W = torch.stack(list(W))\n",
        "\n",
        "                stride = 2 if j == 0 and (i > 0 or self.stride_first) else 1\n",
        "                h = W[0].numel()\n",
        "                x = nn.functional.conv2d(x, W / h ** 0.5, None, stride=stride, padding=self.pad)\n",
        "                x = self.act(x)\n",
        "\n",
        "        x = x.flatten(2).mean(2)\n",
        "\n",
        "        W = self.W\n",
        "        h = len(W)\n",
        "        x = x @ (W / h)\n",
        "        return x.view(-1)\n",
        "\n",
        "\n",
        "class conv(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, bias=True):\n",
        "        super().__init__()\n",
        "\n",
        "        w = torch.randn(out_planes, in_planes, kernel_size, kernel_size)\n",
        "        n = max(1, 256**2 // w[0].numel())\n",
        "        self.w = nn.ParameterList([nn.Parameter(w[j: j + n]) for j in range(0, len(w), n)])\n",
        "\n",
        "        self.b = nn.Parameter(torch.zeros(out_planes)) if bias else None\n",
        "\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "    def forward(self, x):\n",
        "        w = torch.cat(list(self.w))\n",
        "        h = w[0].numel()\n",
        "        return F.conv2d(x, w / h ** 0.5, self.b, self.stride, self.padding)\n",
        "\n",
        "class wide_basic(nn.Module):\n",
        "    def __init__(self, in_planes, planes, act, stride=1, mix_angle=45):\n",
        "        super().__init__()\n",
        "        self.conv1 = conv(in_planes, planes, kernel_size=3, padding=1, bias=True)\n",
        "        self.conv2 = conv(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = conv(in_planes, planes, kernel_size=1, stride=stride, bias=True)\n",
        "\n",
        "        self.act = act\n",
        "        self.mix_angle = mix_angle\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(self.act(x))\n",
        "        out = self.conv2(self.act(out))\n",
        "        cut = self.shortcut(x)\n",
        "\n",
        "        a = self.mix_angle * math.pi / 180\n",
        "        out = math.cos(a) * cut + math.sin(a) * out\n",
        "\n",
        "        return out\n",
        "\n",
        "class Wide_ResNet(nn.Module):\n",
        "    def __init__(self, d, depth, h, act, num_classes, mix_angle=45):\n",
        "        super().__init__()\n",
        "\n",
        "        assert (depth % 6 == 4), 'Wide-resnet depth should be 6n+4'\n",
        "        n = (depth - 4) // 6\n",
        "\n",
        "        nStages = [16, 16 * h, 32 * h, 64 * h]\n",
        "        block = functools.partial(wide_basic, act=act, mix_angle=mix_angle)\n",
        "\n",
        "        self.conv1 = conv(d, nStages[0], kernel_size=3, stride=1, padding=1, bias=True)\n",
        "        self.in_planes = nStages[0]\n",
        "\n",
        "        self.layer1 = self._wide_layer(block, nStages[1], n, stride=1)\n",
        "        self.layer2 = self._wide_layer(block, nStages[2], n, stride=2)\n",
        "        self.layer3 = self._wide_layer(block, nStages[3], n, stride=2)\n",
        "        self.linear = nn.Parameter(torch.randn(num_classes, nStages[3]))\n",
        "        self.bias = nn.Parameter(torch.zeros(num_classes))\n",
        "        self.act = act\n",
        "\n",
        "    def _wide_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride=stride))\n",
        "            self.in_planes = planes\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.act(out)\n",
        "        out = out.flatten(2).mean(2)\n",
        "\n",
        "        h = self.linear.size(1)\n",
        "        out = F.linear(out, self.linear / h, self.bias)\n",
        "\n",
        "        if out.size(1) == 1:\n",
        "            out = out.flatten(0)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# データセット"
      ],
      "metadata": {
        "id": "6JdOHFRoMRGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pylint: disable=no-member, E1102, C\n",
        "\"\"\"\n",
        "- Load mnist or cifar10\n",
        "- perform PCA\n",
        "- shuffle the dataset\n",
        "- split in train and test set in an equilibrated way (same amount of each classes)\n",
        "\"\"\"\n",
        "import functools\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "def pca(x, d, whitening):\n",
        "    '''\n",
        "    :param x: [P, ...]\n",
        "    :return: [P, d]\n",
        "    '''\n",
        "\n",
        "    z = x.flatten(1)\n",
        "    mu = z.mean(0)\n",
        "    cov = (z - mu).t() @ (z - mu) / len(z)\n",
        "\n",
        "    val, vec = cov.symeig(eigenvectors=True)\n",
        "    val, idx = val.sort(descending=True)\n",
        "    vec = vec[:, idx]\n",
        "\n",
        "    u = (z - mu) @ vec[:, :d]\n",
        "    if whitening:\n",
        "        u.mul_(val[:d].rsqrt())\n",
        "    else:\n",
        "        u.mul_(val[:d].mean().rsqrt())\n",
        "\n",
        "    return u\n",
        "\n",
        "\n",
        "def get_binary_pca_dataset(dataset, p, d, whitening, seed=None, device=None):\n",
        "    if seed is None:\n",
        "        seed = torch.randint(2 ** 32, (), dtype=torch.long).item()\n",
        "\n",
        "    x, y = get_normalized_dataset(dataset, seed)\n",
        "\n",
        "    x = pca(x, d, whitening).to(device)\n",
        "    y = (2 * (torch.arange(len(y)) % 2) - 1).type(x.dtype).to(device)\n",
        "\n",
        "    xtr = x[:p]\n",
        "    xte = x[p:]\n",
        "    ytr = y[:p]\n",
        "    yte = y[p:]\n",
        "\n",
        "    return (xtr, ytr), (xte, yte)\n",
        "\n",
        "\n",
        "def get_dataset(dataset, p, seed=None, device=None):\n",
        "    if seed is None:\n",
        "        seed = torch.randint(2 ** 32, (), dtype=torch.long).item()\n",
        "\n",
        "    x, y = get_normalized_dataset(dataset, seed)\n",
        "\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    xtr = x[:p]\n",
        "    xte = x[p:]\n",
        "    ytr = y[:p]\n",
        "    yte = y[p:]\n",
        "\n",
        "    return (xtr, ytr), (xte, yte)\n",
        "\n",
        "\n",
        "def get_binary_dataset(dataset, p, seed=None, device=None):\n",
        "    if seed is None:\n",
        "        seed = torch.randint(2 ** 32, (), dtype=torch.long).item()\n",
        "\n",
        "    x, y = get_normalized_dataset(dataset, seed)\n",
        "\n",
        "    x = x.to(device)\n",
        "    y = (2 * (torch.arange(len(y)) % 2) - 1).type(x.dtype).to(device)\n",
        "\n",
        "    xtr = x[:p]\n",
        "    xte = x[p:]\n",
        "    ytr = y[:p]\n",
        "    yte = y[p:]\n",
        "\n",
        "    return (xtr, ytr), (xte, yte)\n",
        "\n",
        "\n",
        "@functools.lru_cache(maxsize=2)\n",
        "def get_normalized_dataset(dataset, seed):\n",
        "    import torchvision\n",
        "    from itertools import chain\n",
        "\n",
        "    transform = torchvision.transforms.ToTensor()\n",
        "\n",
        "    if dataset == \"mnist\":\n",
        "        tr = torchvision.datasets.MNIST('~/.torchvision/datasets/MNIST', train=True, download=True, transform=transform)\n",
        "        te = torchvision.datasets.MNIST('~/.torchvision/datasets/MNIST', train=False, transform=transform)\n",
        "    elif dataset == \"kmnist\":\n",
        "        tr = torchvision.datasets.KMNIST('~/.torchvision/datasets/KMNIST', train=True, download=True, transform=transform)\n",
        "        te = torchvision.datasets.KMNIST('~/.torchvision/datasets/KMNIST', train=False, transform=transform)\n",
        "    elif dataset == \"emnist-letters\":\n",
        "        tr = torchvision.datasets.EMNIST('~/.torchvision/datasets/EMNIST', train=True, download=True, transform=transform, split='letters')\n",
        "        te = torchvision.datasets.EMNIST('~/.torchvision/datasets/EMNIST', train=False, transform=transform, split='letters')\n",
        "    elif dataset == \"fashion\":\n",
        "        tr = torchvision.datasets.FashionMNIST('~/.torchvision/datasets/FashionMNIST', train=True, download=True, transform=transform)\n",
        "        te = torchvision.datasets.FashionMNIST('~/.torchvision/datasets/FashionMNIST', train=False, transform=transform)\n",
        "    elif dataset == \"cifar10\":\n",
        "        tr = torchvision.datasets.CIFAR10('~/.torchvision/datasets/CIFAR10', train=True, download=True, transform=transform)\n",
        "        te = torchvision.datasets.CIFAR10('~/.torchvision/datasets/CIFAR10', train=False, transform=transform)\n",
        "    elif dataset == \"cifar_catdog\":\n",
        "        tr = [(x, y) for x, y in torchvision.datasets.CIFAR10('~/.torchvision/datasets/CIFAR10', train=True, download=True, transform=transform) if y in [3, 5]]\n",
        "        te = [(x, y) for x, y in torchvision.datasets.CIFAR10('~/.torchvision/datasets/CIFAR10', train=False, transform=transform) if y in [3, 5]]\n",
        "    elif dataset == \"cifar_shipbird\":\n",
        "        tr = [(x, y) for x, y in torchvision.datasets.CIFAR10('~/.torchvision/datasets/CIFAR10', train=True, download=True, transform=transform) if y in [8, 2]]\n",
        "        te = [(x, y) for x, y in torchvision.datasets.CIFAR10('~/.torchvision/datasets/CIFAR10', train=False, transform=transform) if y in [8, 2]]\n",
        "    elif dataset == \"cifar_catplane\":\n",
        "        tr = [(x, y) for x, y in torchvision.datasets.CIFAR10('~/.torchvision/datasets/CIFAR10', train=True, download=True, transform=transform) if y in [3, 0]]\n",
        "        te = [(x, y) for x, y in torchvision.datasets.CIFAR10('~/.torchvision/datasets/CIFAR10', train=False, transform=transform) if y in [3, 0]]\n",
        "    else:\n",
        "        raise ValueError(\"unknown dataset\")\n",
        "\n",
        "    dataset = list(tr) + list(te)\n",
        "    dataset = [(x.type(torch.float64), int(y)) for x, y in dataset]\n",
        "    classes = sorted({y for x, y in dataset})\n",
        "\n",
        "    sets = [[(x, y) for x, y in dataset if y == i] for i in classes]\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    sets = [\n",
        "        [x[i] for i in torch.randperm(len(x))]\n",
        "        for x in sets\n",
        "    ]\n",
        "\n",
        "    dataset = list(chain(*zip(*sets)))\n",
        "\n",
        "    x = torch.stack([x for x, y in dataset])\n",
        "    x = x - x.mean(0)\n",
        "    x = (x[0].numel() ** 0.5) * x / x.flatten(1).norm(dim=1).view(-1, *(1,) * (x.dim() - 1))\n",
        "\n",
        "    y = torch.tensor([y for x, y in dataset], dtype=torch.long)\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "-QmQsiLDMPrT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ダイナミクス"
      ],
      "metadata": {
        "id": "nl2nqKpzfKVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pylint: disable=E1101, C\n",
        "\"\"\"\n",
        "This file implements a continuous version of momentum SGD\n",
        "Dynamics that compares the angle of the gradient between steps and keep it small\n",
        "\n",
        "- stop when margins are reached\n",
        "\n",
        "It contains two implementation of the same dynamics:\n",
        "1. `train_regular` for any kind of models\n",
        "2. `train_kernel` only for linear models\n",
        "\"\"\"\n",
        "import copy\n",
        "import itertools\n",
        "import math\n",
        "from time import perf_counter\n",
        "\n",
        "import torch\n",
        "\n",
        "# from hessian import gradient\n",
        "\n",
        "#####################################################################################################\n",
        "def gradient(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False):\n",
        "    r'''\n",
        "    Compute the gradient of `outputs` with respect to `inputs`\n",
        "    ```\n",
        "    gradient(x.sum(), x)\n",
        "    gradient((x * y).sum(), [x, y])\n",
        "    ```\n",
        "    '''\n",
        "    if torch.is_tensor(inputs):\n",
        "        inputs = [inputs]\n",
        "    else:\n",
        "        inputs = list(inputs)\n",
        "    grads = torch.autograd.grad(outputs, inputs, grad_outputs,\n",
        "                                allow_unused=True,\n",
        "                                retain_graph=retain_graph,\n",
        "                                create_graph=create_graph)\n",
        "    grads = [x if x is not None else torch.zeros_like(y) for x, y in zip(grads, inputs)]\n",
        "    return torch.cat([x.contiguous().view(-1) for x in grads])\n",
        "#####################################################################################################\n",
        "\n",
        "\n",
        "def loglinspace(rate, step, end=None):\n",
        "    t = 0\n",
        "    while end is None or t <= end:\n",
        "        yield t\n",
        "        t = int(t + 1 + step * (1 - math.exp(-t * rate / step)))\n",
        "\n",
        "\n",
        "class ContinuousMomentum(torch.optim.Optimizer):\n",
        "    r\"\"\"Implements a continuous version of momentum.\n",
        "\n",
        "    d/dt velocity = -1/tau (velocity + grad)\n",
        "     or\n",
        "    d/dt velocity = -mu/t (velocity + grad)\n",
        "\n",
        "    d/dt parameters = velocity\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, dt, tau):\n",
        "        defaults = dict(dt=dt, tau=tau)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable): A closure that reevaluates the model and\n",
        "                returns the loss. Optional for most optimizers.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            tau = group['tau']\n",
        "            dt = group['dt']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                param_state = self.state[p]\n",
        "                if 't' not in param_state:\n",
        "                    t = param_state['t'] = 0\n",
        "                else:\n",
        "                    t = param_state['t']\n",
        "\n",
        "                if tau != 0:\n",
        "                    if 'velocity' not in param_state:\n",
        "                        v = param_state['velocity'] = torch.zeros_like(p.data)\n",
        "                    else:\n",
        "                        v = param_state['velocity']\n",
        "\n",
        "                if tau > 0:\n",
        "                    x = math.exp(-dt / tau)\n",
        "                    v.mul_(x).add_(-(1 - x), p.grad.data)\n",
        "                elif tau < 0:\n",
        "                    mu = -tau\n",
        "                    x = (t / (t + dt)) ** mu\n",
        "                    v.mul_(x).add_(-(1 - x), p.grad.data)\n",
        "                else:\n",
        "                    v = -p.grad.data\n",
        "\n",
        "                p.data.add_(dt, v)\n",
        "                param_state['t'] += dt\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "def make_step(f, optimizer, dt, grad):\n",
        "    i = 0\n",
        "    for p in f.parameters():\n",
        "        n = p.numel()\n",
        "        p.grad = grad[i: i + n].view_as(p)\n",
        "        i += n\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['dt'] = dt\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    for p in f.parameters():\n",
        "        p.grad = None\n",
        "\n",
        "\n",
        "def train_regular(f0, x, y, tau, max_walltime, alpha, loss, subf0, max_dgrad=math.inf, max_dout=math.inf):\n",
        "    f = copy.deepcopy(f0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out0 = f0(x) if subf0 else 0\n",
        "\n",
        "    dt = 1\n",
        "    step_change_dt = 0\n",
        "    optimizer = ContinuousMomentum(f.parameters(), dt=dt, tau=tau)\n",
        "\n",
        "    checkpoint_generator = loglinspace(0.01, 100)\n",
        "    checkpoint = next(checkpoint_generator)\n",
        "    wall = perf_counter()\n",
        "    t = 0\n",
        "    converged = False\n",
        "\n",
        "    out = f(x)\n",
        "    grad = gradient(loss((out - out0) * y).mean(), f.parameters())\n",
        "\n",
        "    for step in itertools.count():\n",
        "\n",
        "        state = copy.deepcopy((f.state_dict(), optimizer.state_dict(), t))\n",
        "\n",
        "        while True:\n",
        "            make_step(f, optimizer, dt, grad)\n",
        "            t += dt\n",
        "            current_dt = dt\n",
        "\n",
        "            new_out = f(x)\n",
        "            new_grad = gradient(loss((new_out - out0) * y).mean(), f.parameters())\n",
        "\n",
        "            dout = (out - new_out).mul(alpha).abs().max().item()\n",
        "            if grad.norm() == 0 or new_grad.norm() == 0:\n",
        "                dgrad = 0\n",
        "            else:\n",
        "                dgrad = (grad - new_grad).norm().pow(2).div(grad.norm() * new_grad.norm()).item()\n",
        "\n",
        "            if dgrad < max_dgrad and dout < max_dout:\n",
        "                if dgrad < 0.5 * max_dgrad and dout < 0.5 * max_dout:\n",
        "                    dt *= 1.1\n",
        "                break\n",
        "\n",
        "            dt /= 10\n",
        "\n",
        "            print(\"[{} +{}] [dt={:.1e} dgrad={:.1e} dout={:.1e}]\".format(step, step - step_change_dt, dt, dgrad, dout), flush=True)\n",
        "            step_change_dt = step\n",
        "            f.load_state_dict(state[0])\n",
        "            optimizer.load_state_dict(state[1])\n",
        "            t = state[2]\n",
        "\n",
        "        out = new_out\n",
        "        grad = new_grad\n",
        "\n",
        "        save = False\n",
        "\n",
        "        if step == checkpoint:\n",
        "            checkpoint = next(checkpoint_generator)\n",
        "            assert checkpoint > step\n",
        "            save = True\n",
        "\n",
        "        if (alpha * (out - out0) * y >= 1).all() and not converged:\n",
        "            converged = True\n",
        "            save = True\n",
        "\n",
        "        if save:\n",
        "            state = {\n",
        "                'step': step,\n",
        "                'wall': perf_counter() - wall,\n",
        "                't': t,\n",
        "                'dt': current_dt,\n",
        "                'dgrad': dgrad,\n",
        "                'dout': dout,\n",
        "                'norm': sum(p.norm().pow(2) for p in f.parameters()).sqrt().item(),\n",
        "                'dnorm': sum((p0 - p).norm().pow(2) for p0, p in zip(f0.parameters(), f.parameters())).sqrt().item(),\n",
        "                'grad_norm': grad.norm().item(),\n",
        "            }\n",
        "\n",
        "            yield f, state, converged\n",
        "\n",
        "        if converged:\n",
        "            break\n",
        "\n",
        "        if perf_counter() > wall + max_walltime:\n",
        "            break\n",
        "\n",
        "        if torch.isnan(out).any():\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "def train_kernel(ktrtr, ytr, tau, max_walltime, alpha, loss_prim, max_dgrad=math.inf, max_dout=math.inf):\n",
        "    otr = ktrtr.new_zeros(len(ytr))\n",
        "    velo = otr.clone()\n",
        "\n",
        "    dt = 1\n",
        "    step_change_dt = 0\n",
        "\n",
        "    checkpoint_generator = loglinspace(0.01, 100)\n",
        "    checkpoint = next(checkpoint_generator)\n",
        "    wall = perf_counter()\n",
        "    t = 0\n",
        "    converged = False\n",
        "\n",
        "    lprim = loss_prim(otr * ytr) * ytr\n",
        "    grad = ktrtr @ lprim / len(ytr)\n",
        "\n",
        "    for step in itertools.count():\n",
        "\n",
        "        state = copy.deepcopy((otr, velo, t))\n",
        "\n",
        "        while True:\n",
        "\n",
        "            if tau > 0:\n",
        "                x = math.exp(-dt / tau)\n",
        "                velo.mul_(x).add_(-(1 - x), grad)\n",
        "            elif tau < 0:\n",
        "                mu = -tau\n",
        "                x = (t / (t + dt)) ** mu\n",
        "                velo.mul_(x).add_(-(1 - x), grad)\n",
        "            else:\n",
        "                velo.copy_(-grad)\n",
        "            otr.add_(dt, velo)\n",
        "\n",
        "            t += dt\n",
        "            current_dt = dt\n",
        "\n",
        "            lprim = loss_prim(otr * ytr) * ytr\n",
        "            new_grad = ktrtr @ lprim / len(ytr)\n",
        "\n",
        "            dout = velo.mul(dt * alpha).abs().max().item()\n",
        "            if grad.norm() == 0 or new_grad.norm() == 0:\n",
        "                dgrad = 0\n",
        "            else:\n",
        "                dgrad = (grad - new_grad).norm().pow(2).div(grad.norm() * new_grad.norm()).item()\n",
        "\n",
        "            if dgrad < max_dgrad and dout < max_dout:\n",
        "                if dgrad < 0.1 * max_dgrad and dout < 0.1 * max_dout:\n",
        "                    dt *= 1.1\n",
        "                break\n",
        "\n",
        "            dt /= 10\n",
        "\n",
        "            print(\"[{} +{}] [dt={:.1e} dgrad={:.1e} dout={:.1e}]\".format(step, step - step_change_dt, dt, dgrad, dout), flush=True)\n",
        "            step_change_dt = step\n",
        "            otr.copy_(state[0])\n",
        "            velo.copy_(state[1])\n",
        "            t = state[2]\n",
        "\n",
        "        grad = new_grad\n",
        "\n",
        "        save = False\n",
        "\n",
        "        if step == checkpoint:\n",
        "            checkpoint = next(checkpoint_generator)\n",
        "            assert checkpoint > step\n",
        "            save = True\n",
        "\n",
        "        if (alpha * otr * ytr >= 1).all() and not converged:\n",
        "            converged = True\n",
        "            save = True\n",
        "\n",
        "        if save:\n",
        "            state = {\n",
        "                'step': step,\n",
        "                'wall': perf_counter() - wall,\n",
        "                't': t,\n",
        "                'dt': current_dt,\n",
        "                'dgrad': dgrad,\n",
        "                'dout': dout,\n",
        "                'grad_norm': grad.norm().item(),\n",
        "            }\n",
        "\n",
        "            yield otr, velo, grad, state, converged\n",
        "\n",
        "        if converged:\n",
        "            break\n",
        "\n",
        "        if perf_counter() > wall + max_walltime:\n",
        "            break\n",
        "\n",
        "        if torch.isnan(otr).any():\n",
        "            break"
      ],
      "metadata": {
        "id": "fQjP5tVTMbPZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# カーネル"
      ],
      "metadata": {
        "id": "uyPICO2aM5cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pylint: disable=no-member, C, not-callable\n",
        "\"\"\"\n",
        "Computes the Gram matrix of a given model\n",
        "\"\"\"\n",
        "\n",
        "def compute_kernels(f, xtr, xte):\n",
        "    # from hessian import gradient\n",
        "\n",
        "    ktrtr = xtr.new_zeros(len(xtr), len(xtr))\n",
        "    ktetr = xtr.new_zeros(len(xte), len(xtr))\n",
        "    ktete = xtr.new_zeros(len(xte), len(xte))\n",
        "\n",
        "    params = []\n",
        "    current = []\n",
        "    for p in sorted(f.parameters(), key=lambda p: p.numel(), reverse=True):\n",
        "        current.append(p)\n",
        "        if sum(p.numel() for p in current) > 2e9 // (8 * (len(xtr) + len(xte))):\n",
        "            if len(current) > 1:\n",
        "                params.append(current[:-1])\n",
        "                current = current[-1:]\n",
        "            else:\n",
        "                params.append(current)\n",
        "                current = []\n",
        "    if len(current) > 0:\n",
        "        params.append(current)\n",
        "\n",
        "    for i, p in enumerate(params):\n",
        "        print(\"[{}/{}] [len={} numel={}]\".format(i, len(params), len(p), sum(x.numel() for x in p)), flush=True)\n",
        "\n",
        "        jtr = xtr.new_empty(len(xtr), sum(u.numel() for u in p))  # (P, N~)\n",
        "        jte = xte.new_empty(len(xte), sum(u.numel() for u in p))  # (P, N~)\n",
        "\n",
        "        for j, x in enumerate(xtr):\n",
        "            jtr[j] = gradient(f(x[None]), p)  # (N~)\n",
        "\n",
        "        for j, x in enumerate(xte):\n",
        "            jte[j] = gradient(f(x[None]), p)  # (N~)\n",
        "\n",
        "        ktrtr.add_(jtr @ jtr.t())\n",
        "        ktetr.add_(jte @ jtr.t())\n",
        "        ktete.add_(jte @ jte.t())\n",
        "        del jtr, jte\n",
        "\n",
        "    return ktrtr, ktetr, ktete"
      ],
      "metadata": {
        "id": "z7lXqLT0M4UU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# メイン（GPUに設定していることを前提）"
      ],
      "metadata": {
        "id": "IOA6y6xyfb5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pylint: disable=C, R, bare-except, arguments-differ, no-member, undefined-loop-variable\n",
        "import argparse\n",
        "import math\n",
        "import os\n",
        "import subprocess\n",
        "from functools import partial\n",
        "from time import perf_counter\n",
        "\n",
        "import torch\n",
        "\n",
        "# from archi import CV, FC, Wide_ResNet\n",
        "# from dataset import get_binary_dataset, get_binary_pca_dataset\n",
        "# from dynamics import train_kernel, train_regular\n",
        "# from kernels import compute_kernels\n",
        "\n",
        "\n",
        "def loss_func(args, fy):\n",
        "    if args.loss == 'softhinge':\n",
        "        sp = partial(torch.nn.functional.softplus, beta=args.lossbeta)\n",
        "        return sp(1 - args.alpha * fy) / args.alpha\n",
        "    if args.loss == 'qhinge':\n",
        "        return 0.5 * (1 - args.alpha * fy).relu().pow(2) / args.alpha\n",
        "\n",
        "\n",
        "def loss_func_prime(args, fy):\n",
        "    if args.loss == 'softhinge':\n",
        "        return -torch.sigmoid(args.lossbeta * (1 - args.alpha * fy)).mul(args.lossbeta)\n",
        "    if args.loss == 'qhinge':\n",
        "        return -(1 - args.alpha * fy).relu()\n",
        "\n",
        "\n",
        "class SplitEval(torch.nn.Module):\n",
        "    def __init__(self, f, size):\n",
        "        super().__init__()\n",
        "        self.f = f\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([self.f(x[i: i + self.size]) for i in range(0, len(x), self.size)])\n",
        "\n",
        "\n",
        "def run_kernel(args, ktrtr, ktetr, ktete, f, xtr, ytr, xte, yte):\n",
        "    assert args.f0 == 1\n",
        "\n",
        "    dynamics = []\n",
        "\n",
        "    tau = args.tau_over_h * args.h\n",
        "    if args.tau_alpha_crit is not None:\n",
        "        tau *= min(1, args.tau_alpha_crit / args.alpha)\n",
        "\n",
        "    for otr, _velo, _grad, state, _converged in train_kernel(ktrtr, ytr, tau, args.train_time, args.alpha, partial(loss_func_prime, args), args.max_dgrad, args.max_dout):\n",
        "        state['train'] = {\n",
        "            'loss': loss_func(args, otr * ytr).mean().item(),\n",
        "            'aloss': args.alpha * loss_func(args, otr * ytr).mean().item(),\n",
        "            'err': (otr * ytr <= 0).double().mean().item(),\n",
        "            'nd': (args.alpha * otr * ytr < 1).long().sum().item(),\n",
        "            'dfnorm': otr.pow(2).mean().sqrt(),\n",
        "            'outputs': otr if args.save_outputs else None,\n",
        "            'labels': ytr if args.save_outputs else None,\n",
        "        }\n",
        "\n",
        "        print(\"[i={d[step]:d} t={d[t]:.2e} wall={d[wall]:.0f}] [dt={d[dt]:.1e} dgrad={d[dgrad]:.1e} dout={d[dout]:.1e}] [train aL={d[train][aloss]:.2e} err={d[train][err]:.2f} nd={d[train][nd]}]\".format(d=state), flush=True)\n",
        "        dynamics.append(state)\n",
        "\n",
        "    c = torch.lstsq(otr.view(-1, 1), ktrtr).solution.flatten()\n",
        "\n",
        "    if len(xte) > len(xtr):\n",
        "        # from hessian import gradient\n",
        "        a = gradient(f(xtr) @ c, f.parameters())\n",
        "        ote = torch.stack([gradient(f(x[None]), f.parameters()) @ a for x in xte])\n",
        "    else:\n",
        "        ote = ktetr @ c\n",
        "\n",
        "    out = {\n",
        "        'dynamics': dynamics,\n",
        "        'train': {\n",
        "            'outputs': otr,\n",
        "            'labels': ytr,\n",
        "        },\n",
        "        'test': {\n",
        "            'outputs': ote,\n",
        "            'labels': yte,\n",
        "        },\n",
        "        'kernel': {\n",
        "            'train': {\n",
        "                'value': ktrtr.cpu() if args.store_kernel == 1 else None,\n",
        "                'diag': ktrtr.diag(),\n",
        "                'mean': ktrtr.mean(),\n",
        "                'std': ktrtr.std(),\n",
        "                'norm': ktrtr.norm(),\n",
        "            },\n",
        "            'test': {\n",
        "                'value': ktete.cpu() if args.store_kernel == 1 else None,\n",
        "                'diag': ktete.diag(),\n",
        "                'mean': ktete.mean(),\n",
        "                'std': ktete.std(),\n",
        "                'norm': ktete.norm(),\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def run_regular(args, f0, xtr, ytr, xte, yte):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        otr0 = f0(xtr)\n",
        "        ote0 = f0(xte)\n",
        "\n",
        "    if args.f0 == 0:\n",
        "        otr0 = torch.zeros_like(otr0)\n",
        "        ote0 = torch.zeros_like(ote0)\n",
        "\n",
        "    j = torch.randperm(min(len(xte), len(xtr)))[:10 * args.chunk]\n",
        "    ytrj = ytr[j]\n",
        "    ytej = yte[j]\n",
        "\n",
        "    t = perf_counter()\n",
        "\n",
        "    tau = args.tau_over_h * args.h\n",
        "    if args.tau_alpha_crit is not None:\n",
        "        tau *= min(1, args.tau_alpha_crit / args.alpha)\n",
        "\n",
        "    dynamics = []\n",
        "    for f, state, done in train_regular(f0, xtr, ytr, tau, args.train_time, args.alpha, partial(loss_func, args), bool(args.f0), args.max_dgrad, args.max_dout):\n",
        "        with torch.no_grad():\n",
        "            otr = f(xtr[j]) - otr0[j]\n",
        "            ote = f(xte[j]) - ote0[j]\n",
        "\n",
        "        if args.arch.split('_')[0] == 'fc':\n",
        "            def getw(f, i):\n",
        "                return torch.cat(list(getattr(f.f, \"W{}\".format(i))))\n",
        "            state['wnorm'] = [getw(f, i).norm().item() for i in range(f.f.L + 1)]\n",
        "            state['dwnorm'] = [(getw(f, i) - getw(f0, i)).norm().item() for i in range(f.f.L + 1)]\n",
        "\n",
        "        state['train'] = {\n",
        "            'loss': loss_func(args, otr * ytrj).mean().item(),\n",
        "            'aloss': args.alpha * loss_func(args, otr * ytrj).mean().item(),\n",
        "            'err': (otr * ytr[j] <= 0).double().mean().item(),\n",
        "            'nd': (args.alpha * otr * ytr[j] < 1).long().sum().item(),\n",
        "            'dfnorm': otr.pow(2).mean().sqrt(),\n",
        "            'fnorm': (otr + otr0[j]).pow(2).mean().sqrt(),\n",
        "            'outputs': otr if args.save_outputs else None,\n",
        "            'labels': ytrj if args.save_outputs else None,\n",
        "        }\n",
        "        state['test'] = {\n",
        "            'loss': loss_func(args, ote * ytej).mean().item(),\n",
        "            'aloss': args.alpha * loss_func(args, ote * ytej).mean().item(),\n",
        "            'err': (ote * yte[j] <= 0).double().mean().item(),\n",
        "            'nd': (args.alpha * ote * yte[j] < 1).long().sum().item(),\n",
        "            'dfnorm': ote.pow(2).mean().sqrt(),\n",
        "            'fnorm': (ote + ote0[j]).pow(2).mean().sqrt(),\n",
        "            'outputs': ote if args.save_outputs else None,\n",
        "            'labels': ytej if args.save_outputs else None,\n",
        "        }\n",
        "        print(\"[i={d[step]:d} t={d[t]:.2e} wall={d[wall]:.0f}] [dt={d[dt]:.1e} dgrad={d[dgrad]:.1e} dout={d[dout]:.1e}] [train aL={d[train][aloss]:.2e} err={d[train][err]:.2f} nd={d[train][nd]}/{p}] [test aL={d[test][aloss]:.2e} err={d[test][err]:.2f}]\".format(d=state, p=len(j)), flush=True)\n",
        "        dynamics.append(state)\n",
        "\n",
        "        if done or perf_counter() - t > 120:\n",
        "            t = perf_counter()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                otr = f(xtr) - otr0\n",
        "                ote = f(xte) - ote0\n",
        "\n",
        "            out = {\n",
        "                'dynamics': dynamics,\n",
        "                'train': {\n",
        "                    'f0': otr0,\n",
        "                    'outputs': otr,\n",
        "                    'labels': ytr,\n",
        "                },\n",
        "                'test': {\n",
        "                    'f0': ote0,\n",
        "                    'outputs': ote,\n",
        "                    'labels': yte,\n",
        "                }\n",
        "            }\n",
        "            yield f, out\n",
        "\n",
        "\n",
        "def run_exp(args, f0, xtr, ytr, xte, yte):\n",
        "    run = {\n",
        "        'args': args,\n",
        "        'N': sum(p.numel() for p in f0.parameters()),\n",
        "    }\n",
        "\n",
        "    if args.delta_kernel == 1 or args.init_kernel == 1:\n",
        "        init_kernel = compute_kernels(f0, xtr, xte[:len(xtr)])\n",
        "\n",
        "    if args.init_kernel == 1:\n",
        "        run['init_kernel'] = run_kernel(args, *init_kernel, f0, xtr, ytr, xte, yte)\n",
        "\n",
        "    if args.delta_kernel == 1:\n",
        "        init_kernel = (init_kernel[0].cpu(), init_kernel[2].cpu())\n",
        "    elif args.init_kernel == 1:\n",
        "        del init_kernel\n",
        "\n",
        "    if args.regular == 1:\n",
        "        for f, out in run_regular(args, f0, xtr, ytr, xte, yte):\n",
        "            run['regular'] = out\n",
        "            yield run\n",
        "\n",
        "        if args.delta_kernel == 1 or args.final_kernel == 1:\n",
        "            final_kernel = compute_kernels(f, xtr, xte[:len(xtr)])\n",
        "\n",
        "        if args.final_kernel == 1:\n",
        "            run['final_kernel'] = run_kernel(args, *final_kernel, f, xtr, ytr, xte, yte)\n",
        "\n",
        "        if args.delta_kernel == 1:\n",
        "            final_kernel = (final_kernel[0].cpu(), final_kernel[2].cpu())\n",
        "            run['delta_kernel'] = {\n",
        "                'train': (init_kernel[0] - final_kernel[0]).norm().item(),\n",
        "                'test': (init_kernel[1] - final_kernel[1]).norm().item(),\n",
        "            }\n",
        "\n",
        "    yield run\n",
        "\n",
        "\n",
        "def execute(args):\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    if args.dtype == 'float64':\n",
        "        torch.set_default_dtype(torch.float64)\n",
        "    if args.dtype == 'float32':\n",
        "        torch.set_default_dtype(torch.float32)\n",
        "\n",
        "    if args.d is None or args.d == 0:\n",
        "        (xtr, ytr), (xte, yte) = get_binary_dataset(args.dataset, args.ptr, args.data_seed, args.device)\n",
        "    else:\n",
        "        (xtr, ytr), (xte, yte) = get_binary_pca_dataset(args.dataset, args.ptr, args.d, args.whitening, args.data_seed, args.device)\n",
        "\n",
        "    xtr = xtr.type(torch.get_default_dtype())\n",
        "    xte = xte.type(torch.get_default_dtype())\n",
        "    ytr = ytr.type(torch.get_default_dtype())\n",
        "    yte = yte.type(torch.get_default_dtype())\n",
        "\n",
        "    assert len(xte) >= args.pte\n",
        "    xte = xte[:args.pte]\n",
        "    yte = yte[:args.pte]\n",
        "\n",
        "    torch.manual_seed(args.init_seed + hash(args.alpha))\n",
        "\n",
        "    arch, act = args.arch.split('_')\n",
        "    if act == 'relu':\n",
        "        act = lambda x: 2 ** 0.5 * torch.relu(x)\n",
        "    elif act == 'tanh':\n",
        "        act = torch.tanh\n",
        "    elif act == 'softplus':\n",
        "        factor = torch.nn.functional.softplus(torch.randn(100000, dtype=torch.float64), args.spbeta).pow(2).mean().rsqrt().item()\n",
        "        act = lambda x: torch.nn.functional.softplus(x, beta=args.spbeta).mul(factor)\n",
        "    else:\n",
        "        raise ValueError('act not specified')\n",
        "\n",
        "    if arch == 'fc':\n",
        "        assert args.L is not None\n",
        "        xtr = xtr.flatten(1)\n",
        "        xte = xte.flatten(1)\n",
        "        f = FC(xtr.size(1), args.h, args.L, act, args.bias).to(args.device)\n",
        "    elif arch == 'cv':\n",
        "        assert args.bias == 0\n",
        "        f = CV(xtr.size(1), args.h, L1=args.cv_L1, L2=args.cv_L2, act=act, h_base=args.cv_h_base, fsz=args.cv_fsz, pad=args.cv_pad, stride_first=args.cv_stride_first).to(args.device)\n",
        "    elif arch == 'resnet':\n",
        "        assert args.bias == 0\n",
        "        f = Wide_ResNet(xtr.size(1), 28, args.h, act, 1, args.mix_angle).to(args.device)\n",
        "    else:\n",
        "        raise ValueError('arch not specified')\n",
        "\n",
        "    f = SplitEval(f, args.chunk)\n",
        "\n",
        "    torch.manual_seed(args.batch_seed)\n",
        "    for run in run_exp(args, f, xtr, ytr, xte, yte):\n",
        "        yield run\n",
        "\n",
        "\n",
        "def main():\n",
        "    git = {\n",
        "        'log': subprocess.getoutput('git log --format=\"%H\" -n 1 -z'),\n",
        "        'status': subprocess.getoutput('git status -z'),\n",
        "    }\n",
        "\n",
        "    # コマンドライン引数の代わりに直接変数を設定\n",
        "    args = argparse.Namespace(\n",
        "        device='cuda',\n",
        "        dtype='float64',\n",
        "\n",
        "        init_seed=0,\n",
        "        data_seed=0,\n",
        "        batch_seed=0,\n",
        "\n",
        "        dataset='fashion',\n",
        "        ptr=10000,\n",
        "        pte=50000,\n",
        "        d=None,\n",
        "        whitening=1,\n",
        "\n",
        "        arch='fc_softplus',\n",
        "        bias=0,\n",
        "        L=3,\n",
        "        h=100,\n",
        "        mix_angle=45,\n",
        "        spbeta=5.0,\n",
        "        cv_L1=2,\n",
        "        cv_L2=2,\n",
        "        cv_h_base=1,\n",
        "        cv_fsz=5,\n",
        "        cv_pad=1,\n",
        "        cv_stride_first=1,\n",
        "\n",
        "        init_kernel=0,\n",
        "        regular=1,\n",
        "        final_kernel=0,\n",
        "        store_kernel=0,\n",
        "        delta_kernel=0,\n",
        "        save_outputs=0,\n",
        "\n",
        "        alpha=1e-4,\n",
        "        f0=1,\n",
        "\n",
        "        tau_over_h=1e-3,\n",
        "        tau_alpha_crit=1e3,\n",
        "\n",
        "        train_time=18000,\n",
        "        chunk=None,\n",
        "        max_dgrad=1e-4,\n",
        "        max_dout=1e-1,\n",
        "\n",
        "        loss='softhinge',\n",
        "        lossbeta=20.0,\n",
        "\n",
        "        pickle='F10k3Lsp_alpha.pkl'\n",
        "    )\n",
        "\n",
        "    if args.pte is None:\n",
        "        args.pte = args.ptr\n",
        "\n",
        "    if args.chunk is None:\n",
        "        args.chunk = args.ptr\n",
        "\n",
        "    torch.save(args, args.pickle)\n",
        "    try:\n",
        "        for res in execute(args):\n",
        "            res['git'] = git\n",
        "            with open(args.pickle, 'wb') as f:\n",
        "                torch.save(args, f)\n",
        "                torch.save(res, f)\n",
        "    except:\n",
        "        os.remove(args.pickle)\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "2uWfMlrMerka",
        "outputId": "b1b865a8-fdb9-4348-80a4-eb0132c0bd46"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /root/.torchvision/datasets/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:04<00:00, 6059786.68it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.torchvision/datasets/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to /root/.torchvision/datasets/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /root/.torchvision/datasets/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 278743.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.torchvision/datasets/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /root/.torchvision/datasets/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /root/.torchvision/datasets/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5092269.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.torchvision/datasets/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /root/.torchvision/datasets/FashionMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /root/.torchvision/datasets/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 3948843.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.torchvision/datasets/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.torchvision/datasets/FashionMNIST/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-0fd86409d24f>\u001b[0m in \u001b[0;36m<cell line: 351>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-0fd86409d24f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'git'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-0fd86409d24f>\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mxtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myte\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_binary_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mxtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myte\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_binary_pca_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhitening\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-634e87882728>\u001b[0m in \u001b[0;36mget_binary_dataset\u001b[0;34m(dataset, p, seed, device)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_normalized_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main_adam"
      ],
      "metadata": {
        "id": "Tmq13o4LjN_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import itertools\n",
        "import os\n",
        "import subprocess\n",
        "from time import perf_counter\n",
        "\n",
        "import torch\n",
        "\n",
        "# from archi import CV, FC, Wide_ResNet\n",
        "# from dataset import get_binary_dataset, get_binary_pca_dataset\n",
        "# from dynamics import loglinspace\n",
        "\n",
        "\n",
        "class SplitEval(torch.nn.Module):\n",
        "    def __init__(self, f, size):\n",
        "        super().__init__()\n",
        "        self.f = f\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([self.f(x[i: i + self.size]) for i in range(0, len(x), self.size)])\n",
        "\n",
        "\n",
        "def hinge(out, y, alpha):\n",
        "    return (1 - alpha * out * y).relu().mean() / alpha\n",
        "\n",
        "\n",
        "def quad_hinge(out, y, alpha):\n",
        "    return 0.5 * (1 - alpha * out * y).relu().pow(2).mean() / alpha ** 2\n",
        "\n",
        "\n",
        "def mse(out, y, alpha):\n",
        "    return 0.5 * (1.1 - alpha * out * y).pow(2).mean() / alpha ** 2\n",
        "\n",
        "\n",
        "def run_regular(args, f0, loss, xtr, ytr, xte, yte):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        otr0 = f0(xtr)\n",
        "        ote0 = f0(xte)\n",
        "\n",
        "    f = copy.deepcopy(f0)\n",
        "    optimizer = torch.optim.Adam(f.parameters(), args.lr)\n",
        "\n",
        "    dynamics = []\n",
        "    checkpoint_generator = loglinspace(0.1, 1000)\n",
        "    checkpoint = next(checkpoint_generator)\n",
        "    wall = perf_counter()\n",
        "\n",
        "    for step in itertools.count():\n",
        "\n",
        "        batch = torch.randperm(len(xtr))[:args.bs]\n",
        "        xb = xtr[batch]\n",
        "\n",
        "        loss_value = loss(f(xb) - otr0[batch], ytr[batch], args.alpha)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        save = False\n",
        "\n",
        "        if step == checkpoint:\n",
        "            checkpoint = next(checkpoint_generator)\n",
        "            assert checkpoint > step\n",
        "            save = True\n",
        "\n",
        "        if save:\n",
        "            assert len(xtr) < len(xte)\n",
        "            j = torch.randperm(len(xtr))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                otr = f(xtr[j]) - otr0[j]\n",
        "                ote = f(xte[j]) - ote0[j]\n",
        "\n",
        "            state = {\n",
        "                'step': step,\n",
        "                'wall': perf_counter() - wall,\n",
        "                'batch_loss': loss_value.item(),\n",
        "                'norm': sum(p.norm().pow(2) for p in f.parameters()).sqrt().item(),\n",
        "                'dnorm': sum((p0 - p).norm().pow(2) for p0, p in zip(f0.parameters(), f.parameters())).sqrt().item(),\n",
        "                'train': {\n",
        "                    'loss': loss(otr, ytr[j], args.alpha).item(),\n",
        "                    'aloss': args.alpha * loss(otr, ytr[j], args.alpha).item(),\n",
        "                    'aaloss': args.alpha ** 2 * loss(otr, ytr[j], args.alpha).item(),\n",
        "                    'err': (otr * ytr[j] <= 0).double().mean().item(),\n",
        "                    'nd': (args.alpha * otr * ytr[j] < 1).long().sum().item(),\n",
        "                    'dfnorm': otr.pow(2).mean().sqrt(),\n",
        "                    'fnorm': (otr + otr0[j]).pow(2).mean().sqrt(),\n",
        "                },\n",
        "                'test': {\n",
        "                    'loss': loss(ote, yte[j], args.alpha).item(),\n",
        "                    'aloss': args.alpha * loss(ote, yte[j], args.alpha).item(),\n",
        "                    'aaloss': args.alpha ** 2 * loss(ote, yte[j], args.alpha).item(),\n",
        "                    'err': (ote * yte[j] <= 0).double().mean().item(),\n",
        "                    'nd': (args.alpha * ote * yte[j] < 1).long().sum().item(),\n",
        "                    'dfnorm': ote.pow(2).mean().sqrt(),\n",
        "                    'fnorm': (ote + ote0[j]).pow(2).mean().sqrt(),\n",
        "                },\n",
        "            }\n",
        "\n",
        "            if args.arch.split('_')[0] == 'fc':\n",
        "                def getw(f, i):\n",
        "                    return torch.cat(list(getattr(f.f, \"W{}\".format(i))))\n",
        "                state['wnorm'] = [getw(f, i).norm().item() for i in range(f.f.L + 1)]\n",
        "                state['dwnorm'] = [(getw(f, i) - getw(f0, i)).norm().item() for i in range(f.f.L + 1)]\n",
        "\n",
        "            print(\"[i={d[step]:d} wall={d[wall]:.0f}] [train aL={d[train][aloss]:.2e} err={d[train][err]:.2f} nd={d[train][nd]}/{p}] [test aL={d[test][aloss]:.2e} err={d[test][err]:.2f}]\".format(d=state, p=len(j)), flush=True)\n",
        "\n",
        "            dynamics.append(state)\n",
        "\n",
        "            if state['train']['nd'] == 0:\n",
        "                break\n",
        "\n",
        "        if perf_counter() > wall + args.train_time:\n",
        "            break\n",
        "\n",
        "    with torch.no_grad():\n",
        "        otr = f(xtr) - otr0\n",
        "        ote = f(xte) - ote0\n",
        "\n",
        "    out = {\n",
        "        'dynamics': dynamics,\n",
        "        'train': {\n",
        "            'f0': otr0,\n",
        "            'outputs': otr,\n",
        "            'labels': ytr,\n",
        "        },\n",
        "        'test': {\n",
        "            'f0': ote0,\n",
        "            'outputs': ote,\n",
        "            'labels': yte,\n",
        "        }\n",
        "    }\n",
        "    return f, out\n",
        "\n",
        "\n",
        "def run_exp(args, f0, xtr, ytr, xte, yte):\n",
        "    run = {\n",
        "        'args': args,\n",
        "        'N': sum(p.numel() for p in f0.parameters()),\n",
        "    }\n",
        "\n",
        "    if args.loss == 'hinge':\n",
        "        loss = hinge\n",
        "    if args.loss == 'quad_hinge':\n",
        "        loss = quad_hinge\n",
        "    if args.loss == 'mse':\n",
        "        loss = mse\n",
        "\n",
        "    _f, out = run_regular(args, f0, loss, xtr, ytr, xte, yte)\n",
        "    run['regular'] = out\n",
        "\n",
        "    return run\n",
        "\n",
        "\n",
        "def execute(args):\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    if args.dtype == 'float64':\n",
        "        torch.set_default_dtype(torch.float64)\n",
        "    if args.dtype == 'float32':\n",
        "        torch.set_default_dtype(torch.float32)\n",
        "\n",
        "    if args.d is None or args.d == 0:\n",
        "        (xtr, ytr), (xte, yte) = get_binary_dataset(args.dataset, args.ptr, args.data_seed, args.device)\n",
        "    else:\n",
        "        (xtr, ytr), (xte, yte) = get_binary_pca_dataset(args.dataset, args.ptr, args.d, args.whitening, args.data_seed, args.device)\n",
        "\n",
        "    xtr = xtr.type(torch.get_default_dtype())\n",
        "    xte = xte.type(torch.get_default_dtype())\n",
        "    ytr = ytr.type(torch.get_default_dtype())\n",
        "    yte = yte.type(torch.get_default_dtype())\n",
        "\n",
        "    assert len(xte) >= args.pte\n",
        "    xte = xte[:args.pte]\n",
        "    yte = yte[:args.pte]\n",
        "\n",
        "    torch.manual_seed(args.init_seed + hash(args.alpha))\n",
        "\n",
        "    arch, act = args.arch.split('_')\n",
        "    if act == 'relu':\n",
        "        act = lambda x: 2 ** 0.5 * torch.relu(x)\n",
        "    elif act == 'tanh':\n",
        "        act = torch.tanh\n",
        "    elif act == 'softplus':\n",
        "        factor = torch.nn.functional.softplus(torch.randn(100000, dtype=torch.float64), args.spbeta).pow(2).mean().rsqrt().item()\n",
        "        act = lambda x: torch.nn.functional.softplus(x, beta=args.spbeta).mul(factor)\n",
        "    else:\n",
        "        raise ValueError('act not specified')\n",
        "\n",
        "    if arch == 'fc':\n",
        "        assert args.L is not None\n",
        "        xtr = xtr.flatten(1)\n",
        "        xte = xte.flatten(1)\n",
        "        f = FC(xtr.size(1), args.h, args.L, act).to(args.device)\n",
        "    elif arch == 'cv':\n",
        "        f = CV(xtr.size(1), args.h, h_base=1, L1=2, L2=2, act=act, fsz=5, pad=1, stride_first=True).to(args.device)\n",
        "    elif arch == 'resnet':\n",
        "        f = Wide_ResNet(xtr.size(1), 28, args.h, act, 1, args.mix_angle).to(args.device)\n",
        "    else:\n",
        "        raise ValueError('arch not specified')\n",
        "\n",
        "    f = SplitEval(f, args.chunk)\n",
        "\n",
        "    torch.manual_seed(args.batch_seed)\n",
        "    run = run_exp(args, f, xtr, ytr, xte, yte)\n",
        "    return run\n",
        "\n",
        "\n",
        "# Move Args class outside of main function\n",
        "class Args:\n",
        "    arch = 'fc_softplus'\n",
        "    alpha = 1e-3\n",
        "    batch_seed = 0\n",
        "    bs = 32\n",
        "    d = 0\n",
        "    dataset = 'cifar10'\n",
        "    device = 'cuda'\n",
        "    dtype = 'float32'\n",
        "    h = 100\n",
        "    init_seed = 0\n",
        "    L = 3\n",
        "    loss = 'hinge'\n",
        "    mix_angle = 0.5\n",
        "    pte = 50000\n",
        "    ptr = 10000\n",
        "    spbeta = 5.0\n",
        "    train_time = 28800.0\n",
        "    whitening = True\n",
        "    data_seed = 0\n",
        "    chunk = 128\n",
        "    lr = 1e-3\n",
        "\n",
        "\n",
        "def main():\n",
        "    git = {\n",
        "        'commit': subprocess.getoutput(\"git rev-parse HEAD\"),\n",
        "        'branch': subprocess.getoutput(\"git rev-parse --abbrev-ref HEAD\"),\n",
        "        'message': subprocess.getoutput(\"git log -1 --pretty=format:'%h %s'\"),\n",
        "    }\n",
        "\n",
        "    # Grid search over `alpha` and `init_seed`\n",
        "    alpha_values = [1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7]\n",
        "    init_seed_values = list(range(20))\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for alpha, init_seed in itertools.product(alpha_values, init_seed_values):\n",
        "        args = copy.deepcopy(Args)\n",
        "        args.alpha = alpha\n",
        "        args.init_seed = init_seed\n",
        "\n",
        "        run = execute(args)\n",
        "\n",
        "        result_file = f\"results/experiment_{args.arch}_{args.alpha}_{args.loss}_{args.h}_seed{args.init_seed}.pt\"\n",
        "        os.makedirs(\"results\", exist_ok=True)\n",
        "        torch.save(run, result_file)\n",
        "        results.append(result_file)\n",
        "\n",
        "        print(f\"Results saved to {result_file}\")\n",
        "\n",
        "    # Print Git information\n",
        "    print(\"Git information:\")\n",
        "    for k, v in git.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17WF2RfWhrbE",
        "outputId": "c6de897f-b45d-4b89-cd84-3734d6557f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[i=0 wall=0] [train aL=1.00e+00 err=0.47 nd=10000/10000] [test aL=1.00e+00 err=0.48]\n",
            "[i=1 wall=0] [train aL=1.00e+00 err=0.45 nd=10000/10000] [test aL=1.00e+00 err=0.46]\n",
            "[i=2 wall=0] [train aL=1.00e+00 err=0.45 nd=10000/10000] [test aL=1.00e+00 err=0.46]\n",
            "[i=3 wall=1] [train aL=1.00e+00 err=0.45 nd=10000/10000] [test aL=1.00e+00 err=0.46]\n",
            "[i=4 wall=1] [train aL=1.00e+00 err=0.46 nd=10000/10000] [test aL=1.00e+00 err=0.46]\n",
            "[i=5 wall=1] [train aL=1.00e+00 err=0.46 nd=10000/10000] [test aL=1.00e+00 err=0.46]\n",
            "[i=6 wall=1] [train aL=1.00e+00 err=0.46 nd=10000/10000] [test aL=1.00e+00 err=0.46]\n",
            "[i=7 wall=1] [train aL=1.00e+00 err=0.45 nd=10000/10000] [test aL=1.00e+00 err=0.46]\n",
            "[i=8 wall=1] [train aL=1.00e+00 err=0.45 nd=10000/10000] [test aL=1.00e+00 err=0.46]\n",
            "[i=9 wall=1] [train aL=1.00e+00 err=0.45 nd=10000/10000] [test aL=1.00e+00 err=0.46]\n",
            "[i=10 wall=1] [train aL=1.00e+00 err=0.45 nd=10000/10000] [test aL=1.00e+00 err=0.46]\n",
            "[i=11 wall=2] [train aL=1.00e+00 err=0.45 nd=10000/10000] [test aL=1.00e+00 err=0.46]\n",
            "[i=13 wall=2] [train aL=1.00e+00 err=0.44 nd=10000/10000] [test aL=1.00e+00 err=0.45]\n",
            "[i=15 wall=2] [train aL=1.00e+00 err=0.43 nd=10000/10000] [test aL=1.00e+00 err=0.44]\n",
            "[i=17 wall=2] [train aL=1.00e+00 err=0.42 nd=10000/10000] [test aL=1.00e+00 err=0.43]\n",
            "[i=19 wall=2] [train aL=1.00e+00 err=0.41 nd=10000/10000] [test aL=1.00e+00 err=0.42]\n",
            "[i=21 wall=2] [train aL=1.00e+00 err=0.40 nd=10000/10000] [test aL=1.00e+00 err=0.41]\n",
            "[i=24 wall=2] [train aL=1.00e+00 err=0.39 nd=10000/10000] [test aL=1.00e+00 err=0.40]\n",
            "[i=27 wall=2] [train aL=1.00e+00 err=0.38 nd=10000/10000] [test aL=1.00e+00 err=0.39]\n",
            "[i=30 wall=2] [train aL=1.00e+00 err=0.37 nd=10000/10000] [test aL=1.00e+00 err=0.38]\n",
            "[i=33 wall=2] [train aL=1.00e+00 err=0.36 nd=10000/10000] [test aL=1.00e+00 err=0.37]\n",
            "[i=37 wall=3] [train aL=1.00e+00 err=0.36 nd=10000/10000] [test aL=1.00e+00 err=0.37]\n",
            "[i=41 wall=3] [train aL=1.00e+00 err=0.35 nd=10000/10000] [test aL=1.00e+00 err=0.36]\n",
            "[i=46 wall=3] [train aL=1.00e+00 err=0.34 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=51 wall=3] [train aL=1.00e+00 err=0.33 nd=10000/10000] [test aL=1.00e+00 err=0.34]\n",
            "[i=57 wall=3] [train aL=1.00e+00 err=0.33 nd=10000/10000] [test aL=1.00e+00 err=0.34]\n",
            "[i=63 wall=3] [train aL=1.00e+00 err=0.33 nd=10000/10000] [test aL=1.00e+00 err=0.34]\n",
            "[i=70 wall=3] [train aL=1.00e+00 err=0.33 nd=10000/10000] [test aL=1.00e+00 err=0.34]\n",
            "[i=77 wall=3] [train aL=1.00e+00 err=0.33 nd=10000/10000] [test aL=1.00e+00 err=0.34]\n",
            "[i=85 wall=3] [train aL=1.00e+00 err=0.33 nd=10000/10000] [test aL=1.00e+00 err=0.34]\n",
            "[i=94 wall=3] [train aL=1.00e+00 err=0.34 nd=10000/10000] [test aL=1.00e+00 err=0.34]\n",
            "[i=104 wall=4] [train aL=1.00e+00 err=0.34 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=115 wall=4] [train aL=1.00e+00 err=0.36 nd=10000/10000] [test aL=1.00e+00 err=0.37]\n",
            "[i=127 wall=4] [train aL=1.00e+00 err=0.36 nd=10000/10000] [test aL=1.00e+00 err=0.37]\n",
            "[i=140 wall=4] [train aL=1.00e+00 err=0.36 nd=10000/10000] [test aL=1.00e+00 err=0.36]\n",
            "[i=154 wall=4] [train aL=1.00e+00 err=0.36 nd=10000/10000] [test aL=1.00e+00 err=0.37]\n",
            "[i=170 wall=4] [train aL=1.00e+00 err=0.36 nd=10000/10000] [test aL=1.00e+00 err=0.36]\n",
            "[i=187 wall=4] [train aL=1.00e+00 err=0.38 nd=10000/10000] [test aL=1.00e+00 err=0.38]\n",
            "[i=206 wall=4] [train aL=1.00e+00 err=0.36 nd=10000/10000] [test aL=1.00e+00 err=0.37]\n",
            "[i=227 wall=5] [train aL=9.99e-01 err=0.34 nd=10000/10000] [test aL=9.99e-01 err=0.35]\n",
            "[i=250 wall=5] [train aL=9.99e-01 err=0.33 nd=10000/10000] [test aL=9.99e-01 err=0.33]\n",
            "[i=275 wall=5] [train aL=9.99e-01 err=0.33 nd=10000/10000] [test aL=9.99e-01 err=0.33]\n",
            "[i=303 wall=5] [train aL=9.98e-01 err=0.34 nd=10000/10000] [test aL=9.98e-01 err=0.34]\n",
            "[i=333 wall=5] [train aL=9.98e-01 err=0.34 nd=10000/10000] [test aL=9.98e-01 err=0.35]\n",
            "[i=366 wall=5] [train aL=9.96e-01 err=0.35 nd=10000/10000] [test aL=9.96e-01 err=0.36]\n",
            "[i=402 wall=6] [train aL=9.95e-01 err=0.33 nd=10000/10000] [test aL=9.95e-01 err=0.34]\n",
            "[i=442 wall=6] [train aL=9.92e-01 err=0.34 nd=10000/10000] [test aL=9.92e-01 err=0.35]\n",
            "[i=486 wall=6] [train aL=9.88e-01 err=0.34 nd=10000/10000] [test aL=9.88e-01 err=0.35]\n",
            "[i=534 wall=6] [train aL=9.81e-01 err=0.33 nd=10000/10000] [test aL=9.82e-01 err=0.34]\n",
            "[i=586 wall=6] [train aL=9.70e-01 err=0.34 nd=10000/10000] [test aL=9.71e-01 err=0.35]\n",
            "[i=643 wall=7] [train aL=9.56e-01 err=0.34 nd=10000/10000] [test aL=9.57e-01 err=0.35]\n",
            "[i=706 wall=7] [train aL=9.33e-01 err=0.34 nd=10000/10000] [test aL=9.34e-01 err=0.35]\n",
            "[i=775 wall=7] [train aL=8.93e-01 err=0.35 nd=9978/10000] [test aL=8.94e-01 err=0.35]\n",
            "[i=850 wall=8] [train aL=8.56e-01 err=0.34 nd=9554/10000] [test aL=8.57e-01 err=0.35]\n",
            "[i=932 wall=8] [train aL=8.21e-01 err=0.34 nd=9312/10000] [test aL=8.24e-01 err=0.34]\n",
            "[i=1021 wall=8] [train aL=7.85e-01 err=0.32 nd=9081/10000] [test aL=7.89e-01 err=0.33]\n",
            "[i=1119 wall=9] [train aL=7.54e-01 err=0.32 nd=8658/10000] [test aL=7.62e-01 err=0.33]\n",
            "[i=1225 wall=9] [train aL=7.24e-01 err=0.32 nd=8352/10000] [test aL=7.38e-01 err=0.33]\n",
            "[i=1341 wall=9] [train aL=7.00e-01 err=0.32 nd=7820/10000] [test aL=7.19e-01 err=0.33]\n",
            "[i=1467 wall=10] [train aL=6.86e-01 err=0.31 nd=7554/10000] [test aL=7.08e-01 err=0.32]\n",
            "[i=1604 wall=10] [train aL=6.76e-01 err=0.30 nd=7270/10000] [test aL=7.00e-01 err=0.32]\n",
            "[i=1753 wall=11] [train aL=6.68e-01 err=0.30 nd=7152/10000] [test aL=6.94e-01 err=0.32]\n",
            "[i=1914 wall=11] [train aL=6.60e-01 err=0.29 nd=6959/10000] [test aL=6.88e-01 err=0.31]\n",
            "[i=2089 wall=12] [train aL=6.52e-01 err=0.29 nd=6844/10000] [test aL=6.81e-01 err=0.31]\n",
            "[i=2278 wall=13] [train aL=6.45e-01 err=0.29 nd=6722/10000] [test aL=6.76e-01 err=0.31]\n",
            "[i=2482 wall=13] [train aL=6.38e-01 err=0.28 nd=6694/10000] [test aL=6.70e-01 err=0.30]\n",
            "[i=2702 wall=14] [train aL=6.30e-01 err=0.28 nd=6631/10000] [test aL=6.63e-01 err=0.30]\n",
            "[i=2939 wall=15] [train aL=6.23e-01 err=0.28 nd=6514/10000] [test aL=6.58e-01 err=0.30]\n",
            "[i=3194 wall=16] [train aL=6.16e-01 err=0.27 nd=6453/10000] [test aL=6.52e-01 err=0.29]\n",
            "[i=3468 wall=16] [train aL=6.09e-01 err=0.27 nd=6366/10000] [test aL=6.47e-01 err=0.29]\n",
            "[i=3762 wall=17] [train aL=6.04e-01 err=0.27 nd=6220/10000] [test aL=6.44e-01 err=0.29]\n",
            "[i=4076 wall=18] [train aL=5.98e-01 err=0.26 nd=6194/10000] [test aL=6.40e-01 err=0.29]\n",
            "[i=4411 wall=19] [train aL=5.90e-01 err=0.26 nd=6143/10000] [test aL=6.35e-01 err=0.28]\n",
            "[i=4768 wall=20] [train aL=5.84e-01 err=0.26 nd=6111/10000] [test aL=6.30e-01 err=0.28]\n",
            "[i=5148 wall=21] [train aL=5.78e-01 err=0.25 nd=6040/10000] [test aL=6.27e-01 err=0.28]\n",
            "[i=5551 wall=22] [train aL=5.71e-01 err=0.25 nd=5949/10000] [test aL=6.23e-01 err=0.28]\n",
            "[i=5977 wall=24] [train aL=5.65e-01 err=0.25 nd=5774/10000] [test aL=6.21e-01 err=0.28]\n",
            "[i=6427 wall=26] [train aL=5.59e-01 err=0.25 nd=5743/10000] [test aL=6.18e-01 err=0.28]\n",
            "[i=6902 wall=27] [train aL=5.52e-01 err=0.24 nd=5644/10000] [test aL=6.15e-01 err=0.28]\n",
            "[i=7401 wall=28] [train aL=5.46e-01 err=0.24 nd=5580/10000] [test aL=6.13e-01 err=0.28]\n",
            "[i=7924 wall=30] [train aL=5.40e-01 err=0.24 nd=5452/10000] [test aL=6.10e-01 err=0.28]\n",
            "[i=8472 wall=31] [train aL=5.35e-01 err=0.24 nd=5452/10000] [test aL=6.07e-01 err=0.27]\n",
            "[i=9044 wall=33] [train aL=5.28e-01 err=0.23 nd=5325/10000] [test aL=6.05e-01 err=0.27]\n",
            "[i=9640 wall=35] [train aL=5.21e-01 err=0.23 nd=5237/10000] [test aL=6.04e-01 err=0.27]\n",
            "[i=10259 wall=36] [train aL=5.14e-01 err=0.23 nd=5209/10000] [test aL=6.02e-01 err=0.27]\n",
            "[i=10901 wall=39] [train aL=5.07e-01 err=0.22 nd=5191/10000] [test aL=6.01e-01 err=0.27]\n",
            "[i=11565 wall=40] [train aL=4.99e-01 err=0.22 nd=5259/10000] [test aL=5.98e-01 err=0.27]\n",
            "[i=12251 wall=42] [train aL=4.91e-01 err=0.22 nd=5099/10000] [test aL=5.96e-01 err=0.27]\n",
            "[i=12958 wall=44] [train aL=4.84e-01 err=0.21 nd=4964/10000] [test aL=5.95e-01 err=0.27]\n",
            "[i=13685 wall=46] [train aL=4.75e-01 err=0.21 nd=4934/10000] [test aL=5.93e-01 err=0.27]\n",
            "[i=14431 wall=49] [train aL=4.67e-01 err=0.20 nd=4754/10000] [test aL=5.92e-01 err=0.27]\n",
            "[i=15195 wall=51] [train aL=4.58e-01 err=0.20 nd=4685/10000] [test aL=5.89e-01 err=0.27]\n",
            "[i=15977 wall=53] [train aL=4.50e-01 err=0.20 nd=4760/10000] [test aL=5.89e-01 err=0.27]\n",
            "[i=16775 wall=55] [train aL=4.40e-01 err=0.19 nd=4602/10000] [test aL=5.88e-01 err=0.27]\n",
            "[i=17589 wall=59] [train aL=4.32e-01 err=0.19 nd=4488/10000] [test aL=5.87e-01 err=0.27]\n",
            "[i=18417 wall=62] [train aL=4.21e-01 err=0.18 nd=4405/10000] [test aL=5.85e-01 err=0.27]\n",
            "[i=19259 wall=64] [train aL=4.12e-01 err=0.18 nd=4413/10000] [test aL=5.83e-01 err=0.27]\n",
            "[i=20114 wall=66] [train aL=4.04e-01 err=0.18 nd=4252/10000] [test aL=5.85e-01 err=0.27]\n",
            "[i=20981 wall=69] [train aL=3.94e-01 err=0.17 nd=4087/10000] [test aL=5.84e-01 err=0.27]\n",
            "[i=21859 wall=71] [train aL=3.83e-01 err=0.17 nd=4105/10000] [test aL=5.83e-01 err=0.27]\n",
            "[i=22747 wall=74] [train aL=3.75e-01 err=0.16 nd=3942/10000] [test aL=5.84e-01 err=0.27]\n",
            "[i=23645 wall=77] [train aL=3.67e-01 err=0.16 nd=3982/10000] [test aL=5.84e-01 err=0.27]\n",
            "[i=24552 wall=79] [train aL=3.53e-01 err=0.15 nd=3725/10000] [test aL=5.85e-01 err=0.27]\n",
            "[i=25467 wall=82] [train aL=3.43e-01 err=0.15 nd=3620/10000] [test aL=5.85e-01 err=0.27]\n",
            "[i=26389 wall=84] [train aL=3.34e-01 err=0.14 nd=3554/10000] [test aL=5.87e-01 err=0.27]\n",
            "[i=27318 wall=87] [train aL=3.24e-01 err=0.14 nd=3374/10000] [test aL=5.86e-01 err=0.27]\n",
            "[i=28253 wall=90] [train aL=3.17e-01 err=0.14 nd=3479/10000] [test aL=5.86e-01 err=0.27]\n",
            "[i=29194 wall=92] [train aL=3.06e-01 err=0.13 nd=3435/10000] [test aL=5.86e-01 err=0.27]\n",
            "[i=30141 wall=95] [train aL=2.99e-01 err=0.13 nd=3253/10000] [test aL=5.86e-01 err=0.27]\n",
            "[i=31092 wall=98] [train aL=2.87e-01 err=0.12 nd=3011/10000] [test aL=5.87e-01 err=0.27]\n",
            "[i=32048 wall=101] [train aL=2.80e-01 err=0.12 nd=3060/10000] [test aL=5.87e-01 err=0.27]\n",
            "[i=33008 wall=103] [train aL=2.71e-01 err=0.12 nd=2911/10000] [test aL=5.87e-01 err=0.27]\n",
            "[i=33972 wall=106] [train aL=2.65e-01 err=0.12 nd=2885/10000] [test aL=5.85e-01 err=0.28]\n",
            "[i=34939 wall=109] [train aL=2.54e-01 err=0.11 nd=2729/10000] [test aL=5.87e-01 err=0.27]\n",
            "[i=35909 wall=112] [train aL=2.47e-01 err=0.11 nd=2654/10000] [test aL=5.86e-01 err=0.28]\n",
            "[i=36882 wall=114] [train aL=2.39e-01 err=0.11 nd=2495/10000] [test aL=5.85e-01 err=0.28]\n",
            "[i=37857 wall=117] [train aL=2.34e-01 err=0.10 nd=2574/10000] [test aL=5.87e-01 err=0.27]\n",
            "[i=38835 wall=120] [train aL=2.27e-01 err=0.10 nd=2353/10000] [test aL=5.85e-01 err=0.27]\n",
            "[i=39815 wall=124] [train aL=2.21e-01 err=0.10 nd=2425/10000] [test aL=5.86e-01 err=0.28]\n",
            "[i=40797 wall=127] [train aL=2.16e-01 err=0.10 nd=2192/10000] [test aL=5.86e-01 err=0.28]\n",
            "[i=41781 wall=129] [train aL=2.12e-01 err=0.10 nd=2278/10000] [test aL=5.85e-01 err=0.28]\n",
            "[i=42766 wall=132] [train aL=2.08e-01 err=0.10 nd=2171/10000] [test aL=5.87e-01 err=0.28]\n",
            "[i=43753 wall=135] [train aL=2.03e-01 err=0.09 nd=2123/10000] [test aL=5.86e-01 err=0.28]\n",
            "[i=44741 wall=138] [train aL=2.01e-01 err=0.09 nd=2044/10000] [test aL=5.86e-01 err=0.28]\n",
            "[i=45730 wall=141] [train aL=1.99e-01 err=0.09 nd=2198/10000] [test aL=5.87e-01 err=0.28]\n",
            "[i=46720 wall=143] [train aL=1.94e-01 err=0.09 nd=1886/10000] [test aL=5.88e-01 err=0.28]\n",
            "[i=47711 wall=147] [train aL=1.91e-01 err=0.09 nd=1877/10000] [test aL=5.87e-01 err=0.28]\n",
            "[i=48703 wall=149] [train aL=1.89e-01 err=0.09 nd=2031/10000] [test aL=5.86e-01 err=0.28]\n",
            "[i=49696 wall=152] [train aL=1.91e-01 err=0.09 nd=2124/10000] [test aL=5.89e-01 err=0.28]\n",
            "[i=50690 wall=155] [train aL=1.84e-01 err=0.09 nd=2082/10000] [test aL=5.88e-01 err=0.28]\n",
            "[i=51684 wall=158] [train aL=1.84e-01 err=0.09 nd=1846/10000] [test aL=5.85e-01 err=0.27]\n",
            "[i=52679 wall=161] [train aL=1.80e-01 err=0.09 nd=1999/10000] [test aL=5.85e-01 err=0.28]\n",
            "[i=53674 wall=164] [train aL=1.79e-01 err=0.09 nd=1857/10000] [test aL=5.86e-01 err=0.28]\n",
            "[i=54670 wall=167] [train aL=1.76e-01 err=0.09 nd=1865/10000] [test aL=5.88e-01 err=0.28]\n",
            "[i=55666 wall=173] [train aL=1.75e-01 err=0.08 nd=1751/10000] [test aL=5.86e-01 err=0.28]\n",
            "[i=56663 wall=176] [train aL=1.74e-01 err=0.08 nd=1919/10000] [test aL=5.90e-01 err=0.28]\n",
            "[i=57660 wall=179] [train aL=1.72e-01 err=0.08 nd=1625/10000] [test aL=5.88e-01 err=0.28]\n",
            "[i=58657 wall=181] [train aL=1.71e-01 err=0.08 nd=1791/10000] [test aL=5.90e-01 err=0.28]\n",
            "[i=59655 wall=185] [train aL=1.68e-01 err=0.08 nd=1665/10000] [test aL=5.90e-01 err=0.28]\n",
            "[i=60653 wall=187] [train aL=1.70e-01 err=0.08 nd=1707/10000] [test aL=5.93e-01 err=0.28]\n",
            "[i=61651 wall=190] [train aL=1.66e-01 err=0.08 nd=1933/10000] [test aL=5.87e-01 err=0.28]\n",
            "[i=62649 wall=193] [train aL=1.66e-01 err=0.08 nd=1794/10000] [test aL=5.87e-01 err=0.28]\n",
            "[i=63648 wall=196] [train aL=1.63e-01 err=0.08 nd=1528/10000] [test aL=5.90e-01 err=0.28]\n",
            "[i=64647 wall=199] [train aL=1.61e-01 err=0.08 nd=1741/10000] [test aL=5.88e-01 err=0.28]\n",
            "[i=65646 wall=201] [train aL=1.60e-01 err=0.08 nd=1693/10000] [test aL=5.90e-01 err=0.28]\n",
            "[i=66645 wall=204] [train aL=1.60e-01 err=0.08 nd=1647/10000] [test aL=5.88e-01 err=0.28]\n",
            "[i=67644 wall=207] [train aL=1.59e-01 err=0.08 nd=1428/10000] [test aL=5.92e-01 err=0.28]\n",
            "[i=68643 wall=210] [train aL=1.58e-01 err=0.08 nd=1548/10000] [test aL=5.92e-01 err=0.28]\n",
            "[i=69642 wall=213] [train aL=1.57e-01 err=0.08 nd=1747/10000] [test aL=5.89e-01 err=0.28]\n",
            "[i=70642 wall=216] [train aL=1.57e-01 err=0.08 nd=1523/10000] [test aL=5.89e-01 err=0.28]\n",
            "[i=71642 wall=218] [train aL=1.56e-01 err=0.08 nd=1441/10000] [test aL=5.89e-01 err=0.28]\n",
            "[i=72642 wall=222] [train aL=1.56e-01 err=0.08 nd=1467/10000] [test aL=5.90e-01 err=0.28]\n",
            "[i=73642 wall=224] [train aL=1.54e-01 err=0.08 nd=1456/10000] [test aL=5.88e-01 err=0.28]\n",
            "[i=74642 wall=227] [train aL=1.54e-01 err=0.08 nd=1763/10000] [test aL=5.88e-01 err=0.28]\n",
            "[i=75642 wall=230] [train aL=1.55e-01 err=0.08 nd=1649/10000] [test aL=5.88e-01 err=0.28]\n",
            "[i=76642 wall=233] [train aL=1.52e-01 err=0.08 nd=1600/10000] [test aL=5.92e-01 err=0.28]\n",
            "[i=77642 wall=237] [train aL=1.51e-01 err=0.08 nd=1600/10000] [test aL=5.91e-01 err=0.28]\n",
            "[i=78642 wall=240] [train aL=1.51e-01 err=0.08 nd=1653/10000] [test aL=5.93e-01 err=0.28]\n",
            "[i=79642 wall=242] [train aL=1.50e-01 err=0.08 nd=1379/10000] [test aL=5.94e-01 err=0.28]\n",
            "[i=80642 wall=246] [train aL=1.49e-01 err=0.08 nd=1700/10000] [test aL=5.93e-01 err=0.28]\n",
            "[i=81642 wall=249] [train aL=1.48e-01 err=0.08 nd=1566/10000] [test aL=5.98e-01 err=0.28]\n",
            "[i=82642 wall=251] [train aL=1.46e-01 err=0.08 nd=1620/10000] [test aL=5.97e-01 err=0.28]\n",
            "[i=83642 wall=254] [train aL=1.46e-01 err=0.08 nd=1531/10000] [test aL=5.98e-01 err=0.28]\n",
            "[i=84642 wall=258] [train aL=1.44e-01 err=0.08 nd=1389/10000] [test aL=5.97e-01 err=0.28]\n",
            "[i=85642 wall=260] [train aL=1.43e-01 err=0.08 nd=1195/10000] [test aL=6.00e-01 err=0.28]\n",
            "[i=86642 wall=263] [train aL=1.43e-01 err=0.08 nd=1631/10000] [test aL=6.00e-01 err=0.28]\n",
            "[i=87642 wall=266] [train aL=1.41e-01 err=0.08 nd=1453/10000] [test aL=6.01e-01 err=0.28]\n",
            "[i=88642 wall=271] [train aL=1.40e-01 err=0.08 nd=1300/10000] [test aL=6.02e-01 err=0.28]\n",
            "[i=89642 wall=274] [train aL=1.40e-01 err=0.08 nd=1503/10000] [test aL=6.06e-01 err=0.28]\n",
            "[i=90642 wall=276] [train aL=1.39e-01 err=0.08 nd=1353/10000] [test aL=6.10e-01 err=0.28]\n",
            "[i=91642 wall=279] [train aL=1.37e-01 err=0.08 nd=1214/10000] [test aL=6.06e-01 err=0.28]\n",
            "[i=92642 wall=282] [train aL=1.35e-01 err=0.08 nd=1352/10000] [test aL=6.08e-01 err=0.28]\n",
            "[i=93642 wall=285] [train aL=1.34e-01 err=0.08 nd=1308/10000] [test aL=6.12e-01 err=0.28]\n",
            "[i=94642 wall=288] [train aL=1.33e-01 err=0.08 nd=1345/10000] [test aL=6.11e-01 err=0.28]\n",
            "[i=95642 wall=290] [train aL=1.31e-01 err=0.08 nd=1249/10000] [test aL=6.15e-01 err=0.28]\n",
            "[i=96642 wall=294] [train aL=1.31e-01 err=0.08 nd=1385/10000] [test aL=6.24e-01 err=0.28]\n",
            "[i=97642 wall=296] [train aL=1.31e-01 err=0.08 nd=1609/10000] [test aL=6.28e-01 err=0.28]\n",
            "[i=98642 wall=299] [train aL=1.26e-01 err=0.07 nd=1448/10000] [test aL=6.25e-01 err=0.28]\n",
            "[i=99642 wall=302] [train aL=1.25e-01 err=0.07 nd=1483/10000] [test aL=6.29e-01 err=0.28]\n",
            "[i=100642 wall=305] [train aL=1.23e-01 err=0.07 nd=1552/10000] [test aL=6.29e-01 err=0.28]\n",
            "[i=101642 wall=308] [train aL=1.20e-01 err=0.07 nd=1177/10000] [test aL=6.36e-01 err=0.28]\n",
            "[i=102642 wall=311] [train aL=1.22e-01 err=0.07 nd=1558/10000] [test aL=6.42e-01 err=0.28]\n",
            "[i=103642 wall=313] [train aL=1.17e-01 err=0.06 nd=1582/10000] [test aL=6.46e-01 err=0.28]\n",
            "[i=104642 wall=316] [train aL=1.15e-01 err=0.06 nd=1630/10000] [test aL=6.51e-01 err=0.28]\n",
            "[i=105642 wall=319] [train aL=1.12e-01 err=0.06 nd=1200/10000] [test aL=6.55e-01 err=0.28]\n",
            "[i=106642 wall=323] [train aL=1.09e-01 err=0.06 nd=1270/10000] [test aL=6.65e-01 err=0.28]\n",
            "[i=107642 wall=325] [train aL=1.07e-01 err=0.05 nd=1339/10000] [test aL=6.61e-01 err=0.28]\n",
            "[i=108642 wall=328] [train aL=1.04e-01 err=0.05 nd=1194/10000] [test aL=6.74e-01 err=0.28]\n",
            "[i=109642 wall=333] [train aL=1.03e-01 err=0.05 nd=1285/10000] [test aL=6.77e-01 err=0.28]\n",
            "[i=110642 wall=335] [train aL=9.99e-02 err=0.05 nd=1358/10000] [test aL=6.83e-01 err=0.28]\n",
            "[i=111642 wall=338] [train aL=9.75e-02 err=0.04 nd=1016/10000] [test aL=6.92e-01 err=0.28]\n",
            "[i=112642 wall=341] [train aL=9.51e-02 err=0.04 nd=1461/10000] [test aL=6.93e-01 err=0.28]\n",
            "[i=113642 wall=344] [train aL=9.36e-02 err=0.04 nd=1287/10000] [test aL=6.97e-01 err=0.28]\n",
            "[i=114642 wall=347] [train aL=9.20e-02 err=0.04 nd=1049/10000] [test aL=7.13e-01 err=0.28]\n",
            "[i=115642 wall=349] [train aL=8.80e-02 err=0.04 nd=1154/10000] [test aL=7.25e-01 err=0.29]\n",
            "[i=116642 wall=352] [train aL=8.36e-02 err=0.03 nd=1003/10000] [test aL=7.31e-01 err=0.29]\n",
            "[i=117642 wall=355] [train aL=1.09e-01 err=0.04 nd=1473/10000] [test aL=7.62e-01 err=0.29]\n",
            "[i=118642 wall=359] [train aL=7.87e-02 err=0.03 nd=1136/10000] [test aL=7.44e-01 err=0.29]\n",
            "[i=119642 wall=362] [train aL=7.31e-02 err=0.03 nd=969/10000] [test aL=7.53e-01 err=0.29]\n",
            "[i=120642 wall=365] [train aL=6.86e-02 err=0.03 nd=1198/10000] [test aL=7.68e-01 err=0.29]\n",
            "[i=121642 wall=368] [train aL=6.46e-02 err=0.03 nd=1056/10000] [test aL=7.78e-01 err=0.29]\n",
            "[i=122642 wall=371] [train aL=5.95e-02 err=0.03 nd=969/10000] [test aL=7.82e-01 err=0.29]\n",
            "[i=123642 wall=374] [train aL=5.65e-02 err=0.03 nd=816/10000] [test aL=8.10e-01 err=0.29]\n",
            "[i=124642 wall=376] [train aL=5.22e-02 err=0.02 nd=750/10000] [test aL=8.11e-01 err=0.29]\n",
            "[i=125642 wall=380] [train aL=4.96e-02 err=0.02 nd=874/10000] [test aL=8.26e-01 err=0.29]\n",
            "[i=126642 wall=382] [train aL=4.71e-02 err=0.02 nd=835/10000] [test aL=8.34e-01 err=0.29]\n",
            "[i=127642 wall=385] [train aL=4.44e-02 err=0.02 nd=680/10000] [test aL=8.42e-01 err=0.29]\n",
            "[i=128642 wall=388] [train aL=4.36e-02 err=0.02 nd=802/10000] [test aL=8.52e-01 err=0.29]\n",
            "[i=129642 wall=391] [train aL=4.11e-02 err=0.02 nd=695/10000] [test aL=8.67e-01 err=0.29]\n",
            "[i=130642 wall=394] [train aL=4.28e-02 err=0.02 nd=759/10000] [test aL=8.75e-01 err=0.29]\n",
            "[i=131642 wall=397] [train aL=4.01e-02 err=0.02 nd=692/10000] [test aL=8.79e-01 err=0.29]\n",
            "[i=132642 wall=399] [train aL=3.80e-02 err=0.02 nd=481/10000] [test aL=9.10e-01 err=0.29]\n",
            "[i=133642 wall=402] [train aL=3.68e-02 err=0.01 nd=478/10000] [test aL=9.20e-01 err=0.29]\n",
            "[i=134642 wall=405] [train aL=3.56e-02 err=0.01 nd=459/10000] [test aL=9.22e-01 err=0.29]\n",
            "[i=135642 wall=408] [train aL=3.58e-02 err=0.01 nd=409/10000] [test aL=9.41e-01 err=0.29]\n",
            "[i=136642 wall=411] [train aL=3.35e-02 err=0.01 nd=484/10000] [test aL=9.50e-01 err=0.30]\n",
            "[i=137642 wall=414] [train aL=3.21e-02 err=0.01 nd=465/10000] [test aL=9.54e-01 err=0.30]\n",
            "[i=138642 wall=417] [train aL=3.10e-02 err=0.01 nd=450/10000] [test aL=9.43e-01 err=0.29]\n",
            "[i=139642 wall=419] [train aL=2.96e-02 err=0.01 nd=432/10000] [test aL=9.66e-01 err=0.29]\n",
            "[i=140642 wall=422] [train aL=2.86e-02 err=0.01 nd=386/10000] [test aL=9.73e-01 err=0.29]\n",
            "[i=141642 wall=425] [train aL=3.08e-02 err=0.01 nd=557/10000] [test aL=9.76e-01 err=0.30]\n",
            "[i=142642 wall=428] [train aL=2.87e-02 err=0.01 nd=398/10000] [test aL=1.00e+00 err=0.29]\n",
            "[i=143642 wall=431] [train aL=2.46e-02 err=0.00 nd=498/10000] [test aL=1.00e+00 err=0.30]\n",
            "[i=144642 wall=434] [train aL=2.32e-02 err=0.00 nd=396/10000] [test aL=1.01e+00 err=0.29]\n",
            "[i=145642 wall=436] [train aL=2.22e-02 err=0.00 nd=373/10000] [test aL=1.03e+00 err=0.29]\n",
            "[i=146642 wall=440] [train aL=2.11e-02 err=0.00 nd=345/10000] [test aL=1.03e+00 err=0.30]\n",
            "[i=147642 wall=443] [train aL=1.89e-02 err=0.00 nd=440/10000] [test aL=1.04e+00 err=0.30]\n",
            "[i=148642 wall=446] [train aL=1.71e-02 err=0.00 nd=343/10000] [test aL=1.06e+00 err=0.30]\n",
            "[i=149642 wall=449] [train aL=1.73e-02 err=0.00 nd=334/10000] [test aL=1.09e+00 err=0.30]\n",
            "[i=150642 wall=452] [train aL=1.50e-02 err=0.00 nd=315/10000] [test aL=1.09e+00 err=0.29]\n",
            "[i=151642 wall=455] [train aL=1.30e-02 err=0.00 nd=344/10000] [test aL=1.09e+00 err=0.30]\n",
            "[i=152642 wall=457] [train aL=1.19e-02 err=0.00 nd=300/10000] [test aL=1.12e+00 err=0.30]\n",
            "[i=153642 wall=460] [train aL=1.00e-02 err=0.00 nd=266/10000] [test aL=1.12e+00 err=0.30]\n",
            "[i=154642 wall=463] [train aL=8.96e-03 err=0.00 nd=332/10000] [test aL=1.13e+00 err=0.30]\n",
            "[i=155642 wall=466] [train aL=8.38e-03 err=0.00 nd=315/10000] [test aL=1.12e+00 err=0.30]\n",
            "[i=156642 wall=469] [train aL=9.35e-03 err=0.00 nd=255/10000] [test aL=1.18e+00 err=0.30]\n",
            "[i=157642 wall=472] [train aL=6.35e-03 err=0.00 nd=284/10000] [test aL=1.16e+00 err=0.30]\n",
            "[i=158642 wall=475] [train aL=5.24e-03 err=0.00 nd=277/10000] [test aL=1.16e+00 err=0.30]\n",
            "[i=159642 wall=478] [train aL=3.94e-03 err=0.00 nd=144/10000] [test aL=1.17e+00 err=0.30]\n",
            "[i=160642 wall=480] [train aL=5.18e-03 err=0.00 nd=190/10000] [test aL=1.21e+00 err=0.30]\n",
            "[i=161642 wall=483] [train aL=3.83e-03 err=0.00 nd=154/10000] [test aL=1.19e+00 err=0.30]\n",
            "[i=162642 wall=486] [train aL=4.85e-03 err=0.00 nd=166/10000] [test aL=1.21e+00 err=0.30]\n",
            "[i=163642 wall=489] [train aL=2.23e-03 err=0.00 nd=149/10000] [test aL=1.20e+00 err=0.30]\n",
            "[i=164642 wall=492] [train aL=3.88e-03 err=0.00 nd=151/10000] [test aL=1.22e+00 err=0.30]\n",
            "[i=165642 wall=495] [train aL=2.87e-03 err=0.00 nd=102/10000] [test aL=1.23e+00 err=0.30]\n",
            "[i=166642 wall=497] [train aL=1.54e-03 err=0.00 nd=95/10000] [test aL=1.24e+00 err=0.30]\n",
            "[i=167642 wall=501] [train aL=2.51e-03 err=0.00 nd=120/10000] [test aL=1.24e+00 err=0.30]\n",
            "[i=168642 wall=503] [train aL=1.56e-03 err=0.00 nd=78/10000] [test aL=1.26e+00 err=0.30]\n",
            "[i=169642 wall=506] [train aL=1.64e-03 err=0.00 nd=81/10000] [test aL=1.26e+00 err=0.30]\n",
            "[i=170642 wall=509] [train aL=1.24e-03 err=0.00 nd=49/10000] [test aL=1.27e+00 err=0.30]\n",
            "[i=171642 wall=512] [train aL=1.30e-03 err=0.00 nd=54/10000] [test aL=1.27e+00 err=0.30]\n",
            "[i=172642 wall=515] [train aL=1.54e-03 err=0.00 nd=56/10000] [test aL=1.26e+00 err=0.30]\n",
            "[i=173642 wall=519] [train aL=1.25e-03 err=0.00 nd=65/10000] [test aL=1.28e+00 err=0.30]\n",
            "[i=174642 wall=523] [train aL=1.63e-03 err=0.00 nd=77/10000] [test aL=1.30e+00 err=0.30]\n",
            "[i=175642 wall=526] [train aL=2.15e-03 err=0.00 nd=70/10000] [test aL=1.32e+00 err=0.29]\n",
            "[i=176642 wall=529] [train aL=7.00e-04 err=0.00 nd=31/10000] [test aL=1.31e+00 err=0.30]\n",
            "[i=177642 wall=531] [train aL=8.47e-04 err=0.00 nd=52/10000] [test aL=1.28e+00 err=0.30]\n",
            "[i=178642 wall=534] [train aL=1.11e-03 err=0.00 nd=39/10000] [test aL=1.31e+00 err=0.30]\n",
            "[i=179642 wall=537] [train aL=8.82e-04 err=0.00 nd=35/10000] [test aL=1.33e+00 err=0.30]\n",
            "[i=180642 wall=540] [train aL=5.19e-04 err=0.00 nd=34/10000] [test aL=1.30e+00 err=0.30]\n",
            "[i=181642 wall=543] [train aL=1.80e-02 err=0.01 nd=167/10000] [test aL=1.37e+00 err=0.30]\n",
            "[i=182642 wall=546] [train aL=5.72e-04 err=0.00 nd=37/10000] [test aL=1.33e+00 err=0.30]\n",
            "[i=183642 wall=549] [train aL=7.07e-04 err=0.00 nd=20/10000] [test aL=1.33e+00 err=0.30]\n",
            "[i=184642 wall=552] [train aL=9.72e-04 err=0.00 nd=59/10000] [test aL=1.34e+00 err=0.30]\n",
            "[i=185642 wall=554] [train aL=4.86e-04 err=0.00 nd=34/10000] [test aL=1.34e+00 err=0.30]\n",
            "[i=186642 wall=557] [train aL=2.61e-03 err=0.00 nd=59/10000] [test aL=1.35e+00 err=0.30]\n",
            "[i=187642 wall=560] [train aL=2.99e-04 err=0.00 nd=24/10000] [test aL=1.33e+00 err=0.30]\n",
            "[i=188642 wall=563] [train aL=8.74e-04 err=0.00 nd=31/10000] [test aL=1.36e+00 err=0.30]\n",
            "[i=189642 wall=566] [train aL=6.27e-04 err=0.00 nd=28/10000] [test aL=1.34e+00 err=0.30]\n",
            "[i=190642 wall=568] [train aL=1.93e-04 err=0.00 nd=15/10000] [test aL=1.36e+00 err=0.30]\n",
            "[i=191642 wall=571] [train aL=2.96e-04 err=0.00 nd=12/10000] [test aL=1.37e+00 err=0.30]\n",
            "[i=192642 wall=574] [train aL=3.25e-04 err=0.00 nd=20/10000] [test aL=1.35e+00 err=0.31]\n",
            "[i=193642 wall=577] [train aL=1.83e-04 err=0.00 nd=17/10000] [test aL=1.36e+00 err=0.30]\n",
            "[i=194642 wall=580] [train aL=2.61e-03 err=0.00 nd=59/10000] [test aL=1.36e+00 err=0.30]\n",
            "[i=195642 wall=583] [train aL=3.60e-04 err=0.00 nd=26/10000] [test aL=1.36e+00 err=0.30]\n",
            "[i=196642 wall=586] [train aL=6.77e-04 err=0.00 nd=18/10000] [test aL=1.38e+00 err=0.30]\n",
            "[i=197642 wall=589] [train aL=4.44e-04 err=0.00 nd=26/10000] [test aL=1.41e+00 err=0.30]\n",
            "[i=198642 wall=591] [train aL=2.06e-04 err=0.00 nd=21/10000] [test aL=1.38e+00 err=0.30]\n",
            "[i=199642 wall=594] [train aL=8.72e-03 err=0.00 nd=101/10000] [test aL=1.38e+00 err=0.29]\n",
            "[i=200642 wall=597] [train aL=4.32e-04 err=0.00 nd=35/10000] [test aL=1.37e+00 err=0.30]\n",
            "[i=201642 wall=600] [train aL=6.53e-04 err=0.00 nd=35/10000] [test aL=1.40e+00 err=0.30]\n",
            "[i=202642 wall=603] [train aL=2.68e-04 err=0.00 nd=17/10000] [test aL=1.40e+00 err=0.30]\n",
            "[i=203642 wall=606] [train aL=3.68e-04 err=0.00 nd=13/10000] [test aL=1.40e+00 err=0.30]\n",
            "[i=204642 wall=609] [train aL=4.07e-04 err=0.00 nd=12/10000] [test aL=1.41e+00 err=0.30]\n",
            "[i=205642 wall=612] [train aL=2.32e-04 err=0.00 nd=15/10000] [test aL=1.42e+00 err=0.30]\n",
            "[i=206642 wall=615] [train aL=9.28e-05 err=0.00 nd=11/10000] [test aL=1.41e+00 err=0.30]\n",
            "[i=207642 wall=617] [train aL=5.88e-04 err=0.00 nd=39/10000] [test aL=1.43e+00 err=0.30]\n",
            "[i=208642 wall=620] [train aL=3.84e-04 err=0.00 nd=22/10000] [test aL=1.42e+00 err=0.30]\n",
            "[i=209642 wall=624] [train aL=7.08e-04 err=0.00 nd=21/10000] [test aL=1.41e+00 err=0.30]\n",
            "[i=210642 wall=626] [train aL=8.84e-05 err=0.00 nd=5/10000] [test aL=1.40e+00 err=0.30]\n",
            "[i=211642 wall=629] [train aL=1.18e-04 err=0.00 nd=12/10000] [test aL=1.42e+00 err=0.30]\n",
            "[i=212642 wall=632] [train aL=3.74e-04 err=0.00 nd=15/10000] [test aL=1.44e+00 err=0.30]\n",
            "[i=213642 wall=635] [train aL=1.66e-04 err=0.00 nd=9/10000] [test aL=1.43e+00 err=0.31]\n",
            "[i=214642 wall=638] [train aL=7.61e-04 err=0.00 nd=13/10000] [test aL=1.43e+00 err=0.30]\n",
            "[i=215642 wall=640] [train aL=3.91e-04 err=0.00 nd=15/10000] [test aL=1.43e+00 err=0.30]\n",
            "[i=216642 wall=643] [train aL=1.07e-03 err=0.00 nd=40/10000] [test aL=1.44e+00 err=0.30]\n",
            "[i=217642 wall=647] [train aL=3.08e-04 err=0.00 nd=24/10000] [test aL=1.41e+00 err=0.30]\n",
            "[i=218642 wall=649] [train aL=3.85e-05 err=0.00 nd=6/10000] [test aL=1.43e+00 err=0.30]\n",
            "[i=219642 wall=652] [train aL=1.26e-04 err=0.00 nd=7/10000] [test aL=1.44e+00 err=0.30]\n",
            "[i=220642 wall=655] [train aL=1.16e-02 err=0.00 nd=89/10000] [test aL=1.45e+00 err=0.30]\n",
            "[i=221642 wall=658] [train aL=8.38e-04 err=0.00 nd=22/10000] [test aL=1.45e+00 err=0.30]\n",
            "[i=222642 wall=661] [train aL=4.11e-04 err=0.00 nd=17/10000] [test aL=1.45e+00 err=0.30]\n",
            "[i=223642 wall=664] [train aL=2.30e-04 err=0.00 nd=10/10000] [test aL=1.45e+00 err=0.30]\n",
            "[i=224642 wall=666] [train aL=1.53e-04 err=0.00 nd=8/10000] [test aL=1.46e+00 err=0.30]\n",
            "[i=225642 wall=669] [train aL=3.78e-04 err=0.00 nd=26/10000] [test aL=1.47e+00 err=0.30]\n",
            "[i=226642 wall=672] [train aL=9.64e-04 err=0.00 nd=17/10000] [test aL=1.44e+00 err=0.30]\n",
            "[i=227642 wall=675] [train aL=1.03e-04 err=0.00 nd=11/10000] [test aL=1.46e+00 err=0.30]\n",
            "[i=228642 wall=678] [train aL=1.74e-04 err=0.00 nd=12/10000] [test aL=1.49e+00 err=0.30]\n",
            "[i=229642 wall=681] [train aL=2.78e-03 err=0.00 nd=43/10000] [test aL=1.48e+00 err=0.30]\n",
            "[i=230642 wall=684] [train aL=3.62e-04 err=0.00 nd=17/10000] [test aL=1.48e+00 err=0.30]\n",
            "[i=231642 wall=687] [train aL=2.92e-04 err=0.00 nd=7/10000] [test aL=1.47e+00 err=0.30]\n",
            "[i=232642 wall=691] [train aL=5.60e-05 err=0.00 nd=1/10000] [test aL=1.46e+00 err=0.30]\n",
            "[i=233642 wall=695] [train aL=3.38e-04 err=0.00 nd=10/10000] [test aL=1.49e+00 err=0.30]\n",
            "[i=234642 wall=699] [train aL=3.61e-04 err=0.00 nd=19/10000] [test aL=1.47e+00 err=0.30]\n",
            "[i=235642 wall=701] [train aL=1.81e-04 err=0.00 nd=7/10000] [test aL=1.46e+00 err=0.30]\n",
            "[i=236642 wall=704] [train aL=9.71e-05 err=0.00 nd=6/10000] [test aL=1.49e+00 err=0.30]\n",
            "[i=237642 wall=709] [train aL=1.20e-03 err=0.00 nd=47/10000] [test aL=1.49e+00 err=0.30]\n",
            "[i=238642 wall=714] [train aL=4.10e-05 err=0.00 nd=6/10000] [test aL=1.47e+00 err=0.30]\n",
            "[i=239642 wall=716] [train aL=8.86e-05 err=0.00 nd=6/10000] [test aL=1.47e+00 err=0.30]\n",
            "[i=240642 wall=720] [train aL=1.30e-04 err=0.00 nd=7/10000] [test aL=1.50e+00 err=0.30]\n",
            "[i=241642 wall=723] [train aL=1.07e-04 err=0.00 nd=5/10000] [test aL=1.48e+00 err=0.30]\n",
            "[i=242642 wall=727] [train aL=1.62e-05 err=0.00 nd=5/10000] [test aL=1.46e+00 err=0.30]\n",
            "[i=243642 wall=730] [train aL=1.88e-05 err=0.00 nd=2/10000] [test aL=1.49e+00 err=0.30]\n",
            "[i=244642 wall=733] [train aL=1.86e-03 err=0.00 nd=51/10000] [test aL=1.51e+00 err=0.30]\n",
            "[i=245642 wall=736] [train aL=4.20e-04 err=0.00 nd=14/10000] [test aL=1.51e+00 err=0.30]\n",
            "[i=246642 wall=739] [train aL=1.70e-03 err=0.00 nd=30/10000] [test aL=1.51e+00 err=0.30]\n",
            "[i=247642 wall=741] [train aL=1.18e-04 err=0.00 nd=3/10000] [test aL=1.51e+00 err=0.30]\n",
            "[i=248642 wall=745] [train aL=3.42e-05 err=0.00 nd=6/10000] [test aL=1.48e+00 err=0.30]\n",
            "[i=249642 wall=747] [train aL=9.41e-05 err=0.00 nd=10/10000] [test aL=1.49e+00 err=0.30]\n",
            "[i=250642 wall=750] [train aL=9.28e-05 err=0.00 nd=6/10000] [test aL=1.50e+00 err=0.30]\n",
            "[i=251642 wall=753] [train aL=8.81e-06 err=0.00 nd=3/10000] [test aL=1.49e+00 err=0.30]\n",
            "[i=252642 wall=756] [train aL=0.00e+00 err=0.00 nd=0/10000] [test aL=1.50e+00 err=0.30]\n",
            "Results saved to results/experiment_fc_softplus_0.001_hinge_100_seed0.pt\n",
            "[i=0 wall=0] [train aL=1.00e+00 err=0.47 nd=10000/10000] [test aL=1.00e+00 err=0.47]\n",
            "[i=1 wall=0] [train aL=1.00e+00 err=0.45 nd=10000/10000] [test aL=1.00e+00 err=0.45]\n",
            "[i=2 wall=0] [train aL=1.00e+00 err=0.44 nd=10000/10000] [test aL=1.00e+00 err=0.45]\n",
            "[i=3 wall=0] [train aL=1.00e+00 err=0.45 nd=10000/10000] [test aL=1.00e+00 err=0.46]\n",
            "[i=4 wall=0] [train aL=1.00e+00 err=0.45 nd=10000/10000] [test aL=1.00e+00 err=0.46]\n",
            "[i=5 wall=1] [train aL=1.00e+00 err=0.45 nd=10000/10000] [test aL=1.00e+00 err=0.46]\n",
            "[i=6 wall=1] [train aL=1.00e+00 err=0.45 nd=10000/10000] [test aL=1.00e+00 err=0.45]\n",
            "[i=7 wall=1] [train aL=1.00e+00 err=0.44 nd=10000/10000] [test aL=1.00e+00 err=0.45]\n",
            "[i=8 wall=1] [train aL=1.00e+00 err=0.44 nd=10000/10000] [test aL=1.00e+00 err=0.44]\n",
            "[i=9 wall=1] [train aL=1.00e+00 err=0.44 nd=10000/10000] [test aL=1.00e+00 err=0.44]\n",
            "[i=10 wall=1] [train aL=1.00e+00 err=0.44 nd=10000/10000] [test aL=1.00e+00 err=0.44]\n",
            "[i=11 wall=1] [train aL=1.00e+00 err=0.43 nd=10000/10000] [test aL=1.00e+00 err=0.44]\n",
            "[i=13 wall=1] [train aL=1.00e+00 err=0.43 nd=10000/10000] [test aL=1.00e+00 err=0.43]\n",
            "[i=15 wall=1] [train aL=1.00e+00 err=0.42 nd=10000/10000] [test aL=1.00e+00 err=0.42]\n",
            "[i=17 wall=1] [train aL=1.00e+00 err=0.41 nd=10000/10000] [test aL=1.00e+00 err=0.41]\n",
            "[i=19 wall=1] [train aL=1.00e+00 err=0.40 nd=10000/10000] [test aL=1.00e+00 err=0.40]\n",
            "[i=21 wall=2] [train aL=1.00e+00 err=0.39 nd=10000/10000] [test aL=1.00e+00 err=0.39]\n",
            "[i=24 wall=2] [train aL=1.00e+00 err=0.38 nd=10000/10000] [test aL=1.00e+00 err=0.39]\n",
            "[i=27 wall=2] [train aL=1.00e+00 err=0.38 nd=10000/10000] [test aL=1.00e+00 err=0.38]\n",
            "[i=30 wall=2] [train aL=1.00e+00 err=0.37 nd=10000/10000] [test aL=1.00e+00 err=0.38]\n",
            "[i=33 wall=2] [train aL=1.00e+00 err=0.36 nd=10000/10000] [test aL=1.00e+00 err=0.37]\n",
            "[i=37 wall=2] [train aL=1.00e+00 err=0.36 nd=10000/10000] [test aL=1.00e+00 err=0.36]\n",
            "[i=41 wall=2] [train aL=1.00e+00 err=0.35 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=46 wall=2] [train aL=1.00e+00 err=0.35 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=51 wall=2] [train aL=1.00e+00 err=0.35 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=57 wall=2] [train aL=1.00e+00 err=0.34 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=63 wall=3] [train aL=1.00e+00 err=0.35 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=70 wall=3] [train aL=1.00e+00 err=0.35 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=77 wall=3] [train aL=1.00e+00 err=0.35 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=85 wall=3] [train aL=1.00e+00 err=0.35 nd=10000/10000] [test aL=1.00e+00 err=0.36]\n",
            "[i=94 wall=3] [train aL=1.00e+00 err=0.35 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=104 wall=3] [train aL=1.00e+00 err=0.35 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=115 wall=3] [train aL=1.00e+00 err=0.34 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=127 wall=3] [train aL=1.00e+00 err=0.35 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=140 wall=3] [train aL=1.00e+00 err=0.35 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=154 wall=4] [train aL=1.00e+00 err=0.35 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=170 wall=4] [train aL=1.00e+00 err=0.35 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=187 wall=4] [train aL=1.00e+00 err=0.34 nd=10000/10000] [test aL=1.00e+00 err=0.34]\n",
            "[i=206 wall=4] [train aL=9.99e-01 err=0.35 nd=10000/10000] [test aL=9.99e-01 err=0.35]\n",
            "[i=227 wall=4] [train aL=9.99e-01 err=0.36 nd=10000/10000] [test aL=9.99e-01 err=0.36]\n",
            "[i=250 wall=4] [train aL=9.99e-01 err=0.37 nd=10000/10000] [test aL=9.99e-01 err=0.38]\n",
            "[i=275 wall=4] [train aL=9.98e-01 err=0.38 nd=10000/10000] [test aL=9.99e-01 err=0.38]\n",
            "[i=303 wall=5] [train aL=9.98e-01 err=0.40 nd=10000/10000] [test aL=9.98e-01 err=0.40]\n",
            "[i=333 wall=5] [train aL=9.97e-01 err=0.40 nd=10000/10000] [test aL=9.97e-01 err=0.40]\n",
            "[i=366 wall=5] [train aL=9.95e-01 err=0.38 nd=10000/10000] [test aL=9.95e-01 err=0.38]\n",
            "[i=402 wall=5] [train aL=9.93e-01 err=0.38 nd=10000/10000] [test aL=9.93e-01 err=0.38]\n",
            "[i=442 wall=5] [train aL=9.89e-01 err=0.36 nd=10000/10000] [test aL=9.89e-01 err=0.36]\n",
            "[i=486 wall=6] [train aL=9.83e-01 err=0.36 nd=10000/10000] [test aL=9.83e-01 err=0.36]\n",
            "[i=534 wall=6] [train aL=9.75e-01 err=0.36 nd=10000/10000] [test aL=9.76e-01 err=0.36]\n",
            "[i=586 wall=6] [train aL=9.60e-01 err=0.37 nd=10000/10000] [test aL=9.61e-01 err=0.37]\n",
            "[i=643 wall=6] [train aL=9.42e-01 err=0.37 nd=10000/10000] [test aL=9.44e-01 err=0.37]\n",
            "[i=706 wall=6] [train aL=9.12e-01 err=0.37 nd=9999/10000] [test aL=9.14e-01 err=0.37]\n",
            "[i=775 wall=7] [train aL=8.77e-01 err=0.38 nd=9433/10000] [test aL=8.79e-01 err=0.38]\n",
            "[i=850 wall=7] [train aL=8.52e-01 err=0.37 nd=9365/10000] [test aL=8.56e-01 err=0.37]\n",
            "[i=932 wall=7] [train aL=8.23e-01 err=0.36 nd=9213/10000] [test aL=8.29e-01 err=0.36]\n",
            "[i=1021 wall=8] [train aL=7.85e-01 err=0.34 nd=9188/10000] [test aL=7.94e-01 err=0.34]\n",
            "[i=1119 wall=8] [train aL=7.49e-01 err=0.33 nd=8643/10000] [test aL=7.59e-01 err=0.33]\n",
            "[i=1225 wall=8] [train aL=7.26e-01 err=0.32 nd=8183/10000] [test aL=7.40e-01 err=0.32]\n",
            "[i=1341 wall=9] [train aL=7.07e-01 err=0.31 nd=7869/10000] [test aL=7.24e-01 err=0.32]\n",
            "[i=1467 wall=9] [train aL=6.90e-01 err=0.30 nd=7786/10000] [test aL=7.10e-01 err=0.31]\n",
            "[i=1604 wall=10] [train aL=6.76e-01 err=0.30 nd=7423/10000] [test aL=6.99e-01 err=0.32]\n",
            "[i=1753 wall=11] [train aL=6.66e-01 err=0.30 nd=7257/10000] [test aL=6.92e-01 err=0.31]\n",
            "[i=1914 wall=11] [train aL=6.57e-01 err=0.29 nd=6998/10000] [test aL=6.85e-01 err=0.31]\n",
            "[i=2089 wall=12] [train aL=6.49e-01 err=0.29 nd=6819/10000] [test aL=6.79e-01 err=0.31]\n",
            "[i=2278 wall=12] [train aL=6.42e-01 err=0.28 nd=6666/10000] [test aL=6.74e-01 err=0.30]\n",
            "[i=2482 wall=13] [train aL=6.35e-01 err=0.28 nd=6638/10000] [test aL=6.68e-01 err=0.30]\n",
            "[i=2702 wall=14] [train aL=6.28e-01 err=0.28 nd=6571/10000] [test aL=6.62e-01 err=0.30]\n",
            "[i=2939 wall=14] [train aL=6.20e-01 err=0.27 nd=6459/10000] [test aL=6.56e-01 err=0.29]\n",
            "[i=3194 wall=15] [train aL=6.13e-01 err=0.27 nd=6382/10000] [test aL=6.50e-01 err=0.29]\n",
            "[i=3468 wall=16] [train aL=6.06e-01 err=0.27 nd=6301/10000] [test aL=6.45e-01 err=0.29]\n",
            "[i=3762 wall=17] [train aL=6.01e-01 err=0.27 nd=6171/10000] [test aL=6.42e-01 err=0.29]\n",
            "[i=4076 wall=18] [train aL=5.94e-01 err=0.26 nd=6136/10000] [test aL=6.38e-01 err=0.29]\n",
            "[i=4411 wall=19] [train aL=5.86e-01 err=0.26 nd=6114/10000] [test aL=6.33e-01 err=0.28]\n",
            "[i=4768 wall=20] [train aL=5.79e-01 err=0.25 nd=6082/10000] [test aL=6.28e-01 err=0.28]\n",
            "[i=5148 wall=21] [train aL=5.72e-01 err=0.25 nd=5988/10000] [test aL=6.24e-01 err=0.28]\n",
            "[i=5551 wall=24] [train aL=5.65e-01 err=0.25 nd=5927/10000] [test aL=6.20e-01 err=0.28]\n",
            "[i=5977 wall=25] [train aL=5.59e-01 err=0.25 nd=5744/10000] [test aL=6.18e-01 err=0.28]\n",
            "[i=6427 wall=26] [train aL=5.53e-01 err=0.24 nd=5698/10000] [test aL=6.16e-01 err=0.27]\n",
            "[i=6902 wall=28] [train aL=5.46e-01 err=0.24 nd=5626/10000] [test aL=6.12e-01 err=0.28]\n",
            "[i=7401 wall=29] [train aL=5.40e-01 err=0.24 nd=5499/10000] [test aL=6.10e-01 err=0.28]\n",
            "[i=7924 wall=31] [train aL=5.33e-01 err=0.24 nd=5403/10000] [test aL=6.08e-01 err=0.27]\n",
            "[i=8472 wall=32] [train aL=5.28e-01 err=0.23 nd=5393/10000] [test aL=6.05e-01 err=0.27]\n",
            "[i=9044 wall=34] [train aL=5.20e-01 err=0.23 nd=5278/10000] [test aL=6.02e-01 err=0.27]\n",
            "[i=9640 wall=36] [train aL=5.13e-01 err=0.23 nd=5151/10000] [test aL=6.01e-01 err=0.27]\n",
            "[i=10259 wall=38] [train aL=5.06e-01 err=0.22 nd=5143/10000] [test aL=6.00e-01 err=0.27]\n",
            "[i=10901 wall=40] [train aL=4.98e-01 err=0.22 nd=5114/10000] [test aL=5.97e-01 err=0.27]\n",
            "[i=11565 wall=41] [train aL=4.91e-01 err=0.22 nd=5170/10000] [test aL=5.95e-01 err=0.27]\n",
            "[i=12251 wall=43] [train aL=4.82e-01 err=0.21 nd=4979/10000] [test aL=5.93e-01 err=0.27]\n",
            "[i=12958 wall=45] [train aL=4.74e-01 err=0.21 nd=4842/10000] [test aL=5.92e-01 err=0.27]\n",
            "[i=13685 wall=48] [train aL=4.65e-01 err=0.20 nd=4821/10000] [test aL=5.91e-01 err=0.27]\n",
            "[i=14431 wall=50] [train aL=4.55e-01 err=0.20 nd=4670/10000] [test aL=5.91e-01 err=0.27]\n",
            "[i=15195 wall=52] [train aL=4.46e-01 err=0.19 nd=4619/10000] [test aL=5.89e-01 err=0.27]\n",
            "[i=15977 wall=54] [train aL=4.38e-01 err=0.19 nd=4581/10000] [test aL=5.89e-01 err=0.27]\n",
            "[i=16775 wall=56] [train aL=4.28e-01 err=0.18 nd=4472/10000] [test aL=5.88e-01 err=0.27]\n",
            "[i=17589 wall=59] [train aL=4.19e-01 err=0.18 nd=4376/10000] [test aL=5.89e-01 err=0.27]\n",
            "[i=18417 wall=61] [train aL=4.08e-01 err=0.17 nd=4315/10000] [test aL=5.88e-01 err=0.27]\n",
            "[i=19259 wall=64] [train aL=4.00e-01 err=0.17 nd=4259/10000] [test aL=5.86e-01 err=0.27]\n",
            "[i=20114 wall=66] [train aL=3.90e-01 err=0.17 nd=4115/10000] [test aL=5.88e-01 err=0.27]\n",
            "[i=20981 wall=68] [train aL=3.80e-01 err=0.16 nd=3939/10000] [test aL=5.89e-01 err=0.27]\n",
            "[i=21859 wall=71] [train aL=3.70e-01 err=0.16 nd=3978/10000] [test aL=5.89e-01 err=0.27]\n",
            "[i=22747 wall=75] [train aL=3.62e-01 err=0.15 nd=3770/10000] [test aL=5.92e-01 err=0.27]\n",
            "[i=23645 wall=77] [train aL=3.54e-01 err=0.15 nd=3860/10000] [test aL=5.95e-01 err=0.27]\n",
            "[i=24552 wall=80] [train aL=3.40e-01 err=0.14 nd=3565/10000] [test aL=5.95e-01 err=0.27]\n",
            "[i=25467 wall=82] [train aL=3.32e-01 err=0.14 nd=3528/10000] [test aL=5.96e-01 err=0.27]\n",
            "[i=26389 wall=85] [train aL=3.23e-01 err=0.13 nd=3421/10000] [test aL=5.99e-01 err=0.27]\n",
            "[i=27318 wall=88] [train aL=3.12e-01 err=0.13 nd=3286/10000] [test aL=6.00e-01 err=0.27]\n",
            "[i=28253 wall=90] [train aL=3.05e-01 err=0.13 nd=3318/10000] [test aL=6.02e-01 err=0.27]\n",
            "[i=29194 wall=93] [train aL=2.93e-01 err=0.12 nd=3267/10000] [test aL=6.03e-01 err=0.27]\n",
            "[i=30141 wall=96] [train aL=2.85e-01 err=0.12 nd=3085/10000] [test aL=6.06e-01 err=0.27]\n",
            "[i=31092 wall=99] [train aL=2.73e-01 err=0.11 nd=2901/10000] [test aL=6.07e-01 err=0.27]\n",
            "[i=32048 wall=101] [train aL=2.64e-01 err=0.11 nd=2881/10000] [test aL=6.09e-01 err=0.27]\n",
            "[i=33008 wall=104] [train aL=2.55e-01 err=0.11 nd=2783/10000] [test aL=6.12e-01 err=0.28]\n",
            "[i=33972 wall=107] [train aL=2.50e-01 err=0.10 nd=2761/10000] [test aL=6.12e-01 err=0.28]\n",
            "[i=34939 wall=110] [train aL=2.39e-01 err=0.10 nd=2549/10000] [test aL=6.17e-01 err=0.28]\n",
            "[i=35909 wall=112] [train aL=2.32e-01 err=0.10 nd=2638/10000] [test aL=6.15e-01 err=0.28]\n",
            "[i=36882 wall=115] [train aL=2.21e-01 err=0.09 nd=2388/10000] [test aL=6.16e-01 err=0.28]\n",
            "[i=37857 wall=118] [train aL=2.17e-01 err=0.09 nd=2333/10000] [test aL=6.17e-01 err=0.28]\n",
            "[i=38835 wall=121] [train aL=2.07e-01 err=0.09 nd=2123/10000] [test aL=6.17e-01 err=0.28]\n",
            "[i=39815 wall=124] [train aL=1.99e-01 err=0.09 nd=2130/10000] [test aL=6.20e-01 err=0.28]\n",
            "[i=40797 wall=126] [train aL=1.92e-01 err=0.08 nd=2040/10000] [test aL=6.18e-01 err=0.28]\n",
            "[i=41781 wall=129] [train aL=1.89e-01 err=0.08 nd=2076/10000] [test aL=6.21e-01 err=0.28]\n",
            "[i=42766 wall=132] [train aL=1.80e-01 err=0.08 nd=1937/10000] [test aL=6.23e-01 err=0.28]\n",
            "[i=43753 wall=135] [train aL=1.74e-01 err=0.07 nd=1856/10000] [test aL=6.21e-01 err=0.28]\n",
            "[i=44741 wall=138] [train aL=1.66e-01 err=0.07 nd=1767/10000] [test aL=6.21e-01 err=0.28]\n",
            "[i=45730 wall=140] [train aL=1.61e-01 err=0.07 nd=1674/10000] [test aL=6.25e-01 err=0.28]\n",
            "[i=46720 wall=143] [train aL=1.58e-01 err=0.07 nd=1729/10000] [test aL=6.24e-01 err=0.28]\n",
            "[i=47711 wall=146] [train aL=1.54e-01 err=0.07 nd=1594/10000] [test aL=6.26e-01 err=0.28]\n",
            "[i=48703 wall=149] [train aL=1.49e-01 err=0.06 nd=1619/10000] [test aL=6.27e-01 err=0.28]\n",
            "[i=49696 wall=152] [train aL=1.41e-01 err=0.06 nd=1451/10000] [test aL=6.28e-01 err=0.28]\n",
            "[i=50690 wall=155] [train aL=1.40e-01 err=0.06 nd=1560/10000] [test aL=6.29e-01 err=0.28]\n",
            "[i=51684 wall=158] [train aL=1.34e-01 err=0.06 nd=1459/10000] [test aL=6.32e-01 err=0.28]\n",
            "[i=52679 wall=161] [train aL=1.32e-01 err=0.06 nd=1406/10000] [test aL=6.33e-01 err=0.28]\n",
            "[i=53674 wall=163] [train aL=1.34e-01 err=0.06 nd=1600/10000] [test aL=6.32e-01 err=0.28]\n",
            "[i=54670 wall=166] [train aL=1.29e-01 err=0.06 nd=1435/10000] [test aL=6.39e-01 err=0.29]\n",
            "[i=55666 wall=169] [train aL=1.23e-01 err=0.05 nd=1334/10000] [test aL=6.33e-01 err=0.29]\n",
            "[i=56663 wall=172] [train aL=1.18e-01 err=0.05 nd=1318/10000] [test aL=6.34e-01 err=0.29]\n",
            "[i=57660 wall=175] [train aL=1.19e-01 err=0.05 nd=1406/10000] [test aL=6.39e-01 err=0.29]\n",
            "[i=58657 wall=177] [train aL=1.12e-01 err=0.05 nd=1277/10000] [test aL=6.35e-01 err=0.29]\n",
            "[i=59655 wall=180] [train aL=1.09e-01 err=0.05 nd=1122/10000] [test aL=6.34e-01 err=0.29]\n",
            "[i=60653 wall=183] [train aL=1.07e-01 err=0.05 nd=1266/10000] [test aL=6.36e-01 err=0.29]\n",
            "[i=61651 wall=186] [train aL=1.06e-01 err=0.05 nd=1207/10000] [test aL=6.40e-01 err=0.29]\n",
            "[i=62649 wall=189] [train aL=1.01e-01 err=0.05 nd=1025/10000] [test aL=6.38e-01 err=0.29]\n",
            "[i=63648 wall=192] [train aL=1.01e-01 err=0.05 nd=1028/10000] [test aL=6.37e-01 err=0.29]\n",
            "[i=64647 wall=195] [train aL=9.77e-02 err=0.05 nd=1045/10000] [test aL=6.40e-01 err=0.29]\n",
            "[i=65646 wall=197] [train aL=9.79e-02 err=0.05 nd=1119/10000] [test aL=6.39e-01 err=0.29]\n",
            "[i=66645 wall=200] [train aL=9.37e-02 err=0.04 nd=954/10000] [test aL=6.47e-01 err=0.29]\n",
            "[i=67644 wall=203] [train aL=9.07e-02 err=0.04 nd=923/10000] [test aL=6.46e-01 err=0.29]\n",
            "[i=68643 wall=206] [train aL=9.04e-02 err=0.04 nd=1053/10000] [test aL=6.47e-01 err=0.29]\n",
            "[i=69642 wall=209] [train aL=8.78e-02 err=0.04 nd=944/10000] [test aL=6.50e-01 err=0.29]\n",
            "[i=70642 wall=212] [train aL=8.71e-02 err=0.04 nd=867/10000] [test aL=6.49e-01 err=0.29]\n",
            "[i=71642 wall=214] [train aL=8.46e-02 err=0.04 nd=829/10000] [test aL=6.50e-01 err=0.29]\n",
            "[i=72642 wall=217] [train aL=8.25e-02 err=0.04 nd=760/10000] [test aL=6.48e-01 err=0.29]\n",
            "[i=73642 wall=220] [train aL=8.55e-02 err=0.04 nd=1002/10000] [test aL=6.51e-01 err=0.29]\n",
            "[i=74642 wall=223] [train aL=7.91e-02 err=0.04 nd=708/10000] [test aL=6.53e-01 err=0.29]\n",
            "[i=75642 wall=226] [train aL=7.71e-02 err=0.04 nd=862/10000] [test aL=6.52e-01 err=0.29]\n",
            "[i=76642 wall=229] [train aL=7.49e-02 err=0.04 nd=765/10000] [test aL=6.53e-01 err=0.29]\n",
            "[i=77642 wall=232] [train aL=7.35e-02 err=0.04 nd=803/10000] [test aL=6.52e-01 err=0.29]\n",
            "[i=78642 wall=235] [train aL=7.30e-02 err=0.04 nd=896/10000] [test aL=6.54e-01 err=0.29]\n",
            "[i=79642 wall=237] [train aL=7.35e-02 err=0.04 nd=761/10000] [test aL=6.58e-01 err=0.29]\n",
            "[i=80642 wall=240] [train aL=7.01e-02 err=0.04 nd=675/10000] [test aL=6.63e-01 err=0.29]\n",
            "[i=81642 wall=243] [train aL=6.95e-02 err=0.03 nd=807/10000] [test aL=6.58e-01 err=0.29]\n",
            "[i=82642 wall=246] [train aL=6.89e-02 err=0.03 nd=625/10000] [test aL=6.58e-01 err=0.29]\n",
            "[i=83642 wall=249] [train aL=6.78e-02 err=0.03 nd=732/10000] [test aL=6.67e-01 err=0.29]\n",
            "[i=84642 wall=251] [train aL=6.68e-02 err=0.03 nd=737/10000] [test aL=6.61e-01 err=0.29]\n",
            "[i=85642 wall=255] [train aL=6.75e-02 err=0.03 nd=934/10000] [test aL=6.67e-01 err=0.29]\n",
            "[i=86642 wall=257] [train aL=6.31e-02 err=0.03 nd=680/10000] [test aL=6.60e-01 err=0.29]\n",
            "[i=87642 wall=260] [train aL=6.26e-02 err=0.03 nd=716/10000] [test aL=6.63e-01 err=0.29]\n",
            "[i=88642 wall=263] [train aL=6.46e-02 err=0.03 nd=727/10000] [test aL=6.69e-01 err=0.29]\n",
            "[i=89642 wall=266] [train aL=6.23e-02 err=0.03 nd=579/10000] [test aL=6.67e-01 err=0.29]\n",
            "[i=90642 wall=269] [train aL=6.08e-02 err=0.03 nd=639/10000] [test aL=6.66e-01 err=0.29]\n",
            "[i=91642 wall=272] [train aL=6.31e-02 err=0.03 nd=624/10000] [test aL=6.79e-01 err=0.29]\n",
            "[i=92642 wall=274] [train aL=6.07e-02 err=0.03 nd=591/10000] [test aL=6.75e-01 err=0.29]\n",
            "[i=93642 wall=277] [train aL=5.77e-02 err=0.03 nd=547/10000] [test aL=6.75e-01 err=0.29]\n",
            "[i=94642 wall=280] [train aL=5.77e-02 err=0.03 nd=652/10000] [test aL=6.73e-01 err=0.29]\n",
            "[i=95642 wall=283] [train aL=5.55e-02 err=0.03 nd=508/10000] [test aL=6.84e-01 err=0.29]\n",
            "[i=96642 wall=286] [train aL=5.54e-02 err=0.03 nd=488/10000] [test aL=6.78e-01 err=0.29]\n",
            "[i=97642 wall=289] [train aL=5.43e-02 err=0.03 nd=627/10000] [test aL=6.77e-01 err=0.29]\n",
            "[i=98642 wall=292] [train aL=5.40e-02 err=0.03 nd=491/10000] [test aL=6.78e-01 err=0.29]\n",
            "[i=99642 wall=295] [train aL=5.41e-02 err=0.03 nd=563/10000] [test aL=6.89e-01 err=0.29]\n",
            "[i=100642 wall=297] [train aL=5.27e-02 err=0.03 nd=664/10000] [test aL=6.85e-01 err=0.29]\n",
            "[i=101642 wall=300] [train aL=5.17e-02 err=0.03 nd=470/10000] [test aL=6.83e-01 err=0.29]\n",
            "[i=102642 wall=303] [train aL=5.30e-02 err=0.03 nd=591/10000] [test aL=6.95e-01 err=0.29]\n",
            "[i=103642 wall=306] [train aL=5.16e-02 err=0.03 nd=734/10000] [test aL=6.86e-01 err=0.29]\n",
            "[i=104642 wall=309] [train aL=5.06e-02 err=0.03 nd=708/10000] [test aL=6.95e-01 err=0.29]\n",
            "[i=105642 wall=311] [train aL=4.91e-02 err=0.03 nd=533/10000] [test aL=6.94e-01 err=0.29]\n",
            "[i=106642 wall=315] [train aL=4.81e-02 err=0.03 nd=545/10000] [test aL=6.93e-01 err=0.29]\n",
            "[i=107642 wall=317] [train aL=5.09e-02 err=0.03 nd=652/10000] [test aL=7.01e-01 err=0.29]\n",
            "[i=108642 wall=320] [train aL=4.71e-02 err=0.03 nd=489/10000] [test aL=7.14e-01 err=0.30]\n",
            "[i=109642 wall=323] [train aL=4.52e-02 err=0.03 nd=517/10000] [test aL=7.05e-01 err=0.29]\n",
            "[i=110642 wall=326] [train aL=4.49e-02 err=0.03 nd=410/10000] [test aL=7.18e-01 err=0.29]\n",
            "[i=111642 wall=329] [train aL=4.59e-02 err=0.03 nd=482/10000] [test aL=7.19e-01 err=0.29]\n",
            "[i=112642 wall=332] [train aL=4.43e-02 err=0.03 nd=465/10000] [test aL=7.20e-01 err=0.29]\n",
            "[i=113642 wall=334] [train aL=4.44e-02 err=0.03 nd=430/10000] [test aL=7.21e-01 err=0.29]\n",
            "[i=114642 wall=337] [train aL=4.22e-02 err=0.02 nd=468/10000] [test aL=7.27e-01 err=0.29]\n",
            "[i=115642 wall=341] [train aL=5.91e-02 err=0.03 nd=650/10000] [test aL=7.43e-01 err=0.30]\n",
            "[i=116642 wall=343] [train aL=4.19e-02 err=0.02 nd=536/10000] [test aL=7.32e-01 err=0.29]\n",
            "[i=117642 wall=346] [train aL=4.37e-02 err=0.02 nd=690/10000] [test aL=7.43e-01 err=0.30]\n",
            "[i=118642 wall=349] [train aL=4.74e-02 err=0.03 nd=564/10000] [test aL=7.50e-01 err=0.30]\n",
            "[i=119642 wall=352] [train aL=3.86e-02 err=0.02 nd=457/10000] [test aL=7.38e-01 err=0.29]\n",
            "[i=120642 wall=355] [train aL=3.75e-02 err=0.02 nd=397/10000] [test aL=7.43e-01 err=0.29]\n",
            "[i=121642 wall=357] [train aL=3.84e-02 err=0.02 nd=423/10000] [test aL=7.51e-01 err=0.29]\n",
            "[i=122642 wall=360] [train aL=3.81e-02 err=0.02 nd=524/10000] [test aL=7.70e-01 err=0.30]\n",
            "[i=123642 wall=363] [train aL=3.56e-02 err=0.02 nd=375/10000] [test aL=7.67e-01 err=0.29]\n",
            "[i=124642 wall=366] [train aL=3.43e-02 err=0.02 nd=392/10000] [test aL=7.67e-01 err=0.29]\n",
            "[i=125642 wall=369] [train aL=3.34e-02 err=0.02 nd=370/10000] [test aL=7.64e-01 err=0.29]\n",
            "[i=126642 wall=372] [train aL=3.39e-02 err=0.02 nd=407/10000] [test aL=7.71e-01 err=0.29]\n",
            "[i=127642 wall=374] [train aL=3.32e-02 err=0.02 nd=459/10000] [test aL=7.76e-01 err=0.29]\n",
            "[i=128642 wall=378] [train aL=3.28e-02 err=0.02 nd=363/10000] [test aL=7.93e-01 err=0.29]\n",
            "[i=129642 wall=380] [train aL=2.98e-02 err=0.01 nd=389/10000] [test aL=7.97e-01 err=0.29]\n",
            "[i=130642 wall=383] [train aL=2.92e-02 err=0.01 nd=320/10000] [test aL=8.03e-01 err=0.29]\n",
            "[i=131642 wall=386] [train aL=2.83e-02 err=0.01 nd=380/10000] [test aL=8.10e-01 err=0.29]\n",
            "[i=132642 wall=389] [train aL=3.22e-02 err=0.01 nd=483/10000] [test aL=8.26e-01 err=0.30]\n",
            "[i=133642 wall=392] [train aL=2.84e-02 err=0.01 nd=331/10000] [test aL=8.32e-01 err=0.29]\n",
            "[i=134642 wall=394] [train aL=2.52e-02 err=0.01 nd=325/10000] [test aL=8.38e-01 err=0.29]\n",
            "[i=135642 wall=397] [train aL=2.45e-02 err=0.01 nd=396/10000] [test aL=8.34e-01 err=0.29]\n",
            "[i=136642 wall=400] [train aL=2.44e-02 err=0.01 nd=363/10000] [test aL=8.40e-01 err=0.29]\n",
            "[i=137642 wall=403] [train aL=2.24e-02 err=0.01 nd=300/10000] [test aL=8.49e-01 err=0.29]\n",
            "[i=138642 wall=406] [train aL=2.10e-02 err=0.01 nd=359/10000] [test aL=8.63e-01 err=0.29]\n",
            "[i=139642 wall=409] [train aL=1.98e-02 err=0.01 nd=261/10000] [test aL=8.75e-01 err=0.29]\n",
            "[i=140642 wall=412] [train aL=1.88e-02 err=0.00 nd=374/10000] [test aL=8.77e-01 err=0.30]\n",
            "[i=141642 wall=415] [train aL=1.80e-02 err=0.00 nd=282/10000] [test aL=8.94e-01 err=0.29]\n",
            "[i=142642 wall=417] [train aL=1.60e-02 err=0.00 nd=324/10000] [test aL=9.12e-01 err=0.30]\n",
            "[i=143642 wall=420] [train aL=1.58e-02 err=0.00 nd=323/10000] [test aL=9.23e-01 err=0.29]\n",
            "[i=144642 wall=423] [train aL=1.28e-02 err=0.00 nd=331/10000] [test aL=9.38e-01 err=0.30]\n",
            "[i=145642 wall=426] [train aL=2.33e-02 err=0.01 nd=451/10000] [test aL=9.68e-01 err=0.30]\n",
            "[i=146642 wall=429] [train aL=9.55e-03 err=0.00 nd=270/10000] [test aL=9.75e-01 err=0.30]\n",
            "[i=147642 wall=431] [train aL=7.26e-03 err=0.00 nd=223/10000] [test aL=9.97e-01 err=0.30]\n",
            "[i=148642 wall=434] [train aL=9.50e-03 err=0.00 nd=313/10000] [test aL=1.01e+00 err=0.29]\n",
            "[i=149642 wall=437] [train aL=5.52e-03 err=0.00 nd=199/10000] [test aL=1.02e+00 err=0.30]\n",
            "[i=150642 wall=440] [train aL=3.83e-03 err=0.00 nd=143/10000] [test aL=1.03e+00 err=0.30]\n",
            "[i=151642 wall=443] [train aL=4.93e-03 err=0.00 nd=199/10000] [test aL=1.04e+00 err=0.30]\n",
            "[i=152642 wall=446] [train aL=4.35e-03 err=0.00 nd=180/10000] [test aL=1.06e+00 err=0.30]\n",
            "[i=153642 wall=449] [train aL=2.17e-03 err=0.00 nd=77/10000] [test aL=1.08e+00 err=0.30]\n",
            "[i=154642 wall=452] [train aL=3.18e-03 err=0.00 nd=86/10000] [test aL=1.09e+00 err=0.30]\n",
            "[i=155642 wall=454] [train aL=6.10e-03 err=0.00 nd=129/10000] [test aL=1.12e+00 err=0.30]\n",
            "[i=156642 wall=457] [train aL=1.92e-03 err=0.00 nd=55/10000] [test aL=1.12e+00 err=0.30]\n",
            "[i=157642 wall=460] [train aL=2.22e-03 err=0.00 nd=77/10000] [test aL=1.12e+00 err=0.30]\n",
            "[i=158642 wall=463] [train aL=1.15e-03 err=0.00 nd=43/10000] [test aL=1.11e+00 err=0.30]\n",
            "[i=159642 wall=466] [train aL=9.85e-04 err=0.00 nd=60/10000] [test aL=1.13e+00 err=0.30]\n",
            "[i=160642 wall=469] [train aL=4.26e-04 err=0.00 nd=19/10000] [test aL=1.13e+00 err=0.30]\n",
            "[i=161642 wall=471] [train aL=8.48e-04 err=0.00 nd=44/10000] [test aL=1.16e+00 err=0.30]\n",
            "[i=162642 wall=475] [train aL=9.50e-04 err=0.00 nd=28/10000] [test aL=1.15e+00 err=0.30]\n",
            "[i=163642 wall=477] [train aL=1.31e-03 err=0.00 nd=49/10000] [test aL=1.17e+00 err=0.30]\n",
            "[i=164642 wall=480] [train aL=9.74e-03 err=0.00 nd=157/10000] [test aL=1.18e+00 err=0.30]\n",
            "[i=165642 wall=483] [train aL=2.88e-03 err=0.00 nd=59/10000] [test aL=1.18e+00 err=0.30]\n",
            "[i=166642 wall=486] [train aL=1.25e-03 err=0.00 nd=47/10000] [test aL=1.19e+00 err=0.30]\n",
            "[i=167642 wall=489] [train aL=5.36e-04 err=0.00 nd=20/10000] [test aL=1.18e+00 err=0.30]\n",
            "[i=168642 wall=491] [train aL=8.52e-04 err=0.00 nd=22/10000] [test aL=1.19e+00 err=0.30]\n",
            "[i=169642 wall=494] [train aL=5.99e-05 err=0.00 nd=5/10000] [test aL=1.18e+00 err=0.30]\n",
            "[i=170642 wall=497] [train aL=9.43e-05 err=0.00 nd=12/10000] [test aL=1.18e+00 err=0.30]\n",
            "[i=171642 wall=500] [train aL=3.23e-04 err=0.00 nd=13/10000] [test aL=1.19e+00 err=0.30]\n",
            "[i=172642 wall=503] [train aL=4.48e-04 err=0.00 nd=17/10000] [test aL=1.19e+00 err=0.30]\n",
            "[i=173642 wall=506] [train aL=1.36e-03 err=0.00 nd=36/10000] [test aL=1.20e+00 err=0.30]\n",
            "[i=174642 wall=509] [train aL=1.87e-03 err=0.00 nd=44/10000] [test aL=1.21e+00 err=0.30]\n",
            "[i=175642 wall=512] [train aL=8.02e-04 err=0.00 nd=36/10000] [test aL=1.21e+00 err=0.30]\n",
            "[i=176642 wall=514] [train aL=6.66e-04 err=0.00 nd=15/10000] [test aL=1.23e+00 err=0.30]\n",
            "[i=177642 wall=517] [train aL=2.66e-04 err=0.00 nd=17/10000] [test aL=1.23e+00 err=0.30]\n",
            "[i=178642 wall=520] [train aL=1.14e-03 err=0.00 nd=41/10000] [test aL=1.24e+00 err=0.30]\n",
            "[i=179642 wall=523] [train aL=3.46e-04 err=0.00 nd=9/10000] [test aL=1.23e+00 err=0.30]\n",
            "[i=180642 wall=526] [train aL=1.53e-03 err=0.00 nd=19/10000] [test aL=1.23e+00 err=0.30]\n",
            "[i=181642 wall=528] [train aL=2.95e-04 err=0.00 nd=16/10000] [test aL=1.25e+00 err=0.30]\n",
            "[i=182642 wall=531] [train aL=4.83e-05 err=0.00 nd=2/10000] [test aL=1.24e+00 err=0.30]\n",
            "[i=183642 wall=534] [train aL=1.25e-03 err=0.00 nd=36/10000] [test aL=1.24e+00 err=0.30]\n",
            "[i=184642 wall=537] [train aL=3.91e-04 err=0.00 nd=10/10000] [test aL=1.24e+00 err=0.30]\n",
            "[i=185642 wall=540] [train aL=1.38e-02 err=0.01 nd=124/10000] [test aL=1.25e+00 err=0.30]\n",
            "[i=186642 wall=543] [train aL=3.59e-04 err=0.00 nd=7/10000] [test aL=1.25e+00 err=0.30]\n",
            "[i=187642 wall=546] [train aL=3.39e-04 err=0.00 nd=14/10000] [test aL=1.26e+00 err=0.30]\n",
            "[i=188642 wall=549] [train aL=1.62e-03 err=0.00 nd=32/10000] [test aL=1.27e+00 err=0.30]\n",
            "[i=189642 wall=551] [train aL=1.77e-05 err=0.00 nd=4/10000] [test aL=1.26e+00 err=0.30]\n",
            "[i=190642 wall=554] [train aL=1.22e-03 err=0.00 nd=30/10000] [test aL=1.26e+00 err=0.30]\n",
            "[i=191642 wall=557] [train aL=5.21e-04 err=0.00 nd=20/10000] [test aL=1.26e+00 err=0.30]\n",
            "[i=192642 wall=560] [train aL=4.60e-04 err=0.00 nd=13/10000] [test aL=1.26e+00 err=0.30]\n",
            "[i=193642 wall=563] [train aL=7.44e-04 err=0.00 nd=15/10000] [test aL=1.27e+00 err=0.30]\n",
            "[i=194642 wall=565] [train aL=1.77e-03 err=0.00 nd=36/10000] [test aL=1.28e+00 err=0.30]\n",
            "[i=195642 wall=568] [train aL=1.69e-03 err=0.00 nd=33/10000] [test aL=1.27e+00 err=0.30]\n",
            "[i=196642 wall=571] [train aL=1.25e-03 err=0.00 nd=23/10000] [test aL=1.28e+00 err=0.30]\n",
            "[i=197642 wall=574] [train aL=1.39e-03 err=0.00 nd=25/10000] [test aL=1.28e+00 err=0.30]\n",
            "[i=198642 wall=577] [train aL=9.33e-04 err=0.00 nd=17/10000] [test aL=1.26e+00 err=0.29]\n",
            "[i=199642 wall=580] [train aL=1.12e-03 err=0.00 nd=21/10000] [test aL=1.31e+00 err=0.30]\n",
            "[i=200642 wall=583] [train aL=3.97e-03 err=0.00 nd=68/10000] [test aL=1.28e+00 err=0.30]\n",
            "[i=201642 wall=586] [train aL=7.59e-06 err=0.00 nd=4/10000] [test aL=1.26e+00 err=0.29]\n",
            "[i=202642 wall=588] [train aL=1.18e-06 err=0.00 nd=1/10000] [test aL=1.27e+00 err=0.29]\n",
            "[i=203642 wall=591] [train aL=0.00e+00 err=0.00 nd=0/10000] [test aL=1.27e+00 err=0.29]\n",
            "Results saved to results/experiment_fc_softplus_0.001_hinge_100_seed1.pt\n",
            "[i=0 wall=0] [train aL=1.00e+00 err=0.47 nd=10000/10000] [test aL=1.00e+00 err=0.49]\n",
            "[i=1 wall=0] [train aL=1.00e+00 err=0.46 nd=10000/10000] [test aL=1.00e+00 err=0.47]\n",
            "[i=2 wall=0] [train aL=1.00e+00 err=0.45 nd=10000/10000] [test aL=1.00e+00 err=0.47]\n",
            "[i=3 wall=0] [train aL=1.00e+00 err=0.44 nd=10000/10000] [test aL=1.00e+00 err=0.46]\n",
            "[i=4 wall=1] [train aL=1.00e+00 err=0.44 nd=10000/10000] [test aL=1.00e+00 err=0.45]\n",
            "[i=5 wall=1] [train aL=1.00e+00 err=0.44 nd=10000/10000] [test aL=1.00e+00 err=0.45]\n",
            "[i=6 wall=1] [train aL=1.00e+00 err=0.43 nd=10000/10000] [test aL=1.00e+00 err=0.44]\n",
            "[i=7 wall=1] [train aL=1.00e+00 err=0.43 nd=10000/10000] [test aL=1.00e+00 err=0.44]\n",
            "[i=8 wall=1] [train aL=1.00e+00 err=0.42 nd=10000/10000] [test aL=1.00e+00 err=0.44]\n",
            "[i=9 wall=1] [train aL=1.00e+00 err=0.42 nd=10000/10000] [test aL=1.00e+00 err=0.43]\n",
            "[i=10 wall=1] [train aL=1.00e+00 err=0.42 nd=10000/10000] [test aL=1.00e+00 err=0.43]\n",
            "[i=11 wall=2] [train aL=1.00e+00 err=0.42 nd=10000/10000] [test aL=1.00e+00 err=0.43]\n",
            "[i=13 wall=2] [train aL=1.00e+00 err=0.41 nd=10000/10000] [test aL=1.00e+00 err=0.42]\n",
            "[i=15 wall=2] [train aL=1.00e+00 err=0.40 nd=10000/10000] [test aL=1.00e+00 err=0.41]\n",
            "[i=17 wall=2] [train aL=1.00e+00 err=0.39 nd=10000/10000] [test aL=1.00e+00 err=0.40]\n",
            "[i=19 wall=2] [train aL=1.00e+00 err=0.37 nd=10000/10000] [test aL=1.00e+00 err=0.39]\n",
            "[i=21 wall=2] [train aL=1.00e+00 err=0.36 nd=10000/10000] [test aL=1.00e+00 err=0.38]\n",
            "[i=24 wall=2] [train aL=1.00e+00 err=0.35 nd=10000/10000] [test aL=1.00e+00 err=0.36]\n",
            "[i=27 wall=2] [train aL=1.00e+00 err=0.35 nd=10000/10000] [test aL=1.00e+00 err=0.36]\n",
            "[i=30 wall=2] [train aL=1.00e+00 err=0.34 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=33 wall=3] [train aL=1.00e+00 err=0.34 nd=10000/10000] [test aL=1.00e+00 err=0.34]\n",
            "[i=37 wall=3] [train aL=1.00e+00 err=0.33 nd=10000/10000] [test aL=1.00e+00 err=0.34]\n",
            "[i=41 wall=3] [train aL=1.00e+00 err=0.33 nd=10000/10000] [test aL=1.00e+00 err=0.34]\n",
            "[i=46 wall=3] [train aL=1.00e+00 err=0.33 nd=10000/10000] [test aL=1.00e+00 err=0.34]\n",
            "[i=51 wall=3] [train aL=1.00e+00 err=0.33 nd=10000/10000] [test aL=1.00e+00 err=0.34]\n",
            "[i=57 wall=3] [train aL=1.00e+00 err=0.33 nd=10000/10000] [test aL=1.00e+00 err=0.34]\n",
            "[i=63 wall=3] [train aL=1.00e+00 err=0.34 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=70 wall=3] [train aL=1.00e+00 err=0.34 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=77 wall=3] [train aL=1.00e+00 err=0.34 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=85 wall=3] [train aL=1.00e+00 err=0.35 nd=10000/10000] [test aL=1.00e+00 err=0.36]\n",
            "[i=94 wall=4] [train aL=1.00e+00 err=0.33 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=104 wall=4] [train aL=1.00e+00 err=0.34 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=115 wall=4] [train aL=1.00e+00 err=0.34 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=127 wall=4] [train aL=1.00e+00 err=0.34 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=140 wall=4] [train aL=1.00e+00 err=0.34 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=154 wall=4] [train aL=1.00e+00 err=0.35 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=170 wall=4] [train aL=1.00e+00 err=0.34 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=187 wall=4] [train aL=1.00e+00 err=0.34 nd=10000/10000] [test aL=1.00e+00 err=0.35]\n",
            "[i=206 wall=5] [train aL=1.00e+00 err=0.34 nd=10000/10000] [test aL=1.00e+00 err=0.34]\n",
            "[i=227 wall=5] [train aL=9.99e-01 err=0.34 nd=10000/10000] [test aL=9.99e-01 err=0.35]\n",
            "[i=250 wall=5] [train aL=9.99e-01 err=0.35 nd=10000/10000] [test aL=9.99e-01 err=0.35]\n",
            "[i=275 wall=5] [train aL=9.99e-01 err=0.35 nd=10000/10000] [test aL=9.99e-01 err=0.36]\n",
            "[i=303 wall=5] [train aL=9.98e-01 err=0.37 nd=10000/10000] [test aL=9.98e-01 err=0.37]\n",
            "[i=333 wall=5] [train aL=9.98e-01 err=0.37 nd=10000/10000] [test aL=9.98e-01 err=0.37]\n",
            "[i=366 wall=6] [train aL=9.96e-01 err=0.36 nd=10000/10000] [test aL=9.96e-01 err=0.36]\n",
            "[i=402 wall=6] [train aL=9.95e-01 err=0.36 nd=10000/10000] [test aL=9.95e-01 err=0.35]\n",
            "[i=442 wall=6] [train aL=9.92e-01 err=0.34 nd=10000/10000] [test aL=9.92e-01 err=0.35]\n",
            "[i=486 wall=6] [train aL=9.87e-01 err=0.35 nd=10000/10000] [test aL=9.87e-01 err=0.35]\n",
            "[i=534 wall=6] [train aL=9.81e-01 err=0.34 nd=10000/10000] [test aL=9.81e-01 err=0.35]\n",
            "[i=586 wall=7] [train aL=9.69e-01 err=0.35 nd=10000/10000] [test aL=9.69e-01 err=0.36]\n",
            "[i=643 wall=7] [train aL=9.55e-01 err=0.35 nd=10000/10000] [test aL=9.55e-01 err=0.35]\n",
            "[i=706 wall=7] [train aL=9.30e-01 err=0.35 nd=10000/10000] [test aL=9.30e-01 err=0.36]\n",
            "[i=775 wall=7] [train aL=8.89e-01 err=0.36 nd=9774/10000] [test aL=8.90e-01 err=0.36]\n",
            "[i=850 wall=8] [train aL=8.60e-01 err=0.35 nd=9451/10000] [test aL=8.61e-01 err=0.36]\n",
            "[i=932 wall=8] [train aL=8.30e-01 err=0.35 nd=9248/10000] [test aL=8.32e-01 err=0.35]\n",
            "[i=1021 wall=8] [train aL=7.95e-01 err=0.33 nd=9090/10000] [test aL=7.97e-01 err=0.33]\n",
            "[i=1119 wall=9] [train aL=7.66e-01 err=0.33 nd=8699/10000] [test aL=7.72e-01 err=0.33]\n",
            "[i=1225 wall=9] [train aL=7.38e-01 err=0.33 nd=8307/10000] [test aL=7.49e-01 err=0.33]\n",
            "[i=1341 wall=9] [train aL=7.18e-01 err=0.32 nd=7914/10000] [test aL=7.33e-01 err=0.33]\n",
            "[i=1467 wall=10] [train aL=6.99e-01 err=0.31 nd=7861/10000] [test aL=7.17e-01 err=0.32]\n",
            "[i=1604 wall=10] [train aL=6.85e-01 err=0.31 nd=7490/10000] [test aL=7.06e-01 err=0.32]\n",
            "[i=1753 wall=11] [train aL=6.73e-01 err=0.30 nd=7333/10000] [test aL=6.98e-01 err=0.31]\n",
            "[i=1914 wall=11] [train aL=6.63e-01 err=0.29 nd=7077/10000] [test aL=6.89e-01 err=0.31]\n",
            "[i=2089 wall=12] [train aL=6.55e-01 err=0.29 nd=6870/10000] [test aL=6.83e-01 err=0.31]\n",
            "[i=2278 wall=13] [train aL=6.47e-01 err=0.29 nd=6770/10000] [test aL=6.78e-01 err=0.30]\n",
            "[i=2482 wall=13] [train aL=6.40e-01 err=0.29 nd=6697/10000] [test aL=6.71e-01 err=0.30]\n",
            "[i=2702 wall=14] [train aL=6.32e-01 err=0.28 nd=6632/10000] [test aL=6.65e-01 err=0.30]\n",
            "[i=2939 wall=15] [train aL=6.24e-01 err=0.28 nd=6507/10000] [test aL=6.58e-01 err=0.30]\n",
            "[i=3194 wall=16] [train aL=6.17e-01 err=0.27 nd=6420/10000] [test aL=6.52e-01 err=0.29]\n",
            "[i=3468 wall=17] [train aL=6.09e-01 err=0.27 nd=6345/10000] [test aL=6.46e-01 err=0.29]\n",
            "[i=3762 wall=17] [train aL=6.04e-01 err=0.27 nd=6221/10000] [test aL=6.43e-01 err=0.29]\n",
            "[i=4076 wall=18] [train aL=5.97e-01 err=0.26 nd=6206/10000] [test aL=6.39e-01 err=0.29]\n",
            "[i=4411 wall=19] [train aL=5.89e-01 err=0.26 nd=6151/10000] [test aL=6.33e-01 err=0.28]\n",
            "[i=4768 wall=20] [train aL=5.82e-01 err=0.26 nd=6073/10000] [test aL=6.28e-01 err=0.28]\n",
            "[i=5148 wall=21] [train aL=5.76e-01 err=0.25 nd=6005/10000] [test aL=6.25e-01 err=0.28]\n",
            "[i=5551 wall=23] [train aL=5.68e-01 err=0.25 nd=5933/10000] [test aL=6.21e-01 err=0.28]\n",
            "[i=5977 wall=24] [train aL=5.62e-01 err=0.25 nd=5777/10000] [test aL=6.19e-01 err=0.28]\n",
            "[i=6427 wall=26] [train aL=5.56e-01 err=0.25 nd=5726/10000] [test aL=6.16e-01 err=0.28]\n",
            "[i=6902 wall=27] [train aL=5.49e-01 err=0.24 nd=5655/10000] [test aL=6.12e-01 err=0.27]\n",
            "[i=7401 wall=28] [train aL=5.43e-01 err=0.24 nd=5533/10000] [test aL=6.10e-01 err=0.27]\n",
            "[i=7924 wall=30] [train aL=5.37e-01 err=0.24 nd=5393/10000] [test aL=6.08e-01 err=0.27]\n",
            "[i=8472 wall=31] [train aL=5.30e-01 err=0.24 nd=5427/10000] [test aL=6.04e-01 err=0.27]\n",
            "[i=9044 wall=33] [train aL=5.23e-01 err=0.23 nd=5290/10000] [test aL=6.02e-01 err=0.27]\n",
            "[i=9640 wall=35] [train aL=5.16e-01 err=0.23 nd=5181/10000] [test aL=6.00e-01 err=0.27]\n",
            "[i=10259 wall=37] [train aL=5.10e-01 err=0.22 nd=5186/10000] [test aL=5.99e-01 err=0.27]\n",
            "[i=10901 wall=39] [train aL=5.02e-01 err=0.22 nd=5163/10000] [test aL=5.96e-01 err=0.27]\n",
            "[i=11565 wall=41] [train aL=4.95e-01 err=0.22 nd=5213/10000] [test aL=5.94e-01 err=0.27]\n",
            "[i=12251 wall=43] [train aL=4.87e-01 err=0.21 nd=5047/10000] [test aL=5.91e-01 err=0.27]\n",
            "[i=12958 wall=45] [train aL=4.79e-01 err=0.21 nd=4890/10000] [test aL=5.89e-01 err=0.27]\n",
            "[i=13685 wall=47] [train aL=4.71e-01 err=0.21 nd=4889/10000] [test aL=5.87e-01 err=0.27]\n",
            "[i=14431 wall=49] [train aL=4.62e-01 err=0.20 nd=4714/10000] [test aL=5.88e-01 err=0.27]\n",
            "[i=15195 wall=51] [train aL=4.54e-01 err=0.20 nd=4633/10000] [test aL=5.85e-01 err=0.27]\n",
            "[i=15977 wall=54] [train aL=4.45e-01 err=0.19 nd=4668/10000] [test aL=5.85e-01 err=0.27]\n",
            "[i=16775 wall=56] [train aL=4.37e-01 err=0.19 nd=4584/10000] [test aL=5.84e-01 err=0.27]\n",
            "[i=17589 wall=58] [train aL=4.28e-01 err=0.19 nd=4455/10000] [test aL=5.83e-01 err=0.27]\n",
            "[i=18417 wall=60] [train aL=4.17e-01 err=0.18 nd=4360/10000] [test aL=5.83e-01 err=0.27]\n",
            "[i=19259 wall=63] [train aL=4.09e-01 err=0.18 nd=4341/10000] [test aL=5.81e-01 err=0.27]\n",
            "[i=20114 wall=66] [train aL=4.00e-01 err=0.17 nd=4198/10000] [test aL=5.83e-01 err=0.27]\n",
            "[i=20981 wall=68] [train aL=3.90e-01 err=0.17 nd=4036/10000] [test aL=5.84e-01 err=0.27]\n",
            "[i=21859 wall=70] [train aL=3.80e-01 err=0.16 nd=4117/10000] [test aL=5.84e-01 err=0.27]\n",
            "[i=22747 wall=73] [train aL=3.72e-01 err=0.16 nd=3879/10000] [test aL=5.86e-01 err=0.27]\n",
            "[i=23645 wall=76] [train aL=3.64e-01 err=0.15 nd=3902/10000] [test aL=5.88e-01 err=0.27]\n",
            "[i=24552 wall=78] [train aL=3.50e-01 err=0.15 nd=3652/10000] [test aL=5.87e-01 err=0.27]\n",
            "[i=25467 wall=81] [train aL=3.41e-01 err=0.15 nd=3650/10000] [test aL=5.88e-01 err=0.27]\n",
            "[i=26389 wall=83] [train aL=3.30e-01 err=0.14 nd=3514/10000] [test aL=5.89e-01 err=0.27]\n",
            "[i=27318 wall=86] [train aL=3.20e-01 err=0.14 nd=3355/10000] [test aL=5.90e-01 err=0.27]\n",
            "[i=28253 wall=89] [train aL=3.15e-01 err=0.13 nd=3426/10000] [test aL=5.91e-01 err=0.27]\n",
            "[i=29194 wall=92] [train aL=3.01e-01 err=0.13 nd=3366/10000] [test aL=5.91e-01 err=0.27]\n",
            "[i=30141 wall=94] [train aL=2.95e-01 err=0.13 nd=3108/10000] [test aL=5.94e-01 err=0.27]\n",
            "[i=31092 wall=97] [train aL=2.83e-01 err=0.12 nd=2922/10000] [test aL=5.96e-01 err=0.27]\n",
            "[i=32048 wall=100] [train aL=2.74e-01 err=0.12 nd=2933/10000] [test aL=5.96e-01 err=0.27]\n",
            "[i=33008 wall=103] [train aL=2.64e-01 err=0.11 nd=2817/10000] [test aL=5.98e-01 err=0.27]\n",
            "[i=33972 wall=105] [train aL=2.57e-01 err=0.11 nd=2803/10000] [test aL=5.98e-01 err=0.28]\n",
            "[i=34939 wall=108] [train aL=2.47e-01 err=0.11 nd=2635/10000] [test aL=6.01e-01 err=0.28]\n",
            "[i=35909 wall=111] [train aL=2.39e-01 err=0.10 nd=2685/10000] [test aL=6.01e-01 err=0.28]\n",
            "[i=36882 wall=114] [train aL=2.30e-01 err=0.10 nd=2506/10000] [test aL=6.03e-01 err=0.28]\n",
            "[i=37857 wall=116] [train aL=2.23e-01 err=0.09 nd=2471/10000] [test aL=6.04e-01 err=0.28]\n",
            "[i=38835 wall=119] [train aL=2.15e-01 err=0.09 nd=2244/10000] [test aL=6.06e-01 err=0.28]\n",
            "[i=39815 wall=122] [train aL=2.10e-01 err=0.09 nd=2361/10000] [test aL=6.05e-01 err=0.28]\n",
            "[i=40797 wall=125] [train aL=2.00e-01 err=0.09 nd=2205/10000] [test aL=6.06e-01 err=0.28]\n",
            "[i=41781 wall=128] [train aL=1.98e-01 err=0.09 nd=2113/10000] [test aL=6.08e-01 err=0.28]\n",
            "[i=42766 wall=130] [train aL=1.91e-01 err=0.08 nd=2070/10000] [test aL=6.08e-01 err=0.28]\n",
            "[i=43753 wall=133] [train aL=1.85e-01 err=0.08 nd=2008/10000] [test aL=6.07e-01 err=0.28]\n",
            "[i=44741 wall=136] [train aL=1.77e-01 err=0.08 nd=1814/10000] [test aL=6.06e-01 err=0.28]\n",
            "[i=45730 wall=139] [train aL=1.77e-01 err=0.08 nd=2020/10000] [test aL=6.10e-01 err=0.28]\n",
            "[i=46720 wall=142] [train aL=1.66e-01 err=0.08 nd=1790/10000] [test aL=6.07e-01 err=0.28]\n",
            "[i=47711 wall=144] [train aL=1.65e-01 err=0.07 nd=1773/10000] [test aL=6.11e-01 err=0.28]\n",
            "[i=48703 wall=148] [train aL=1.59e-01 err=0.07 nd=1765/10000] [test aL=6.09e-01 err=0.28]\n",
            "[i=49696 wall=150] [train aL=1.56e-01 err=0.07 nd=1769/10000] [test aL=6.06e-01 err=0.28]\n",
            "[i=50690 wall=153] [train aL=1.51e-01 err=0.07 nd=1771/10000] [test aL=6.10e-01 err=0.28]\n",
            "[i=51684 wall=156] [train aL=1.50e-01 err=0.07 nd=1734/10000] [test aL=6.12e-01 err=0.28]\n",
            "[i=52679 wall=159] [train aL=1.43e-01 err=0.07 nd=1465/10000] [test aL=6.10e-01 err=0.28]\n",
            "[i=53674 wall=162] [train aL=1.44e-01 err=0.06 nd=1697/10000] [test aL=6.13e-01 err=0.29]\n",
            "[i=54670 wall=164] [train aL=1.38e-01 err=0.06 nd=1464/10000] [test aL=6.11e-01 err=0.29]\n",
            "[i=55666 wall=167] [train aL=1.35e-01 err=0.06 nd=1274/10000] [test aL=6.13e-01 err=0.28]\n",
            "[i=56663 wall=170] [train aL=1.32e-01 err=0.06 nd=1300/10000] [test aL=6.13e-01 err=0.29]\n",
            "[i=57660 wall=173] [train aL=1.32e-01 err=0.06 nd=1335/10000] [test aL=6.11e-01 err=0.29]\n",
            "[i=58657 wall=176] [train aL=1.31e-01 err=0.06 nd=1416/10000] [test aL=6.10e-01 err=0.28]\n",
            "[i=59655 wall=179] [train aL=1.28e-01 err=0.06 nd=1264/10000] [test aL=6.11e-01 err=0.29]\n",
            "[i=60653 wall=181] [train aL=1.27e-01 err=0.06 nd=1367/10000] [test aL=6.10e-01 err=0.28]\n",
            "[i=61651 wall=185] [train aL=1.28e-01 err=0.06 nd=1440/10000] [test aL=6.11e-01 err=0.29]\n",
            "[i=62649 wall=187] [train aL=1.24e-01 err=0.06 nd=1225/10000] [test aL=6.11e-01 err=0.28]\n",
            "[i=63648 wall=190] [train aL=1.23e-01 err=0.06 nd=1222/10000] [test aL=6.10e-01 err=0.29]\n",
            "[i=64647 wall=193] [train aL=1.21e-01 err=0.06 nd=1240/10000] [test aL=6.12e-01 err=0.29]\n",
            "[i=65646 wall=196] [train aL=1.19e-01 err=0.06 nd=1154/10000] [test aL=6.11e-01 err=0.29]\n",
            "[i=66645 wall=199] [train aL=1.19e-01 err=0.06 nd=1178/10000] [test aL=6.11e-01 err=0.28]\n",
            "[i=67644 wall=201] [train aL=1.18e-01 err=0.06 nd=1195/10000] [test aL=6.15e-01 err=0.29]\n",
            "[i=68643 wall=204] [train aL=1.15e-01 err=0.06 nd=1198/10000] [test aL=6.10e-01 err=0.29]\n",
            "[i=69642 wall=207] [train aL=1.15e-01 err=0.06 nd=1321/10000] [test aL=6.10e-01 err=0.29]\n",
            "[i=70642 wall=210] [train aL=1.14e-01 err=0.06 nd=1052/10000] [test aL=6.14e-01 err=0.29]\n",
            "[i=71642 wall=213] [train aL=1.13e-01 err=0.06 nd=1069/10000] [test aL=6.13e-01 err=0.29]\n",
            "[i=72642 wall=216] [train aL=1.12e-01 err=0.06 nd=1090/10000] [test aL=6.14e-01 err=0.29]\n",
            "[i=73642 wall=219] [train aL=1.12e-01 err=0.06 nd=1234/10000] [test aL=6.10e-01 err=0.29]\n",
            "[i=74642 wall=222] [train aL=1.11e-01 err=0.05 nd=1214/10000] [test aL=6.13e-01 err=0.29]\n",
            "[i=75642 wall=224] [train aL=1.10e-01 err=0.05 nd=1215/10000] [test aL=6.17e-01 err=0.29]\n",
            "[i=76642 wall=227] [train aL=1.07e-01 err=0.05 nd=969/10000] [test aL=6.12e-01 err=0.29]\n",
            "[i=77642 wall=230] [train aL=1.07e-01 err=0.05 nd=1114/10000] [test aL=6.12e-01 err=0.29]\n",
            "[i=78642 wall=233] [train aL=1.06e-01 err=0.05 nd=1111/10000] [test aL=6.16e-01 err=0.29]\n",
            "[i=79642 wall=236] [train aL=1.05e-01 err=0.05 nd=912/10000] [test aL=6.15e-01 err=0.29]\n",
            "[i=80642 wall=239] [train aL=1.04e-01 err=0.05 nd=1058/10000] [test aL=6.15e-01 err=0.29]\n",
            "[i=81642 wall=241] [train aL=1.04e-01 err=0.05 nd=1289/10000] [test aL=6.17e-01 err=0.29]\n",
            "[i=82642 wall=245] [train aL=1.02e-01 err=0.05 nd=1020/10000] [test aL=6.15e-01 err=0.29]\n",
            "[i=83642 wall=247] [train aL=1.01e-01 err=0.05 nd=1196/10000] [test aL=6.16e-01 err=0.29]\n",
            "[i=84642 wall=250] [train aL=1.01e-01 err=0.05 nd=1077/10000] [test aL=6.13e-01 err=0.28]\n",
            "[i=85642 wall=253] [train aL=9.91e-02 err=0.05 nd=1000/10000] [test aL=6.13e-01 err=0.29]\n",
            "[i=86642 wall=256] [train aL=9.89e-02 err=0.05 nd=1055/10000] [test aL=6.17e-01 err=0.29]\n",
            "[i=87642 wall=259] [train aL=9.79e-02 err=0.05 nd=1108/10000] [test aL=6.14e-01 err=0.28]\n",
            "[i=88642 wall=262] [train aL=9.71e-02 err=0.05 nd=1013/10000] [test aL=6.16e-01 err=0.29]\n",
            "[i=89642 wall=264] [train aL=9.76e-02 err=0.05 nd=987/10000] [test aL=6.18e-01 err=0.29]\n",
            "[i=90642 wall=267] [train aL=9.51e-02 err=0.05 nd=954/10000] [test aL=6.24e-01 err=0.29]\n",
            "[i=91642 wall=270] [train aL=9.44e-02 err=0.05 nd=865/10000] [test aL=6.25e-01 err=0.29]\n",
            "[i=92642 wall=273] [train aL=9.31e-02 err=0.05 nd=809/10000] [test aL=6.25e-01 err=0.29]\n",
            "[i=93642 wall=276] [train aL=9.27e-02 err=0.05 nd=830/10000] [test aL=6.28e-01 err=0.29]\n",
            "[i=94642 wall=279] [train aL=9.09e-02 err=0.05 nd=1043/10000] [test aL=6.25e-01 err=0.29]\n",
            "[i=95642 wall=282] [train aL=8.94e-02 err=0.05 nd=858/10000] [test aL=6.29e-01 err=0.29]\n",
            "[i=96642 wall=285] [train aL=9.03e-02 err=0.05 nd=988/10000] [test aL=6.27e-01 err=0.29]\n",
            "[i=97642 wall=287] [train aL=8.76e-02 err=0.05 nd=980/10000] [test aL=6.26e-01 err=0.29]\n",
            "[i=98642 wall=290] [train aL=8.67e-02 err=0.05 nd=912/10000] [test aL=6.32e-01 err=0.29]\n",
            "[i=99642 wall=293] [train aL=8.67e-02 err=0.05 nd=960/10000] [test aL=6.34e-01 err=0.29]\n",
            "[i=100642 wall=296] [train aL=8.58e-02 err=0.05 nd=1246/10000] [test aL=6.29e-01 err=0.29]\n",
            "[i=101642 wall=299] [train aL=8.37e-02 err=0.05 nd=748/10000] [test aL=6.39e-01 err=0.29]\n",
            "[i=102642 wall=302] [train aL=8.26e-02 err=0.05 nd=941/10000] [test aL=6.35e-01 err=0.29]\n",
            "[i=103642 wall=308] [train aL=8.14e-02 err=0.05 nd=1061/10000] [test aL=6.34e-01 err=0.29]\n",
            "[i=104642 wall=311] [train aL=8.11e-02 err=0.04 nd=990/10000] [test aL=6.44e-01 err=0.29]\n",
            "[i=105642 wall=314] [train aL=7.91e-02 err=0.04 nd=742/10000] [test aL=6.39e-01 err=0.29]\n",
            "[i=106642 wall=319] [train aL=7.80e-02 err=0.04 nd=840/10000] [test aL=6.37e-01 err=0.28]\n",
            "[i=107642 wall=322] [train aL=7.75e-02 err=0.04 nd=926/10000] [test aL=6.38e-01 err=0.28]\n",
            "[i=108642 wall=325] [train aL=7.65e-02 err=0.04 nd=1051/10000] [test aL=6.47e-01 err=0.29]\n",
            "[i=109642 wall=327] [train aL=7.56e-02 err=0.04 nd=822/10000] [test aL=6.48e-01 err=0.29]\n",
            "[i=110642 wall=331] [train aL=7.32e-02 err=0.04 nd=947/10000] [test aL=6.56e-01 err=0.29]\n",
            "[i=111642 wall=333] [train aL=7.16e-02 err=0.04 nd=756/10000] [test aL=6.55e-01 err=0.29]\n",
            "[i=112642 wall=336] [train aL=7.07e-02 err=0.04 nd=940/10000] [test aL=6.58e-01 err=0.28]\n",
            "[i=113642 wall=339] [train aL=6.97e-02 err=0.04 nd=865/10000] [test aL=6.67e-01 err=0.29]\n",
            "[i=114642 wall=342] [train aL=6.79e-02 err=0.03 nd=697/10000] [test aL=6.71e-01 err=0.29]\n",
            "[i=115642 wall=345] [train aL=6.77e-02 err=0.03 nd=872/10000] [test aL=6.73e-01 err=0.29]\n",
            "[i=116642 wall=347] [train aL=6.56e-02 err=0.03 nd=716/10000] [test aL=6.73e-01 err=0.29]\n",
            "[i=117642 wall=350] [train aL=7.28e-02 err=0.03 nd=968/10000] [test aL=6.85e-01 err=0.29]\n",
            "[i=118642 wall=353] [train aL=6.50e-02 err=0.03 nd=727/10000] [test aL=6.87e-01 err=0.29]\n",
            "[i=119642 wall=356] [train aL=6.64e-02 err=0.03 nd=799/10000] [test aL=6.85e-01 err=0.29]\n",
            "[i=120642 wall=359] [train aL=6.47e-02 err=0.03 nd=779/10000] [test aL=6.94e-01 err=0.29]\n",
            "[i=121642 wall=362] [train aL=6.11e-02 err=0.03 nd=688/10000] [test aL=6.94e-01 err=0.29]\n",
            "[i=122642 wall=365] [train aL=5.91e-02 err=0.03 nd=618/10000] [test aL=7.02e-01 err=0.29]\n",
            "[i=123642 wall=368] [train aL=5.75e-02 err=0.03 nd=657/10000] [test aL=7.05e-01 err=0.29]\n",
            "[i=124642 wall=371] [train aL=5.67e-02 err=0.02 nd=623/10000] [test aL=7.06e-01 err=0.29]\n",
            "[i=125642 wall=373] [train aL=5.49e-02 err=0.02 nd=705/10000] [test aL=7.04e-01 err=0.28]\n",
            "[i=126642 wall=376] [train aL=5.71e-02 err=0.02 nd=835/10000] [test aL=7.18e-01 err=0.29]\n",
            "[i=127642 wall=379] [train aL=5.24e-02 err=0.02 nd=684/10000] [test aL=7.22e-01 err=0.28]\n",
            "[i=128642 wall=382] [train aL=5.10e-02 err=0.02 nd=809/10000] [test aL=7.28e-01 err=0.29]\n",
            "[i=129642 wall=385] [train aL=4.88e-02 err=0.02 nd=673/10000] [test aL=7.30e-01 err=0.28]\n",
            "[i=130642 wall=387] [train aL=4.72e-02 err=0.02 nd=624/10000] [test aL=7.49e-01 err=0.29]\n",
            "[i=131642 wall=391] [train aL=4.53e-02 err=0.02 nd=644/10000] [test aL=7.46e-01 err=0.29]\n",
            "[i=132642 wall=393] [train aL=4.44e-02 err=0.02 nd=770/10000] [test aL=7.49e-01 err=0.29]\n",
            "[i=133642 wall=396] [train aL=4.21e-02 err=0.02 nd=594/10000] [test aL=7.73e-01 err=0.29]\n",
            "[i=134642 wall=399] [train aL=4.01e-02 err=0.02 nd=676/10000] [test aL=7.71e-01 err=0.29]\n",
            "[i=135642 wall=402] [train aL=3.75e-02 err=0.02 nd=503/10000] [test aL=7.80e-01 err=0.29]\n",
            "[i=136642 wall=405] [train aL=3.57e-02 err=0.02 nd=673/10000] [test aL=7.88e-01 err=0.29]\n",
            "[i=137642 wall=408] [train aL=3.42e-02 err=0.02 nd=520/10000] [test aL=7.98e-01 err=0.29]\n",
            "[i=138642 wall=410] [train aL=3.28e-02 err=0.02 nd=537/10000] [test aL=8.04e-01 err=0.29]\n",
            "[i=139642 wall=413] [train aL=3.16e-02 err=0.02 nd=551/10000] [test aL=8.07e-01 err=0.29]\n",
            "[i=140642 wall=416] [train aL=3.00e-02 err=0.02 nd=450/10000] [test aL=8.19e-01 err=0.29]\n",
            "[i=141642 wall=419] [train aL=3.01e-02 err=0.02 nd=418/10000] [test aL=8.33e-01 err=0.29]\n",
            "[i=142642 wall=422] [train aL=2.96e-02 err=0.02 nd=459/10000] [test aL=8.41e-01 err=0.29]\n",
            "[i=143642 wall=425] [train aL=2.77e-02 err=0.02 nd=416/10000] [test aL=8.40e-01 err=0.29]\n",
            "[i=144642 wall=428] [train aL=2.85e-02 err=0.02 nd=371/10000] [test aL=8.65e-01 err=0.29]\n",
            "[i=145642 wall=431] [train aL=2.65e-02 err=0.02 nd=315/10000] [test aL=8.72e-01 err=0.29]\n",
            "[i=146642 wall=433] [train aL=2.59e-02 err=0.02 nd=271/10000] [test aL=8.77e-01 err=0.29]\n",
            "[i=147642 wall=436] [train aL=2.49e-02 err=0.02 nd=326/10000] [test aL=8.72e-01 err=0.29]\n",
            "[i=148642 wall=439] [train aL=2.52e-02 err=0.02 nd=337/10000] [test aL=8.93e-01 err=0.29]\n",
            "[i=149642 wall=442] [train aL=2.60e-02 err=0.02 nd=330/10000] [test aL=9.04e-01 err=0.29]\n",
            "[i=150642 wall=445] [train aL=3.00e-02 err=0.02 nd=302/10000] [test aL=9.06e-01 err=0.30]\n",
            "[i=151642 wall=447] [train aL=3.62e-02 err=0.02 nd=402/10000] [test aL=9.16e-01 err=0.30]\n",
            "[i=152642 wall=450] [train aL=2.37e-02 err=0.02 nd=260/10000] [test aL=9.02e-01 err=0.29]\n",
            "[i=153642 wall=453] [train aL=2.40e-02 err=0.02 nd=256/10000] [test aL=9.21e-01 err=0.30]\n",
            "[i=154642 wall=456] [train aL=2.31e-02 err=0.02 nd=274/10000] [test aL=9.18e-01 err=0.30]\n",
            "[i=155642 wall=459] [train aL=2.35e-02 err=0.02 nd=305/10000] [test aL=9.18e-01 err=0.29]\n",
            "[i=156642 wall=462] [train aL=2.33e-02 err=0.02 nd=245/10000] [test aL=9.36e-01 err=0.30]\n",
            "[i=157642 wall=465] [train aL=2.26e-02 err=0.02 nd=240/10000] [test aL=9.37e-01 err=0.29]\n",
            "[i=158642 wall=468] [train aL=2.30e-02 err=0.02 nd=342/10000] [test aL=9.49e-01 err=0.29]\n",
            "[i=159642 wall=470] [train aL=2.20e-02 err=0.02 nd=271/10000] [test aL=9.35e-01 err=0.29]\n",
            "[i=160642 wall=473] [train aL=2.23e-02 err=0.02 nd=288/10000] [test aL=9.47e-01 err=0.30]\n",
            "[i=161642 wall=476] [train aL=2.27e-02 err=0.02 nd=291/10000] [test aL=9.62e-01 err=0.29]\n",
            "[i=162642 wall=479] [train aL=2.21e-02 err=0.02 nd=252/10000] [test aL=9.60e-01 err=0.29]\n",
            "[i=163642 wall=482] [train aL=2.80e-02 err=0.02 nd=411/10000] [test aL=1.00e+00 err=0.30]\n",
            "[i=164642 wall=485] [train aL=2.07e-02 err=0.02 nd=208/10000] [test aL=9.57e-01 err=0.29]\n",
            "[i=165642 wall=488] [train aL=2.15e-02 err=0.02 nd=271/10000] [test aL=9.72e-01 err=0.29]\n",
            "[i=166642 wall=491] [train aL=2.13e-02 err=0.02 nd=236/10000] [test aL=9.74e-01 err=0.30]\n",
            "[i=167642 wall=493] [train aL=2.04e-02 err=0.01 nd=239/10000] [test aL=9.74e-01 err=0.30]\n",
            "[i=168642 wall=496] [train aL=2.06e-02 err=0.01 nd=232/10000] [test aL=9.89e-01 err=0.30]\n",
            "[i=169642 wall=499] [train aL=1.99e-02 err=0.01 nd=213/10000] [test aL=9.73e-01 err=0.29]\n",
            "[i=170642 wall=502] [train aL=1.96e-02 err=0.01 nd=188/10000] [test aL=9.90e-01 err=0.29]\n",
            "[i=171642 wall=505] [train aL=1.96e-02 err=0.01 nd=229/10000] [test aL=9.95e-01 err=0.29]\n",
            "[i=172642 wall=508] [train aL=1.95e-02 err=0.01 nd=242/10000] [test aL=9.94e-01 err=0.29]\n",
            "[i=173642 wall=510] [train aL=2.08e-02 err=0.02 nd=229/10000] [test aL=9.84e-01 err=0.29]\n",
            "[i=174642 wall=513] [train aL=1.95e-02 err=0.01 nd=241/10000] [test aL=1.00e+00 err=0.29]\n",
            "[i=175642 wall=516] [train aL=2.67e-02 err=0.02 nd=311/10000] [test aL=1.04e+00 err=0.30]\n",
            "[i=176642 wall=519] [train aL=2.03e-02 err=0.02 nd=212/10000] [test aL=1.03e+00 err=0.29]\n",
            "[i=177642 wall=522] [train aL=1.96e-02 err=0.01 nd=325/10000] [test aL=1.02e+00 err=0.30]\n",
            "[i=178642 wall=525] [train aL=2.06e-02 err=0.01 nd=236/10000] [test aL=1.02e+00 err=0.30]\n",
            "[i=179642 wall=528] [train aL=1.90e-02 err=0.01 nd=191/10000] [test aL=1.04e+00 err=0.29]\n",
            "[i=180642 wall=530] [train aL=1.88e-02 err=0.01 nd=249/10000] [test aL=1.04e+00 err=0.29]\n",
            "[i=181642 wall=533] [train aL=1.91e-02 err=0.01 nd=228/10000] [test aL=1.06e+00 err=0.29]\n",
            "[i=182642 wall=536] [train aL=1.85e-02 err=0.01 nd=207/10000] [test aL=1.04e+00 err=0.29]\n",
            "[i=183642 wall=539] [train aL=1.88e-02 err=0.01 nd=237/10000] [test aL=1.05e+00 err=0.29]\n",
            "[i=184642 wall=542] [train aL=1.90e-02 err=0.01 nd=250/10000] [test aL=1.05e+00 err=0.30]\n",
            "[i=185642 wall=545] [train aL=2.39e-02 err=0.02 nd=296/10000] [test aL=1.07e+00 err=0.29]\n",
            "[i=186642 wall=548] [train aL=1.87e-02 err=0.01 nd=200/10000] [test aL=1.07e+00 err=0.29]\n",
            "[i=187642 wall=551] [train aL=1.89e-02 err=0.01 nd=253/10000] [test aL=1.08e+00 err=0.29]\n",
            "[i=188642 wall=553] [train aL=2.00e-02 err=0.01 nd=222/10000] [test aL=1.07e+00 err=0.29]\n",
            "[i=189642 wall=556] [train aL=1.80e-02 err=0.01 nd=187/10000] [test aL=1.07e+00 err=0.30]\n",
            "[i=190642 wall=559] [train aL=1.91e-02 err=0.01 nd=209/10000] [test aL=1.10e+00 err=0.29]\n",
            "[i=191642 wall=562] [train aL=1.83e-02 err=0.01 nd=192/10000] [test aL=1.09e+00 err=0.29]\n",
            "[i=192642 wall=565] [train aL=1.82e-02 err=0.01 nd=204/10000] [test aL=1.08e+00 err=0.30]\n",
            "[i=193642 wall=567] [train aL=1.81e-02 err=0.01 nd=205/10000] [test aL=1.09e+00 err=0.29]\n",
            "[i=194642 wall=570] [train aL=2.45e-02 err=0.02 nd=254/10000] [test aL=1.10e+00 err=0.30]\n",
            "[i=195642 wall=573] [train aL=1.84e-02 err=0.01 nd=212/10000] [test aL=1.08e+00 err=0.29]\n",
            "[i=196642 wall=576] [train aL=1.80e-02 err=0.01 nd=180/10000] [test aL=1.10e+00 err=0.29]\n",
            "[i=197642 wall=579] [train aL=2.06e-02 err=0.01 nd=235/10000] [test aL=1.13e+00 err=0.30]\n",
            "[i=198642 wall=582] [train aL=1.78e-02 err=0.01 nd=196/10000] [test aL=1.11e+00 err=0.30]\n",
            "[i=199642 wall=585] [train aL=1.87e-02 err=0.01 nd=199/10000] [test aL=1.11e+00 err=0.29]\n",
            "[i=200642 wall=588] [train aL=1.89e-02 err=0.01 nd=217/10000] [test aL=1.14e+00 err=0.30]\n",
            "[i=201642 wall=590] [train aL=2.00e-02 err=0.01 nd=222/10000] [test aL=1.11e+00 err=0.29]\n",
            "[i=202642 wall=593] [train aL=1.78e-02 err=0.01 nd=179/10000] [test aL=1.11e+00 err=0.30]\n",
            "[i=203642 wall=596] [train aL=2.10e-02 err=0.01 nd=223/10000] [test aL=1.13e+00 err=0.29]\n",
            "[i=204642 wall=599] [train aL=1.93e-02 err=0.01 nd=206/10000] [test aL=1.12e+00 err=0.30]\n",
            "[i=205642 wall=602] [train aL=1.77e-02 err=0.01 nd=188/10000] [test aL=1.12e+00 err=0.29]\n",
            "[i=206642 wall=605] [train aL=1.79e-02 err=0.01 nd=206/10000] [test aL=1.12e+00 err=0.29]\n",
            "[i=207642 wall=607] [train aL=1.85e-02 err=0.01 nd=195/10000] [test aL=1.12e+00 err=0.29]\n",
            "[i=208642 wall=611] [train aL=1.86e-02 err=0.01 nd=200/10000] [test aL=1.15e+00 err=0.29]\n",
            "[i=209642 wall=613] [train aL=1.81e-02 err=0.01 nd=189/10000] [test aL=1.13e+00 err=0.29]\n",
            "[i=210642 wall=616] [train aL=1.92e-02 err=0.01 nd=197/10000] [test aL=1.14e+00 err=0.29]\n",
            "[i=211642 wall=619] [train aL=1.78e-02 err=0.01 nd=191/10000] [test aL=1.13e+00 err=0.29]\n",
            "[i=212642 wall=622] [train aL=1.78e-02 err=0.01 nd=184/10000] [test aL=1.15e+00 err=0.30]\n",
            "[i=213642 wall=625] [train aL=1.79e-02 err=0.01 nd=209/10000] [test aL=1.15e+00 err=0.29]\n",
            "[i=214642 wall=628] [train aL=1.85e-02 err=0.01 nd=197/10000] [test aL=1.14e+00 err=0.29]\n",
            "[i=215642 wall=630] [train aL=1.78e-02 err=0.01 nd=181/10000] [test aL=1.16e+00 err=0.29]\n",
            "[i=216642 wall=634] [train aL=1.92e-02 err=0.01 nd=205/10000] [test aL=1.16e+00 err=0.29]\n",
            "[i=217642 wall=637] [train aL=1.91e-02 err=0.01 nd=200/10000] [test aL=1.17e+00 err=0.29]\n",
            "[i=218642 wall=639] [train aL=1.78e-02 err=0.01 nd=187/10000] [test aL=1.16e+00 err=0.29]\n",
            "[i=219642 wall=642] [train aL=1.76e-02 err=0.01 nd=181/10000] [test aL=1.14e+00 err=0.29]\n",
            "[i=220642 wall=645] [train aL=1.77e-02 err=0.01 nd=176/10000] [test aL=1.14e+00 err=0.29]\n",
            "[i=221642 wall=648] [train aL=1.79e-02 err=0.01 nd=183/10000] [test aL=1.14e+00 err=0.29]\n",
            "[i=222642 wall=651] [train aL=1.79e-02 err=0.01 nd=201/10000] [test aL=1.14e+00 err=0.29]\n",
            "[i=223642 wall=654] [train aL=2.18e-02 err=0.01 nd=236/10000] [test aL=1.16e+00 err=0.29]\n",
            "[i=224642 wall=656] [train aL=1.86e-02 err=0.01 nd=190/10000] [test aL=1.17e+00 err=0.29]\n",
            "[i=225642 wall=660] [train aL=1.78e-02 err=0.01 nd=186/10000] [test aL=1.17e+00 err=0.29]\n",
            "[i=226642 wall=662] [train aL=1.89e-02 err=0.01 nd=208/10000] [test aL=1.17e+00 err=0.29]\n",
            "[i=227642 wall=665] [train aL=1.77e-02 err=0.01 nd=179/10000] [test aL=1.16e+00 err=0.29]\n",
            "[i=228642 wall=668] [train aL=1.77e-02 err=0.01 nd=181/10000] [test aL=1.17e+00 err=0.29]\n",
            "[i=229642 wall=671] [train aL=1.77e-02 err=0.01 nd=179/10000] [test aL=1.18e+00 err=0.29]\n",
            "[i=230642 wall=674] [train aL=1.82e-02 err=0.01 nd=196/10000] [test aL=1.18e+00 err=0.29]\n",
            "[i=231642 wall=677] [train aL=1.76e-02 err=0.01 nd=180/10000] [test aL=1.17e+00 err=0.29]\n",
            "[i=232642 wall=680] [train aL=1.76e-02 err=0.01 nd=178/10000] [test aL=1.17e+00 err=0.29]\n",
            "[i=233642 wall=683] [train aL=1.78e-02 err=0.01 nd=185/10000] [test aL=1.19e+00 err=0.29]\n",
            "[i=234642 wall=686] [train aL=1.89e-02 err=0.01 nd=208/10000] [test aL=1.20e+00 err=0.29]\n",
            "[i=235642 wall=688] [train aL=7.35e-02 err=0.03 nd=640/10000] [test aL=1.19e+00 err=0.29]\n",
            "[i=236642 wall=691] [train aL=1.77e-02 err=0.01 nd=183/10000] [test aL=1.18e+00 err=0.29]\n",
            "[i=237642 wall=694] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.18e+00 err=0.29]\n",
            "[i=238642 wall=697] [train aL=1.76e-02 err=0.01 nd=179/10000] [test aL=1.18e+00 err=0.29]\n",
            "[i=239642 wall=700] [train aL=2.17e-02 err=0.01 nd=247/10000] [test aL=1.18e+00 err=0.29]\n",
            "[i=240642 wall=703] [train aL=1.91e-02 err=0.01 nd=203/10000] [test aL=1.21e+00 err=0.30]\n",
            "[i=241642 wall=706] [train aL=1.76e-02 err=0.01 nd=177/10000] [test aL=1.20e+00 err=0.30]\n",
            "[i=242642 wall=709] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=243642 wall=711] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=244642 wall=714] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=245642 wall=717] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=246642 wall=720] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=247642 wall=723] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=248642 wall=726] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=249642 wall=728] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=250642 wall=732] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=251642 wall=734] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=252642 wall=737] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=253642 wall=740] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=254642 wall=743] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=255642 wall=746] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=256642 wall=748] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=257642 wall=751] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=258642 wall=754] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=259642 wall=757] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=260642 wall=760] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=261642 wall=763] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=262642 wall=766] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=263642 wall=769] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=264642 wall=772] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=265642 wall=774] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=266642 wall=777] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=267642 wall=780] [train aL=1.83e-02 err=0.01 nd=195/10000] [test aL=1.17e+00 err=0.29]\n",
            "[i=268642 wall=783] [train aL=1.77e-02 err=0.01 nd=181/10000] [test aL=1.19e+00 err=0.30]\n",
            "[i=269642 wall=786] [train aL=2.15e-02 err=0.01 nd=218/10000] [test aL=1.21e+00 err=0.30]\n",
            "[i=270642 wall=788] [train aL=1.77e-02 err=0.01 nd=188/10000] [test aL=1.19e+00 err=0.29]\n",
            "[i=271642 wall=792] [train aL=1.78e-02 err=0.01 nd=185/10000] [test aL=1.19e+00 err=0.29]\n",
            "[i=272642 wall=794] [train aL=1.79e-02 err=0.01 nd=188/10000] [test aL=1.20e+00 err=0.30]\n",
            "[i=273642 wall=797] [train aL=1.76e-02 err=0.01 nd=178/10000] [test aL=1.20e+00 err=0.29]\n",
            "[i=274642 wall=800] [train aL=1.76e-02 err=0.01 nd=180/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=275642 wall=803] [train aL=1.81e-02 err=0.01 nd=186/10000] [test aL=1.21e+00 err=0.30]\n",
            "[i=276642 wall=806] [train aL=1.76e-02 err=0.01 nd=183/10000] [test aL=1.19e+00 err=0.29]\n",
            "[i=277642 wall=809] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=278642 wall=811] [train aL=2.03e-02 err=0.01 nd=248/10000] [test aL=1.20e+00 err=0.30]\n",
            "[i=279642 wall=814] [train aL=1.76e-02 err=0.01 nd=181/10000] [test aL=1.20e+00 err=0.29]\n",
            "[i=280642 wall=817] [train aL=1.84e-02 err=0.01 nd=195/10000] [test aL=1.23e+00 err=0.30]\n",
            "[i=281642 wall=820] [train aL=1.82e-02 err=0.01 nd=192/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=282642 wall=823] [train aL=1.84e-02 err=0.01 nd=187/10000] [test aL=1.21e+00 err=0.30]\n",
            "[i=283642 wall=826] [train aL=1.81e-02 err=0.01 nd=186/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=284642 wall=829] [train aL=1.82e-02 err=0.01 nd=185/10000] [test aL=1.22e+00 err=0.30]\n",
            "[i=285642 wall=832] [train aL=1.79e-02 err=0.01 nd=181/10000] [test aL=1.23e+00 err=0.29]\n",
            "[i=286642 wall=834] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=287642 wall=837] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=288642 wall=840] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=289642 wall=843] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=290642 wall=846] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=291642 wall=849] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=292642 wall=852] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=293642 wall=855] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=294642 wall=857] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=295642 wall=860] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=296642 wall=863] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=297642 wall=866] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=298642 wall=869] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=299642 wall=871] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=300642 wall=874] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=301642 wall=877] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=302642 wall=880] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=303642 wall=883] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=304642 wall=886] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=305642 wall=889] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=306642 wall=892] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=307642 wall=894] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=308642 wall=897] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=309642 wall=900] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=310642 wall=903] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=311642 wall=906] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=312642 wall=909] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=313643 wall=912] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=314644 wall=915] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=315645 wall=917] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=316646 wall=920] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=317647 wall=923] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=318648 wall=926] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=319649 wall=929] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=320650 wall=932] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=321651 wall=934] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=322652 wall=938] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=323653 wall=940] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=324654 wall=943] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=325655 wall=946] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=326656 wall=949] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=327657 wall=952] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=328658 wall=955] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=329659 wall=957] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=330660 wall=960] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=331661 wall=963] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=332662 wall=966] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=333663 wall=969] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=334664 wall=972] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=335665 wall=975] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=336666 wall=978] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=337667 wall=980] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=338668 wall=983] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=339669 wall=986] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=340670 wall=989] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=341671 wall=992] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=342672 wall=995] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=343673 wall=998] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=344674 wall=1001] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=345675 wall=1004] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=346676 wall=1006] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=347677 wall=1010] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=348678 wall=1012] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=349679 wall=1015] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=350680 wall=1018] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=351681 wall=1021] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=352682 wall=1024] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=353683 wall=1026] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=354684 wall=1029] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=355685 wall=1032] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=356686 wall=1035] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=357687 wall=1038] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=358688 wall=1041] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=359689 wall=1043] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=360690 wall=1047] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=361691 wall=1049] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=362692 wall=1052] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=363693 wall=1055] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=364694 wall=1058] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=365695 wall=1061] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=366696 wall=1064] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=367697 wall=1066] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=368698 wall=1070] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=369699 wall=1072] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=370700 wall=1075] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=371701 wall=1078] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=372702 wall=1081] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=373703 wall=1084] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=374704 wall=1087] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=375705 wall=1089] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=376706 wall=1092] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=377707 wall=1095] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=378708 wall=1098] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=379709 wall=1101] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=380710 wall=1104] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=381711 wall=1107] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=382712 wall=1110] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=383713 wall=1112] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=384714 wall=1115] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=385715 wall=1118] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=386716 wall=1121] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=387717 wall=1124] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=388718 wall=1127] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=389719 wall=1130] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=390720 wall=1133] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=391721 wall=1135] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=392722 wall=1138] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=393723 wall=1141] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=394724 wall=1144] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=395725 wall=1147] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=396726 wall=1150] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=397727 wall=1152] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=398728 wall=1156] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=399729 wall=1158] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=400730 wall=1161] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=401731 wall=1164] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=402732 wall=1167] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=403733 wall=1170] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=404734 wall=1173] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=405735 wall=1175] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=406736 wall=1178] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=407737 wall=1181] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=408738 wall=1184] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=409739 wall=1187] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=410740 wall=1190] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=411741 wall=1193] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=412742 wall=1196] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=413743 wall=1199] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=414744 wall=1202] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=415745 wall=1205] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=416746 wall=1208] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=417747 wall=1210] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=418748 wall=1213] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=419749 wall=1217] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=420750 wall=1219] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=421751 wall=1222] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=422752 wall=1225] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=423753 wall=1228] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=424754 wall=1231] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=425755 wall=1234] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=426756 wall=1237] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=427757 wall=1240] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=428758 wall=1243] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=429759 wall=1245] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=430760 wall=1248] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=431761 wall=1251] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=432762 wall=1254] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=433763 wall=1257] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=434764 wall=1260] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=435765 wall=1263] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=436766 wall=1266] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=437767 wall=1269] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=438768 wall=1271] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n",
            "[i=439769 wall=1274] [train aL=1.76e-02 err=0.01 nd=176/10000] [test aL=1.21e+00 err=0.29]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notation\n",
        "\n",
        "The the code and in the article, the conventions differ.\n",
        "\n",
        "change 1\n",
        "- code: `loss = 1/alpha etc`\n",
        "- article: `loss = 1/alpha^2 etc`\n",
        "\n",
        "change 2\n",
        "- code: `1/h` at the end of the network\n",
        "- article: `1/sqrt(h)` at the end of the network\n",
        "\n",
        "```\n",
        "alpha_code = sqrt(h) alpha_article\n",
        "\n",
        "t_code = sqrt(h) / alpha_article * t_article\n",
        "t_article = alpha_code / h * t_code\n",
        "\n",
        "t_code / h = t_article / (sqrt(h) alpha_article)\n",
        "```"
      ],
      "metadata": {
        "id": "9fDsIAiDk9p0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import glob\n",
        "import functools\n",
        "import pickle\n",
        "import math\n",
        "import numpy as np\n",
        "import itertools\n",
        "# from grid import load, print_info\n",
        "\n",
        "####################################################################################################\n",
        "from collections import defaultdict, namedtuple\n",
        "\n",
        "Run = namedtuple(\"Run\", \"file, ctime, args, data\")\n",
        "GLOBALCACHE = defaultdict(dict)\n",
        "\n",
        "def deepmap(fun, data):\n",
        "    if isinstance(data, (list, tuple, set, frozenset)):\n",
        "        return type(data)(deepmap(fun, x) for x in data)\n",
        "\n",
        "    if isinstance(data, dict):\n",
        "        return {key: deepmap(fun, x) for key, x in data.items()}\n",
        "\n",
        "    return fun(data)\n",
        "\n",
        "def torch_to_numpy(data):\n",
        "    import torch\n",
        "\n",
        "    def fun(x):\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            return x.numpy()\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    return deepmap(fun, data)\n",
        "\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def to_dict(a):\n",
        "    if not isinstance(a, dict):\n",
        "        return a.__dict__\n",
        "    return a\n",
        "\n",
        "def load_file(f):\n",
        "    with open(f, \"rb\") as rb:\n",
        "        yield to_dict(pickle.load(rb))\n",
        "        yield pickle.load(rb)\n",
        "\n",
        "def _load_iter(\n",
        "    directory,\n",
        "    pred_args=None,\n",
        "    pred_run=None,\n",
        "    cache=True,\n",
        "    extractor=None,\n",
        "    convertion=None,\n",
        "    tqdm=identity,\n",
        "):\n",
        "    if extractor is not None:\n",
        "        cache = False\n",
        "\n",
        "    directory = os.path.normpath(directory)\n",
        "\n",
        "    if not os.path.isdir(directory):\n",
        "        raise NotADirectoryError(\"{} does not exists\".format(directory))\n",
        "\n",
        "    cache_runs = GLOBALCACHE[(directory, convertion)] if cache else dict()\n",
        "\n",
        "    for file in tqdm(sorted(glob.glob(os.path.join(directory, \"*.pk\")))):\n",
        "        ctime = os.path.getctime(file)\n",
        "\n",
        "        if file in cache_runs and ctime == cache_runs[file].ctime:\n",
        "            x = cache_runs[file]\n",
        "\n",
        "            if pred_args is not None and not pred_args(x.args):\n",
        "                continue\n",
        "\n",
        "            if pred_run is not None and not pred_run(x.data):\n",
        "                continue\n",
        "\n",
        "            yield (x.args, x.data)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            f = load_file(file)\n",
        "            args = next(f)\n",
        "\n",
        "            if pred_args is not None and not pred_args(args):\n",
        "                continue\n",
        "\n",
        "            data = next(f)\n",
        "        except (pickle.PickleError, FileNotFoundError, EOFError):\n",
        "            continue\n",
        "\n",
        "        if extractor is not None:\n",
        "            data = extractor(data)\n",
        "\n",
        "        if pred_run is not None and not pred_run(data):\n",
        "            continue\n",
        "\n",
        "        if convertion == \"torch_to_numpy\":\n",
        "            data = torch_to_numpy(data)\n",
        "        elif convertion == \"args\":\n",
        "            data = args\n",
        "        elif convertion == \"file_args\":\n",
        "            data = (file, args)\n",
        "        else:\n",
        "            assert convertion is None\n",
        "\n",
        "        x = Run(file=file, ctime=ctime, args=args, data=data)\n",
        "        cache_runs[file] = x\n",
        "\n",
        "        yield (x.args, x.data)\n",
        "\n",
        "def load_iter(\n",
        "    directory,\n",
        "    pred_args=None,\n",
        "    pred_run=None,\n",
        "    cache=True,\n",
        "    extractor=None,\n",
        "    convertion=None,\n",
        "    tqdm=identity,\n",
        "    with_args=False,\n",
        "):\n",
        "    for d in directory.split(\":\"):\n",
        "        for a, r in _load_iter(d, pred_args, pred_run, cache, extractor, convertion, tqdm):\n",
        "            if with_args:\n",
        "                yield a, r\n",
        "            else:\n",
        "                yield r\n",
        "\n",
        "def load(\n",
        "    directory,\n",
        "    pred_args=None,\n",
        "    pred_run=None,\n",
        "    cache=True,\n",
        "    extractor=None,\n",
        "    convertion=None,\n",
        "    tqdm=identity,\n",
        "    with_args=False,\n",
        "):\n",
        "    return list(load_iter(directory, pred_args, pred_run, cache, extractor, convertion, tqdm=tqdm, with_args=with_args))\n",
        "####################################################################################################\n",
        "\n",
        "def mean(x):\n",
        "    x = list(x)\n",
        "    return sum(x) / len(x)\n",
        "\n",
        "def median(x):\n",
        "    x = sorted(list(x))\n",
        "    return x[len(x) // 2]\n",
        "\n",
        "def triangle(a, b, c, d=None, slope=None, other=False, color=None, fmt=\"{:.2f}\", textpos=None):\n",
        "    import math\n",
        "\n",
        "    if slope is not None and d is None:\n",
        "        d = math.exp(math.log(c) + slope * (math.log(b) - math.log(a)))\n",
        "    if slope is not None and c is None:\n",
        "        c = math.exp(math.log(d) - slope * (math.log(b) - math.log(a)))\n",
        "    if color is None:\n",
        "        color = 'k'\n",
        "\n",
        "    plt.plot([a, b], [c, d], color=color)\n",
        "    if other:\n",
        "        plt.plot([a, b], [c, c], color=color)\n",
        "        plt.plot([b, b], [c, d], color=color)\n",
        "    else:\n",
        "        plt.plot([a, b], [d, d], color=color)\n",
        "        plt.plot([a, a], [c, d], color=color)\n",
        "\n",
        "    s = (math.log(d) - math.log(c)) / (math.log(b) - math.log(a))\n",
        "    if other:\n",
        "        x = math.exp(0.7 * math.log(b) + 0.3 * math.log(a))\n",
        "        y = math.exp(0.7 * math.log(c) + 0.3 * math.log(d))\n",
        "    else:\n",
        "        x = math.exp(0.7 * math.log(a) + 0.3 * math.log(b))\n",
        "        y = math.exp(0.7 * math.log(d) + 0.3 * math.log(c))\n",
        "    if textpos:\n",
        "        x = textpos[0]\n",
        "        y = textpos[1]\n",
        "    plt.annotate(fmt.format(s), (x, y), horizontalalignment='center', verticalalignment='center')\n",
        "    return s\n",
        "\n",
        "def nd(x, a):\n",
        "    assert not torch.isnan(x['outputs']).any()\n",
        "    return (a * x['outputs'] * x['labels'] < 1).nonzero().numel()\n",
        "\n",
        "def err(x):\n",
        "    assert not torch.isnan(x['outputs']).any()\n",
        "    return (x['outputs'] * x['labels'] <= 0).double().mean().item()\n",
        "\n",
        "def enserr(xs):\n",
        "    f = mean(x['outputs'] for x in xs)\n",
        "    y = xs[0]['labels']\n",
        "    assert all((x['labels'] == y).all() for x in xs)\n",
        "    return (f * y <= 0).double().mean().item()\n",
        "\n",
        "def var(outs, alpha):\n",
        "    otr = alpha * torch.stack(outs)\n",
        "    return otr.sub(otr.mean(0)).pow(2).mean(1).sum(0).item() / (otr.size(0) - 1)\n",
        "\n",
        "def texnum(x, mfmt='{}'):\n",
        "    m, e = \"{:e}\".format(x).split('e')\n",
        "    m, e = float(m), int(e)\n",
        "    mx = mfmt.format(m)\n",
        "    if e == 0:\n",
        "        if m == 1:\n",
        "            return \"1\"\n",
        "        return mx\n",
        "    ex = \"10^{{{}}}\".format(e)\n",
        "    if m == 1:\n",
        "        return ex\n",
        "    return \"{}\\;{}\".format(mx, ex)\n",
        "\n",
        "def logfilter(x, y, num):\n",
        "    import numpy as np\n",
        "    import scipy.ndimage\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    x = np.log(x)\n",
        "    xi = np.linspace(min(x), max(x), num)\n",
        "    yi = np.interp(xi, x, y)\n",
        "    yf = scipy.ndimage.filters.gaussian_filter1d(yi, 2)\n",
        "    return np.exp(xi), yf\n",
        "\n",
        "def yavg(xi, x, y):\n",
        "    import numpy as np\n",
        "    xi = np.array(xi)\n",
        "    xmin = min(np.min(x) for x in x)\n",
        "    xmax = min(np.max(x) for x in x)\n",
        "    xi = xi[np.logical_and(xmin < xi, xi < xmax)]\n",
        "    y = [np.interp(xi, np.array(x), np.array(y)) for x, y in zip(x, y)]\n",
        "    y = np.mean(y, axis=0)\n",
        "    return xi, y\n",
        "\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "@ticker.FuncFormatter\n",
        "def format_percent(x, pos=None):\n",
        "    x = 100 * x\n",
        "    if x % 1 > 0.05:\n",
        "        return r\"${:.1f}\\%$\".format(x)\n",
        "    else:\n",
        "        return r\"${:.0f}\\%$\".format(x)"
      ],
      "metadata": {
        "id": "xWhqtA5DlNNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "ZUQJ-YYvo2jT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(1, 1, figsize=(2.3, 2), dpi=130)\n",
        "\n",
        "\n",
        "plt.sca(ax1)\n",
        "x = torch.linspace(-0.7, 1.5, 200)\n",
        "plt.plot(x, x.neg().add(1).relu(), label='hinge')\n",
        "plt.plot(x, torch.nn.functional.softplus(x.neg().add(1), beta=20), label=r'soft-hinge $\\beta=20$')\n",
        "plt.plot(x, torch.nn.functional.softplus(x.neg().add(1), beta=5), label=r'soft-hinge $\\beta=5$')\n",
        "plt.plot(x, torch.nn.functional.softplus(x.neg().add(1), beta=1), label=r'soft-hinge $\\beta=1$')\n",
        "\n",
        "plt.legend(handlelength=1, labelspacing=0, frameon=False)\n",
        "plt.xlabel(r'$fy$')\n",
        "plt.ylabel(r'$\\ell(fy)$')\n",
        "plt.xlim(min(x), max(x))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('loss.pgf')"
      ],
      "metadata": {
        "id": "aDpHhTg_o5MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disentangling feature learning versus lazy learning from performance"
      ],
      "metadata": {
        "id": "pgYSKN_Oo_D8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G8wvbXTppAIt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
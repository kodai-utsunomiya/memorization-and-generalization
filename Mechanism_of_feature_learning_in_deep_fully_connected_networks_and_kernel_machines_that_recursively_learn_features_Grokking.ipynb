{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyODZl7nlcAWGb3xh04khP6n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kodai-utsunomiya/memorization-and-generalization/blob/main/Mechanism_of_feature_learning_in_deep_fully_connected_networks_and_kernel_machines_that_recursively_learn_features_Grokking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# classic_kernel"
      ],
      "metadata": {
        "id": "1vtGomCj9Q_D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CLQQf5gP8iIK"
      },
      "outputs": [],
      "source": [
        "'''Implementation of kernel functions.'''\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "def euclidean_distances(samples, centers, squared=True):\n",
        "    samples_norm = torch.sum(samples**2, dim=1, keepdim=True)\n",
        "    if samples is centers:\n",
        "        centers_norm = samples_norm\n",
        "    else:\n",
        "        centers_norm = torch.sum(centers**2, dim=1, keepdim=True)\n",
        "    centers_norm = torch.reshape(centers_norm, (1, -1))\n",
        "\n",
        "    distances = samples.mm(torch.t(centers))\n",
        "    distances.mul_(-2)\n",
        "    distances.add_(samples_norm)\n",
        "    distances.add_(centers_norm)\n",
        "    #print(centers_norm.size(), samples_norm.size(), distances.size())\n",
        "    if not squared:\n",
        "        distances.clamp_(min=0)\n",
        "        distances.sqrt_()\n",
        "\n",
        "    return distances\n",
        "\n",
        "\n",
        "def euclidean_distances_M(samples, centers, M, squared=True):\n",
        "\n",
        "    samples_norm = (samples @ M)  * samples\n",
        "    samples_norm = torch.sum(samples_norm, dim=1, keepdim=True)\n",
        "\n",
        "    if samples is centers:\n",
        "        centers_norm = samples_norm\n",
        "    else:\n",
        "        centers_norm = (centers @ M) * centers\n",
        "        centers_norm = torch.sum(centers_norm, dim=1, keepdim=True)\n",
        "\n",
        "    centers_norm = torch.reshape(centers_norm, (1, -1))\n",
        "\n",
        "    distances = samples.mm(M @ torch.t(centers))\n",
        "    distances.mul_(-2)\n",
        "    distances.add_(samples_norm)\n",
        "    distances.add_(centers_norm)\n",
        "\n",
        "    if not squared:\n",
        "        distances.clamp_(min=0)\n",
        "        distances.sqrt_()\n",
        "\n",
        "    return distances\n",
        "\n",
        "\n",
        "def gaussian(samples, centers, bandwidth):\n",
        "    '''Gaussian kernel.\n",
        "\n",
        "    Args:\n",
        "        samples: of shape (n_sample, n_feature).\n",
        "        centers: of shape (n_center, n_feature).\n",
        "        bandwidth: kernel bandwidth.\n",
        "\n",
        "    Returns:\n",
        "        kernel matrix of shape (n_sample, n_center).\n",
        "    '''\n",
        "    assert bandwidth > 0\n",
        "    kernel_mat = euclidean_distances(samples, centers)\n",
        "    kernel_mat.clamp_(min=0)\n",
        "    gamma = 1. / (2 * bandwidth ** 2)\n",
        "    kernel_mat.mul_(-gamma)\n",
        "    kernel_mat.exp_()\n",
        "\n",
        "    #print(samples.size(), centers.size(),\n",
        "    #      kernel_mat.size())\n",
        "    return kernel_mat\n",
        "\n",
        "\n",
        "def laplacian(samples, centers, bandwidth):\n",
        "    '''Laplacian kernel.\n",
        "\n",
        "    Args:\n",
        "        samples: of shape (n_sample, n_feature).\n",
        "        centers: of shape (n_center, n_feature).\n",
        "        bandwidth: kernel bandwidth.\n",
        "\n",
        "    Returns:\n",
        "        kernel matrix of shape (n_sample, n_center).\n",
        "    '''\n",
        "    assert bandwidth > 0\n",
        "    kernel_mat = euclidean_distances(samples, centers, squared=False)\n",
        "    kernel_mat.clamp_(min=0)\n",
        "    gamma = 1. / bandwidth\n",
        "    kernel_mat.mul_(-gamma)\n",
        "    kernel_mat.exp_()\n",
        "    return kernel_mat\n",
        "\n",
        "\n",
        "\n",
        "def laplacian_M(samples, centers, bandwidth, M):\n",
        "    assert bandwidth > 0\n",
        "    kernel_mat = euclidean_distances_M(samples, centers, M, squared=False)\n",
        "    kernel_mat.clamp_(min=0)\n",
        "    gamma = 1. / bandwidth\n",
        "    kernel_mat.mul_(-gamma)\n",
        "    kernel_mat.exp_()\n",
        "    return kernel_mat\n",
        "\n",
        "\n",
        "def dispersal(samples, centers, bandwidth, gamma):\n",
        "    '''Dispersal kernel.\n",
        "\n",
        "    Args:\n",
        "        samples: of shape (n_sample, n_feature).\n",
        "        centers: of shape (n_center, n_feature).\n",
        "        bandwidth: kernel bandwidth.\n",
        "        gamma: dispersal factor.\n",
        "\n",
        "    Returns:\n",
        "        kernel matrix of shape (n_sample, n_center).\n",
        "    '''\n",
        "    assert bandwidth > 0\n",
        "    kernel_mat = euclidean_distances(samples, centers)\n",
        "    kernel_mat.pow_(gamma / 2.)\n",
        "    kernel_mat.mul_(-1. / bandwidth)\n",
        "    kernel_mat.exp_()\n",
        "    return kernel_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# neural_model"
      ],
      "metadata": {
        "id": "WDk9FPe0-mdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable, Function\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from torch.nn.functional import upsample\n",
        "from copy import deepcopy\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Nonlinearity(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Nonlinearity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.relu(x)\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_classes=2):\n",
        "        super(Net, self).__init__()\n",
        "        bias = False\n",
        "        k = 1024\n",
        "        self.dim = dim\n",
        "        self.width = k\n",
        "        self.first = nn.Linear(dim, k, bias=bias)\n",
        "        self.fc = nn.Sequential(Nonlinearity(),\n",
        "                                nn.Linear(k, k, bias=bias),\n",
        "                                Nonlinearity(),\n",
        "                                nn.Linear(k, num_classes, bias=bias))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(self.first(x))"
      ],
      "metadata": {
        "id": "yuGqRTlu-kBH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trainer"
      ],
      "metadata": {
        "id": "LiQrloPF-c_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import time\n",
        "# import neural_model\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def visualize_M(M, idx):\n",
        "    d, _ = M.shape\n",
        "    SIZE = int(np.sqrt(d // 3))\n",
        "    F1 = np.diag(M[:SIZE**2, :SIZE**2]).reshape(SIZE, SIZE)\n",
        "    F2 = np.diag(M[SIZE**2:2*SIZE**2, SIZE**2:2*SIZE**2]).reshape(SIZE, SIZE)\n",
        "    F3 = np.diag(M[2*SIZE**2:, 2*SIZE**2:]).reshape(SIZE, SIZE)\n",
        "    F = np.stack([F1, F2, F3])\n",
        "    print(F.shape)\n",
        "    F = (F - F.min()) / (F.max() - F.min())\n",
        "    F = np.rollaxis(F, 0, 3)\n",
        "    plt.imshow(F)\n",
        "    plt.axis('off')\n",
        "    plt.savefig('./video_logs/' + str(idx).zfill(6) + '.png',\n",
        "                bbox_inches='tight', pad_inches = 0)\n",
        "    return F\n",
        "\n",
        "\n",
        "def train_network(train_loader, val_loader, test_loader,\n",
        "                  num_classes=2, name=None,\n",
        "                  save_frames=False):\n",
        "\n",
        "\n",
        "    for idx, batch in enumerate(train_loader):\n",
        "        inputs, labels = batch\n",
        "        _, dim = inputs.shape\n",
        "        break\n",
        "\n",
        "    # neural_model.Net\n",
        "    net = Net(dim, num_classes=num_classes)\n",
        "\n",
        "    params = 0\n",
        "    for idx, param in enumerate(list(net.parameters())):\n",
        "        size = 1\n",
        "        for idx in range(len(param.size())):\n",
        "            size *= param.size()[idx]\n",
        "            params += size\n",
        "    print(\"NUMBER OF PARAMS: \", params)\n",
        "\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=.1)\n",
        "\n",
        "    net.cuda()\n",
        "    num_epochs = 501\n",
        "    best_val_acc = 0\n",
        "    best_test_acc = 0\n",
        "    best_val_loss = np.float(\"inf\")\n",
        "    best_test_loss = 0\n",
        "\n",
        "    for i in range(num_epochs):\n",
        "        if save_frames:\n",
        "            net.cpu()\n",
        "            for idx, p in enumerate(net.parameters()):\n",
        "                if idx == 0:\n",
        "                    M = p.data.numpy()\n",
        "            M = M.T @ M\n",
        "            visualize_M(M, i)\n",
        "            net.cuda()\n",
        "\n",
        "        if i == 0 or i == 1:\n",
        "            net.cpu()\n",
        "            d = {}\n",
        "            d['state_dict'] = net.state_dict()\n",
        "            if name is not None:\n",
        "                torch.save(d, 'nn_models/' + name + '_trained_nn_' + str(i) + '.pth')\n",
        "            else:\n",
        "                torch.save(d, 'nn_models/trained_nn.pth')\n",
        "            net.cuda()\n",
        "\n",
        "        train_loss = train_step(net, optimizer, train_loader, save_frames=save_frames)\n",
        "        val_loss = val_step(net, val_loader)\n",
        "        test_loss = val_step(net, test_loader)\n",
        "        train_acc = get_acc(net, train_loader)\n",
        "        val_acc = get_acc(net, val_loader)\n",
        "        test_acc = get_acc(net, test_loader)\n",
        "\n",
        "        if val_acc >= best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_test_acc = test_acc\n",
        "            net.cpu()\n",
        "            d = {}\n",
        "            d['state_dict'] = net.state_dict()\n",
        "            if name is not None:\n",
        "                torch.save(d, 'nn_models/' + name + '_trained_nn.pth')\n",
        "            else:\n",
        "                torch.save(d, 'nn_models/trained_nn.pth')\n",
        "            net.cuda()\n",
        "\n",
        "        if val_loss <= best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_test_loss = test_loss\n",
        "\n",
        "        print(\"Epoch: \", i,\n",
        "              \"Train Loss: \", train_loss, \"Test Loss: \", test_loss,\n",
        "              \"Train Acc: \", train_acc, \"Test Acc: \", test_acc,\n",
        "              \"Best Val Acc: \", best_val_acc, \"Best Val Loss: \", best_val_loss,\n",
        "              \"Best Test Acc: \", best_test_acc, \"Best Test Loss: \", best_test_loss)\n",
        "\n",
        "\n",
        "def get_data(loader):\n",
        "    X = []\n",
        "    y = []\n",
        "    for idx, batch in enumerate(loader):\n",
        "        inputs, labels = batch\n",
        "        X.append(inputs)\n",
        "        y.append(labels)\n",
        "    return torch.cat(X, dim=0), torch.cat(y, dim=0)\n",
        "\n",
        "\n",
        "def train_step(net, optimizer, train_loader, save_frames=False):\n",
        "    net.train()\n",
        "    start = time.time()\n",
        "    train_loss = 0.\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        inputs, labels = batch\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        targets = labels\n",
        "        output = net(Variable(inputs))\n",
        "        target = Variable(targets)\n",
        "        loss = torch.mean(torch.pow(output - target, 2))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.cpu().data.numpy() * len(inputs)\n",
        "    end = time.time()\n",
        "    print(\"Time: \", end - start)\n",
        "    train_loss = train_loss / len(train_loader.dataset)\n",
        "    return train_loss\n",
        "\n",
        "def val_step(net, val_loader):\n",
        "    net.eval()\n",
        "    val_loss = 0.\n",
        "\n",
        "    for batch_idx, batch in enumerate(val_loader):\n",
        "        inputs, labels = batch\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        targets = labels\n",
        "        with torch.no_grad():\n",
        "            output = net(Variable(inputs))\n",
        "            target = Variable(targets)\n",
        "        loss = torch.mean(torch.pow(output - target, 2))\n",
        "        val_loss += loss.cpu().data.numpy() * len(inputs)\n",
        "    val_loss = val_loss / len(val_loader.dataset)\n",
        "    return val_loss\n",
        "\n",
        "\n",
        "def get_acc(net, loader):\n",
        "    net.eval()\n",
        "    count = 0\n",
        "    for batch_idx, batch in enumerate(loader):\n",
        "        inputs, targets = batch\n",
        "        with torch.no_grad():\n",
        "            output = net(Variable(inputs).cuda())\n",
        "            target = Variable(targets).cuda()\n",
        "\n",
        "        preds = torch.argmax(output, dim=-1)\n",
        "        labels = torch.argmax(target, dim=-1)\n",
        "\n",
        "        count += torch.sum(labels == preds).cpu().data.numpy()\n",
        "    return count / len(loader.dataset) * 100"
      ],
      "metadata": {
        "id": "sgbQDpoA-b-N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# rfm"
      ],
      "metadata": {
        "id": "gN0lhWVjAiyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hickle==5.0.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khb8bN1cBCes",
        "outputId": "289cdc19-a7c9-4827-ba5c-3f29d0ea5c57"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hickle==5.0.2\n",
            "  Downloading hickle-5.0.2-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from hickle==5.0.2) (3.11.0)\n",
            "Requirement already satisfied: numpy!=1.20,>=1.8 in /usr/local/lib/python3.10/dist-packages (from hickle==5.0.2) (1.26.4)\n",
            "Downloading hickle-5.0.2-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.9/107.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hickle\n",
            "Successfully installed hickle-5.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from numpy.linalg import solve\n",
        "# import classic_kernel\n",
        "from tqdm import tqdm\n",
        "import hickle\n",
        "\n",
        "def laplace_kernel_M(pair1, pair2, bandwidth, M):\n",
        "    # classic_kernel.laplacian_M\n",
        "    return laplacian_M(pair1, pair2, bandwidth, M)\n",
        "\n",
        "\n",
        "def get_grads(X, sol, L, P, batch_size=2):\n",
        "    M = 0.\n",
        "\n",
        "    num_samples = 20000\n",
        "    indices = np.random.randint(len(X), size=num_samples)\n",
        "\n",
        "    if len(X) > len(indices):\n",
        "        x = X[indices, :]\n",
        "    else:\n",
        "        x = X\n",
        "\n",
        "    K = laplace_kernel_M(X, x, L, P)\n",
        "\n",
        "    # classic_kernel.euclidean_distances_M\n",
        "    dist = euclidean_distances_M(X, x, P, squared=False)\n",
        "    dist = torch.where(dist < 1e-10, torch.zeros(1).float(), dist)\n",
        "\n",
        "    K = K/dist\n",
        "    K[K == float(\"Inf\")] = 0.\n",
        "\n",
        "    a1 = torch.from_numpy(sol.T).float()\n",
        "    n, d = X.shape\n",
        "    n, c = a1.shape\n",
        "    m, d = x.shape\n",
        "\n",
        "    a1 = a1.reshape(n, c, 1)\n",
        "    X1 = (X @ P).reshape(n, 1, d)\n",
        "    step1 = a1 @ X1\n",
        "    del a1, X1\n",
        "    step1 = step1.reshape(-1, c*d)\n",
        "\n",
        "    step2 = K.T @ step1\n",
        "    del step1\n",
        "\n",
        "    step2 = step2.reshape(-1, c, d)\n",
        "\n",
        "    a2 = torch.from_numpy(sol).float()\n",
        "    step3 = (a2 @ K).T\n",
        "\n",
        "    del K, a2\n",
        "\n",
        "    step3 = step3.reshape(m, c, 1)\n",
        "    x1 = (x @ P).reshape(m, 1, d)\n",
        "    step3 = step3 @ x1\n",
        "\n",
        "    G = (step2 - step3) * -1/L\n",
        "\n",
        "    M = 0.\n",
        "\n",
        "    bs = batch_size\n",
        "    batches = torch.split(G, bs)\n",
        "    for i in tqdm(range(len(batches))):\n",
        "        grad = batches[i].cuda()\n",
        "        gradT = torch.transpose(grad, 1, 2)\n",
        "        M += torch.sum(gradT @ grad, dim=0).cpu()\n",
        "        del grad, gradT\n",
        "    torch.cuda.empty_cache()\n",
        "    M /= len(G)\n",
        "    M = M.numpy()\n",
        "\n",
        "    return M\n",
        "\n",
        "\n",
        "def rfm(train_loader, val_loader, test_loader,\n",
        "        iters=3, name=None, batch_size=2, reg=1e-3,\n",
        "        train_acc=False):\n",
        "\n",
        "    L = 10\n",
        "\n",
        "    X_train, y_train = get_data(train_loader)\n",
        "    X_val, y_val = get_data(val_loader)\n",
        "    X_test, y_test = get_data(test_loader)\n",
        "\n",
        "    n, d = X_train.shape\n",
        "\n",
        "    M = np.eye(d, dtype='float32')\n",
        "\n",
        "    for i in range(iters):\n",
        "        K_train = laplace_kernel_M(X_train, X_train, L, torch.from_numpy(M)).numpy()\n",
        "        sol = solve(K_train + reg * np.eye(len(K_train)), y_train).T\n",
        "\n",
        "        if train_acc:\n",
        "            preds = (sol @ K_train).T\n",
        "            y_pred = torch.from_numpy(preds)\n",
        "            preds = torch.argmax(y_pred, dim=-1)\n",
        "            labels = torch.argmax(y_train, dim=-1)\n",
        "            count = torch.sum(labels == preds).numpy()\n",
        "            print(\"Round \" + str(i) + \" Train Acc: \", count / len(labels))\n",
        "\n",
        "        K_test = laplace_kernel_M(X_train, X_test, L, torch.from_numpy(M)).numpy()\n",
        "        preds = (sol @ K_test).T\n",
        "        print(\"Round \" + str(i) + \" MSE: \", np.mean(np.square(preds - y_test.numpy())))\n",
        "        y_pred = torch.from_numpy(preds)\n",
        "        preds = torch.argmax(y_pred, dim=-1)\n",
        "        labels = torch.argmax(y_test, dim=-1)\n",
        "        count = torch.sum(labels == preds).numpy()\n",
        "        print(\"Round \" + str(i) + \" Acc: \", count / len(labels))\n",
        "\n",
        "        M  = get_grads(X_train, sol, L, torch.from_numpy(M), batch_size=batch_size)\n",
        "        if name is not None:\n",
        "            hickle.dump(M, 'saved_Ms/M_' + name + '_' + str(i) + '.h')\n",
        "\n",
        "    K_train = laplace_kernel_M(X_train, X_train, L, torch.from_numpy(M)).numpy()\n",
        "    sol = solve(K_train + reg * np.eye(len(K_train)), y_train).T\n",
        "    K_test = laplace_kernel_M(X_train, X_test, L, torch.from_numpy(M)).numpy()\n",
        "    preds = (sol @ K_test).T\n",
        "    mse = np.mean(np.square(preds - y_test.numpy()))\n",
        "    print(\"Final MSE: \", mse)\n",
        "    y_pred = torch.from_numpy(preds)\n",
        "    preds = torch.argmax(y_pred, dim=-1)\n",
        "    labels = torch.argmax(y_test, dim=-1)\n",
        "    count = torch.sum(labels == preds).numpy()\n",
        "    print(\" Final Acc: \", count / len(labels))\n",
        "    return mse\n",
        "\n",
        "\n",
        "def get_data(loader):\n",
        "    X = []\n",
        "    y = []\n",
        "    for idx, batch in enumerate(loader):\n",
        "        inputs, labels = batch\n",
        "        X.append(inputs)\n",
        "        y.append(labels)\n",
        "    return torch.cat(X, dim=0), torch.cat(y, dim=0)"
      ],
      "metadata": {
        "id": "Q8bozCRiAf9X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install visdom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAr8UIlTBhMK",
        "outputId": "984363a6-6a69-4acc-ddae-c0588514f444"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting visdom\n",
            "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.10/dist-packages (from visdom) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from visdom) (1.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from visdom) (2.32.3)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from visdom) (6.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from visdom) (1.16.0)\n",
            "Collecting jsonpatch (from visdom)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from visdom) (1.8.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from visdom) (3.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from visdom) (9.4.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch->visdom)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (2024.7.4)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Building wheels for collected packages: visdom\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408195 sha256=f21357943efda574403f7c7366c8c3526f09e783ceee7de744469dd7d3817b17\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/29/49/5bed207bac4578e4d2c0c5fc0226bfd33a7e2953ea56356855\n",
            "Successfully built visdom\n",
            "Installing collected packages: jsonpointer, jsonpatch, visdom\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 visdom-0.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# grokking_main"
      ],
      "metadata": {
        "id": "wDD3XNi39ZTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# デバイスの選択\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_RuTp4OGNxs",
        "outputId": "84f589ac-3099-4499-a9a7-e5aaa4824dbe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# import trainer as t\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "import torch.backends.cudnn as cudnn\n",
        "# import rfm\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.linalg import norm\n",
        "from random import randint\n",
        "import visdom\n",
        "# import eigenpro_rtfm as erfm\n",
        "import hickle\n",
        "# import neural_model\n",
        "\n",
        "vis = visdom.Visdom('http://127.0.0.1', use_incoming_socket=False)\n",
        "vis.close(env='main')\n",
        "\n",
        "SEED = 5636\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "\n",
        "SIZE = 96\n",
        "h, w = SIZE, SIZE\n",
        "locationx = np.array([randint(0, h-1) for i in range(10)], dtype='int')\n",
        "locationy = np.array([randint(0, w-1) for i in range(10)], dtype='int')\n",
        "\n",
        "shiftx_l = np.array(locationx - 2, dtype='int')\n",
        "shiftx_r = np.array(locationx + 3, dtype='int')\n",
        "shifty_l = np.array(locationy - 2, dtype='int')\n",
        "shifty_r = np.array(locationy + 3, dtype='int')\n",
        "\n",
        "\n",
        "def one_hot_data(dataset, num_samples=-1):\n",
        "    labelset = {}\n",
        "    for i in range(10):\n",
        "        one_hot = torch.zeros(10)\n",
        "        one_hot[i] = 1\n",
        "        labelset[i] = one_hot\n",
        "\n",
        "    subset = [(ex, label) for idx, (ex, label) in enumerate(dataset) \\\n",
        "              if idx < num_samples and label == 0 or label == 9]\n",
        "\n",
        "    adjusted = []\n",
        "\n",
        "    count = 0\n",
        "    for idx, (ex, label) in enumerate(subset):\n",
        "        ex[:, 2:7, 7:12] = 0.\n",
        "        if label == 9:\n",
        "            count += 1\n",
        "            ex[:, 2:7, 7:12] = 1.\n",
        "        if idx < 10:\n",
        "            vis.image(ex)\n",
        "        ex = ex.flatten()\n",
        "        adjusted.append((ex, labelset[label]))\n",
        "    return adjusted\n",
        "\n",
        "def split(trainset, p=.8):\n",
        "    train, val = train_test_split(trainset, train_size=p)\n",
        "    return train, val\n",
        "\n",
        "def load_from_net(SIZE=64, path='./nn_models/trained_nn.pth'):\n",
        "    dim = 3 * SIZE * SIZE\n",
        "    # neural_model.Net\n",
        "    net = Net(dim, num_classes=10)\n",
        "\n",
        "    d = torch.load(path)\n",
        "    net.load_state_dict(d['state_dict'])\n",
        "    for idx, p in enumerate(net.parameters()):\n",
        "        if idx == 0:\n",
        "            M = p.data.numpy()\n",
        "\n",
        "    M = M.T @ M\n",
        "    return M\n",
        "\n",
        "def main():\n",
        "    cudnn.benchmark = True\n",
        "    global SIZE\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    path = '~/datasets/'\n",
        "    trainset = torchvision.datasets.STL10(root=path,\n",
        "                                          split='train',\n",
        "                                          transform=transform,\n",
        "                                          download=True)\n",
        "\n",
        "    trainset = one_hot_data(trainset, num_samples=500)\n",
        "    trainset, valset = split(trainset, p=.8)\n",
        "\n",
        "    print(\"Train Size: \", len(trainset), \"Val Size: \", len(valset))\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=1024,\n",
        "                                              shuffle=True, num_workers=2)\n",
        "\n",
        "    valloader = torch.utils.data.DataLoader(valset, batch_size=100,\n",
        "                                            shuffle=False, num_workers=1)\n",
        "\n",
        "    testset = torchvision.datasets.STL10(root=path,\n",
        "                                         split='test',\n",
        "                                         transform=transform,\n",
        "                                         download=True)\n",
        "    testset = one_hot_data(testset, num_samples=1e10)\n",
        "    print(len(testset))\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=1024,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "    name = 'grokking'\n",
        "    # rfm.rfm\n",
        "    rfm(trainloader, valloader, testloader,\n",
        "            name=name,\n",
        "            iters=5,\n",
        "            train_acc=True, reg=1e-3)\n",
        "\n",
        "    # trainer.train_network\n",
        "    train_network(trainloader, valloader, testloader,\n",
        "                    name=name)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wb1GwGC9WdC",
        "outputId": "1e8ec883-03d7-4041-883b-81e682079446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:visdom:Setting up a new session...\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 203, in _new_conn\n",
            "    sock = connection.create_connection(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
            "    sock.connect(sa)\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 791, in urlopen\n",
            "    response = self._make_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 497, in _make_request\n",
            "    conn.request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 395, in request\n",
            "    self.endheaders()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1278, in endheaders\n",
            "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1038, in _send_output\n",
            "    self.send(msg)\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 976, in send\n",
            "    self.connect()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 243, in connect\n",
            "    self.sock = self._new_conn()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 218, in _new_conn\n",
            "    raise NewConnectionError(\n",
            "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7a9f709a1150>: Failed to establish a new connection: [Errno 111] Connection refused\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 667, in send\n",
            "    resp = conn.urlopen(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 845, in urlopen\n",
            "    retries = retries.increment(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\", line 515, in increment\n",
            "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
            "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=8097): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a9f709a1150>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/visdom/__init__.py\", line 756, in _send\n",
            "    return self._handle_post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/visdom/__init__.py\", line 720, in _handle_post\n",
            "    r = self.session.post(url, data=data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 637, in post\n",
            "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 700, in send\n",
            "    raise ConnectionError(e, request=request)\n",
            "requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=8097): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a9f709a1150>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "WARNING:visdom:Without the incoming socket you cannot receive events from the server or register event handlers to your Visdom client.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 203, in _new_conn\n",
            "    sock = connection.create_connection(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
            "    sock.connect(sa)\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 791, in urlopen\n",
            "    response = self._make_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 497, in _make_request\n",
            "    conn.request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 395, in request\n",
            "    self.endheaders()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1278, in endheaders\n",
            "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1038, in _send_output\n",
            "    self.send(msg)\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 976, in send\n",
            "    self.connect()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 243, in connect\n",
            "    self.sock = self._new_conn()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 218, in _new_conn\n",
            "    raise NewConnectionError(\n",
            "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7a9f709a2440>: Failed to establish a new connection: [Errno 111] Connection refused\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 667, in send\n",
            "    resp = conn.urlopen(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 845, in urlopen\n",
            "    retries = retries.increment(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\", line 515, in increment\n",
            "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
            "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=8097): Max retries exceeded with url: /close (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a9f709a2440>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/visdom/__init__.py\", line 756, in _send\n",
            "    return self._handle_post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/visdom/__init__.py\", line 720, in _handle_post\n",
            "    r = self.session.post(url, data=data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 637, in post\n",
            "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 700, in send\n",
            "    raise ConnectionError(e, request=request)\n",
            "requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=8097): Max retries exceeded with url: /close (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a9f709a2440>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception in user code:\n",
            "------------------------------------------------------------\n",
            "Exception in user code:\n",
            "------------------------------------------------------------\n",
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to /root/datasets/stl10_binary.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 1130856448/2640397119 [00:52<01:37, 15463426.12it/s]"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNuQiyliHKGnG3IKhspUSmE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kodai-utsunomiya/memorization-and-generalization/blob/main/numerical_experiments/1_time_scale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $(k, d)$-Sparse Parity Task\n",
        "\n",
        "$d$-個の数字の内の $k$ 個の数字のパリティを計算する（$k \\le d$）という問題\n",
        "\n",
        "# データセット\n",
        "\n",
        "---\n",
        "\n",
        "- $\\mathcal{D}_{k, d} = \\{ (\\boldsymbol{x}_i , y_i) \\}_{i=1}^n$\n",
        "    - $n$ 個の学習データ\n",
        "    - $\\boldsymbol{x}_i \\in \\{ 0,1 \\}^d$：バイナリーベクトル．$\\boldsymbol{x}_i \\sim \\text{Unif} \\left( \\{0,1\\}^d \\right)$\n",
        "    - $y_i = \\left(\\sum_{i}^k x^{(i)} \\right) \\text{mod} \\hspace{2mm} 2$ ：最初の $k$  個の数字（clean digits）のパリティ\n",
        "\n",
        "<br>\n",
        "\n",
        "  - $\\boldsymbol{x}_i$ の残りの $d-k$ 個の数字（noisy digits）は $y_i$ とは無関係\n",
        "\n",
        "<br>\n",
        "\n",
        "- 例：\n",
        "    - $(3, 30)$-sparse parity dataset\n",
        "        - $\\boldsymbol{x}_1$：<font color=\"blue\">000</font>$110010110001010111001001011$，  $y_1 = 0$\n",
        "        - $\\boldsymbol{x}_2$：<font color=\"blue\">010</font>$110010110001010111001001011$，  $y_2 = 1$\n",
        "            \n",
        "            $\\hspace{2mm} \\vdots$\n",
        "            \n",
        "\n",
        "<br>\n",
        "\n",
        "- 学習データをまとめて，$\\mathcal{X} = \\left[ \\boldsymbol{x}_1, \\ldots, \\boldsymbol{x}_n \\right] \\in \\mathbb{R}^{n \\times d}$，$\\mathcal{Y} = \\left[ y_1 \\ldots, y_n \\right] \\in \\mathbb{R}^n$ と行列表記"
      ],
      "metadata": {
        "id": "vnvu-UoTZ-Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, Subset\n",
        "\n",
        "def _generate_unique_binary_strings(n, total_size, data_seed):\n",
        "    \"\"\"\n",
        "    ユニークなバイナリ文字列を生成する関数\n",
        "\n",
        "    Parameters:\n",
        "    - n: バイナリ文字列の長さ\n",
        "    - total_size: 必要なユニークなバイナリ文字列の数\n",
        "    - data_seed: ランダムシード\n",
        "\n",
        "    Returns:\n",
        "    - list: ユニークなバイナリ文字列のリスト．各バイナリ文字列は長さ n のタプル\n",
        "    \"\"\"\n",
        "    np.random.seed(data_seed)\n",
        "    unique_binary_strings = set()\n",
        "\n",
        "    # 必要な数のユニークなバイナリ文字列が得られるまで繰り返す\n",
        "    while len(unique_binary_strings) < total_size:\n",
        "        binary_string = tuple(np.random.randint(2, size=n))\n",
        "        unique_binary_strings.add(binary_string)\n",
        "    return list(unique_binary_strings)\n",
        "\n",
        "def _prepare_data(binary_strings, k):\n",
        "    \"\"\"\n",
        "    入力データと出力ラベルを準備する関数\n",
        "\n",
        "    Parameters:\n",
        "    - binary_strings: バイナリ文字列のリスト．各バイナリ文字列は長さ n のタプル\n",
        "    - k: 出力ラベルの計算に使用する最初の k 個のビット\n",
        "\n",
        "    Returns:\n",
        "    - tuple: (入力データ, 出力ラベル)\n",
        "      - 入力データ: バイナリ文字列を NumPy 配列として保持し，最後にバイアス列を追加\n",
        "      - 出力ラベル: 最初の k ビットの合計の 2 で割った余りとして計算\n",
        "    \"\"\"\n",
        "    # バイナリ文字列をNumPy配列に変換\n",
        "    inputs = np.array(binary_strings, dtype=np.float32)\n",
        "\n",
        "    # 出力ラベルを計算 (最初の k ビットの合計を 2 で割った余り)\n",
        "    outputs = np.sum(inputs[:, :k], axis=-1) % 2\n",
        "\n",
        "    # 出力ラベルを 0 -> -1, 1 -> 1 に変換（ヒンジ損失用）\n",
        "    outputs = 2 * outputs - 1\n",
        "\n",
        "    return inputs, outputs\n",
        "\n",
        "def _normalize_data(inputs):\n",
        "    \"\"\"\n",
        "    データの中心化と標準化を行うヘルパー関数\n",
        "\n",
        "    Parameters:\n",
        "    - inputs: 正規化対象の入力データ (NumPy配列)\n",
        "\n",
        "    Returns:\n",
        "    - 正規化された入力データ\n",
        "    \"\"\"\n",
        "    # NumPy配列をPyTorchテンソルに変換\n",
        "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
        "\n",
        "    # 1. データの中心化（平均を0に）\n",
        "    inputs = inputs - inputs.mean(dim=0)\n",
        "\n",
        "    # 2. 標準化（各サンプルのノルムで割る）．データが単位ノルムを持つようにスケーリングされる\n",
        "    norm = inputs.flatten(1).norm(dim=1).view(-1, *(1,) * (inputs.dim() - 1))\n",
        "    inputs = (inputs[0].numel() ** 0.5) * inputs / norm\n",
        "\n",
        "    return inputs.numpy()  # NumPy配列に戻す\n",
        "\n",
        "def get_binary_dataset(n, k, train_size, test_size, data_seed, normalize=False, device=None):\n",
        "    \"\"\"\n",
        "    二値分類用にデータセットを取得する関数\n",
        "\n",
        "    Parameters:\n",
        "    - n: バイナリ文字列の長さ\n",
        "    - k: 出力ラベルの計算に使用する最初の k 個のビット\n",
        "    - train_size: 訓練データのサイズ\n",
        "    - test_size: テストデータのサイズ\n",
        "    - data_seed: ランダムシード\n",
        "    - normalize: データを正規化するかどうかを示すフラグ\n",
        "    - device: データを移動するデバイス\n",
        "\n",
        "    Returns:\n",
        "    - tuple: ((訓練データのx, 訓練データのy), (テストデータのx, テストデータのy))\n",
        "      - 訓練データのx: 訓練データの入力データ\n",
        "      - 訓練データのy: 訓練データの出力ラベル\n",
        "      - テストデータのx: テストデータの入力データ\n",
        "      - テストデータのy: テストデータの出力ラベル\n",
        "    \"\"\"\n",
        "    total_size = train_size + test_size\n",
        "\n",
        "    # ユニークなバイナリ文字列を生成\n",
        "    binary_strings = _generate_unique_binary_strings(n, total_size, data_seed)\n",
        "\n",
        "    # 入力データと出力ラベルを準備\n",
        "    inputs, outputs = _prepare_data(binary_strings, k)\n",
        "\n",
        "    # 正規化を行う場合（中心化と標準化）\n",
        "    if normalize:\n",
        "        inputs = _normalize_data(inputs)\n",
        "\n",
        "    # データのインデックスをシャッフル\n",
        "    indices = np.random.permutation(total_size)\n",
        "\n",
        "    # 訓練データとテストデータのインデックスを分割\n",
        "    train_indices = indices[:train_size]\n",
        "    test_indices = indices[train_size:]\n",
        "\n",
        "    # 訓練データとテストデータをNumPy配列からPyTorchテンソルに変換\n",
        "    train_inputs = torch.tensor(inputs[train_indices], dtype=torch.float32)\n",
        "    train_outputs = torch.tensor(outputs[train_indices], dtype=torch.float32)\n",
        "    test_inputs = torch.tensor(inputs[test_indices], dtype=torch.float32)\n",
        "    test_outputs = torch.tensor(outputs[test_indices], dtype=torch.float32)\n",
        "\n",
        "    # データを指定されたデバイスに移動\n",
        "    if device:\n",
        "        train_inputs, train_outputs = train_inputs.to(device), train_outputs.to(device)\n",
        "        test_inputs, test_outputs = test_inputs.to(device), test_outputs.to(device)\n",
        "\n",
        "    return (train_inputs, train_outputs), (test_inputs, test_outputs)"
      ],
      "metadata": {
        "id": "LYe1p9BP1mgt"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = get_binary_dataset(n=30, k=3, train_size=900, test_size=100, data_seed=42, normalize=False)"
      ],
      "metadata": {
        "id": "PMHgp_d5bUAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"訓練データのパターン\\n\", a[0][0].shape)\n",
        "print(a[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6rdETkDbmPW",
        "outputId": "6b1dba6a-a256-4e92-cce9-4908f7fd08ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "訓練データのパターン\n",
            " torch.Size([900, 30])\n",
            "tensor([[1., 0., 1.,  ..., 0., 0., 1.],\n",
            "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
            "        [0., 1., 1.,  ..., 0., 1., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 1., 0., 1.],\n",
            "        [0., 1., 1.,  ..., 0., 0., 1.],\n",
            "        [1., 0., 0.,  ..., 1., 0., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"訓練データのラベル\\n\", a[0][1].shape)\n",
        "print(a[0][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ZOBMuW2IEG",
        "outputId": "504b26e4-4731-4332-8d35-0e1eaf46f14a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "訓練データのラベル\n",
            " torch.Size([900])\n",
            "tensor([-1.,  1., -1., -1., -1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
            "        -1., -1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
            "         1., -1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
            "        -1., -1., -1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,\n",
            "        -1.,  1., -1., -1., -1., -1., -1., -1., -1.,  1., -1.,  1., -1.,  1.,\n",
            "        -1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1.,\n",
            "         1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,\n",
            "         1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,  1.,  1., -1., -1., -1.,\n",
            "        -1., -1.,  1.,  1.,  1., -1., -1., -1., -1., -1.,  1.,  1., -1.,  1.,\n",
            "        -1.,  1.,  1., -1., -1.,  1., -1., -1., -1.,  1., -1.,  1., -1., -1.,\n",
            "         1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1.,\n",
            "         1.,  1., -1., -1., -1., -1.,  1., -1., -1.,  1.,  1., -1., -1., -1.,\n",
            "        -1., -1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,\n",
            "        -1., -1., -1.,  1., -1.,  1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,\n",
            "         1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,\n",
            "         1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,\n",
            "         1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1.,\n",
            "         1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,\n",
            "        -1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1.,\n",
            "        -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,\n",
            "        -1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,\n",
            "        -1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1., -1.,\n",
            "        -1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
            "         1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1.,\n",
            "         1., -1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
            "         1., -1., -1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
            "         1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
            "        -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,\n",
            "         1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
            "        -1.,  1.,  1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1., -1.,  1., -1., -1.,  1., -1.,  1., -1., -1.,  1.,\n",
            "         1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,\n",
            "        -1.,  1.,  1., -1., -1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,\n",
            "         1., -1., -1., -1., -1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,\n",
            "        -1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,\n",
            "        -1.,  1., -1.,  1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,\n",
            "         1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "         1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1., -1., -1.,\n",
            "        -1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1.,  1.,\n",
            "         1., -1., -1., -1.,  1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,\n",
            "        -1., -1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.,  1.,\n",
            "        -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "        -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,  1., -1., -1.,\n",
            "         1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,  1., -1.,\n",
            "         1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,\n",
            "        -1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
            "         1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1., -1., -1., -1.,  1.,\n",
            "        -1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,\n",
            "         1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,\n",
            "        -1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1.,\n",
            "         1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1.,\n",
            "        -1.,  1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,\n",
            "        -1.,  1., -1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,  1.,\n",
            "         1.,  1., -1.,  1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,\n",
            "        -1.,  1., -1., -1.,  1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,\n",
            "        -1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1., -1., -1.,  1., -1., -1.,\n",
            "        -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "        -1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,\n",
            "         1., -1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.,\n",
            "         1., -1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,\n",
            "        -1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,\n",
            "         1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "        -1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,\n",
            "        -1.,  1., -1.,  1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a[1][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc1uo1Bg2M5b",
        "outputId": "77fc0586-8a60-4c0e-af27-9e18b704db26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a[1][1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNeKhB2d2O0L",
        "outputId": "ee03d1d6-e3c6-4538-ffc0-b1d179930355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# モデル\n",
        "\n",
        "---\n",
        "\n",
        "$ F(\\boldsymbol{w}, \\boldsymbol{x}) \\equiv \\alpha \\left\\lbrack f(\\boldsymbol{w}, \\boldsymbol{x}) - f(\\boldsymbol{w}_0, \\boldsymbol{x}) \\right\\rbrack $ という形をしたモデルの学習を考える（これを予測器として使用し，$\\boldsymbol{w}$を学習）"
      ],
      "metadata": {
        "id": "DaTTQLVtbhHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FCクラス．<font color=\"green\"> $f(\\boldsymbol{x})$ の定義 </font>\n",
        "\n",
        "### ネットワークの構造\n",
        "\n",
        "1. **入力層**: 次元数 $d$ の入力を受け取る．\n",
        "2. **隠れ層**: 層数 $L$ の隠れ層があり，各隠れ層のユニット数は $h$．\n",
        "3. **出力層**: 最終層は出力がスカラー値である 1 次元のベクトルを生成．\n",
        "\n",
        "<br>\n",
        "\n",
        "### 層ごとの計算\n",
        "\n",
        "1. **初期化**:\n",
        "   - 隠れ層 $i$ の重み行列 $W_i$ は，次のように初期化：\n",
        "     \n",
        "     $\n",
        "     W_i \\sim \\mathcal{N}(0, 1)\n",
        "     $\n",
        "\n",
        "     ここで，$W_i$ のサイズは $ h \\times \\text{hh}_{i}$ ．\n",
        "     \n",
        "     $\\text{hh}_{i} $ は前の層の出力ユニット数．\n",
        "\n",
        "   - メモリ効率を考慮し，重み行列を分割：\n",
        "     \n",
        "     \\begin{aligned} W_i =  \\begin{bmatrix}\n",
        "        W_i^{(0)} \\\\\n",
        "        W_i^{(1)} \\\\\n",
        "        \\vdots \\\\\n",
        "        W_i^{(n-1)}\n",
        "        \\end{bmatrix}  \\end{aligned}\n",
        "     \n",
        "     各部分行列 $W_i^{(j)}$ はサイズ $m \\times \\text{hh}_{i}$．ここで，$m$ は分割サイズ．\n",
        "\n",
        "<br>\n",
        "\n",
        "2. **順伝播計算**:\n",
        "   - 入力テンソル $x$ は，初期の隠れ層で次のように変換：\n",
        "     \n",
        "     $\n",
        "     x^{(0)} = x W_0^T / \\sqrt{d}\n",
        "     $\n",
        "\n",
        "     ここで，$W_0$ は最初の隠れ層の重み行列．バイアス項がある場合，次のように加算：\n",
        "     \n",
        "     $\n",
        "     x^{(0)} = x^{(0)} + b_0\n",
        "     $\n",
        "\n",
        "     その後，活性化関数 $ \\sigma $ を適用：\n",
        "     \n",
        "     $\n",
        "     x^{(1)} = \\sigma(x^{(0)})\n",
        "     $\n",
        "\n",
        "   - 次の隠れ層も同様に計算．一般的に，隠れ層 $i$ の計算は次のようになる：\n",
        "     \n",
        "     $\n",
        "     x^{(i)} = x^{(i-1)} W_i^T / \\sqrt{h}\n",
        "     $\n",
        "\n",
        "     ここで，$W_i$ は現在の層の重み行列．バイアス項がある場合，次のように加算：\n",
        "\n",
        "     $\n",
        "     x^{(i)} = x^{(i)} + b_i\n",
        "     $\n",
        "\n",
        "     そして，活性化関数を適用：\n",
        "     \n",
        "     $\n",
        "     x^{(i+1)} = \\sigma(x^{(i)})\n",
        "     $\n",
        "\n",
        "   - 最終層では，次のように計算：\n",
        "     \n",
        "     $\n",
        "     x^{(L)} = x^{(L-1)} W_L^T / \\sqrt{h} + b_L\n",
        "     $\n",
        "     \n",
        "     ここで，$W_L$ は最終層の重み行列．出力テンソル $x$ を 1 次元に変換して返す：\n",
        "\n",
        "     $\n",
        "     x^{(L)} = x^{(L)} \\text{view}(-1)\n",
        "     $"
      ],
      "metadata": {
        "id": "zTfCKOJKWt-E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNvFCjVAUR0R"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\"\"\"\n",
        "全結合ネットワーク（Fully Connected Network, FC）のクラスを定義．\n",
        "任意の層数 L を持ち，各層のユニット数は h で指定．\n",
        "活性化関数 act は任意に指定可能で，バイアス項の有無も指定可能．\n",
        "\"\"\"\n",
        "\n",
        "class FC(nn.Module):\n",
        "    def __init__(self, d, h, L, act, bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # ネットワークの初期化\n",
        "        hh = d  # 入力の次元数\n",
        "        for i in range(L):\n",
        "            # 隠れ層の重み行列を正規分布で初期化\n",
        "            W = torch.randn(h, hh)\n",
        "\n",
        "            # メモリ効率を考慮し，重み行列を部分行列に分割して ParameterList に格納\n",
        "            # next two line are here to avoid memory issue when computing the kerne\n",
        "            n = max(1, 128 * 256 // hh)  # 分割サイズを計算\n",
        "            W = nn.ParameterList([nn.Parameter(W[j: j+n]) for j in range(0, len(W), n)])\n",
        "\n",
        "            # 分割した重み行列をレイヤーとして登録\n",
        "            setattr(self, \"W{}\".format(i), W)\n",
        "\n",
        "            # バイアス項が指定されている場合は，それをゼロで初期化して登録\n",
        "            if bias:\n",
        "                self.register_parameter(\"B{}\".format(i), nn.Parameter(torch.zeros(h)))\n",
        "\n",
        "            # 次のレイヤーの入力次元は現在の隠れ層のユニット数になる\n",
        "            hh = h\n",
        "\n",
        "        # 最終層の重み行列を初期化（出力がスカラー値なので次元は (1, h)）\n",
        "        self.register_parameter(\"W{}\".format(L), nn.Parameter(torch.randn(1, hh)))\n",
        "\n",
        "        # バイアス項が指定されている場合は，最終層のバイアスをゼロで初期化\n",
        "        if bias:\n",
        "            self.register_parameter(\"B{}\".format(L), nn.Parameter(torch.zeros(1)))\n",
        "\n",
        "        # クラス変数としてレイヤー数，活性化関数，バイアスの有無を保持\n",
        "        self.L = L\n",
        "        self.act = act\n",
        "        self.bias = bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 順伝播計算\n",
        "        for i in range(self.L + 1):\n",
        "            # i 番目の層の重み行列を取得\n",
        "            W = getattr(self, \"W{}\".format(i))\n",
        "\n",
        "            # ParameterList 形式の重み行列をフルの行列に結合\n",
        "            if isinstance(W, nn.ParameterList):\n",
        "                W = torch.cat(list(W))\n",
        "\n",
        "            # バイアス項が指定されている場合は，バイアスを取得\n",
        "            if self.bias:\n",
        "                B = self.bias * getattr(self, \"B{}\".format(i))\n",
        "            else:\n",
        "                B = 0\n",
        "\n",
        "            # 現在の入力の次元数を取得\n",
        "            h = x.size(1)\n",
        "\n",
        "            if i < self.L:\n",
        "                # 隠れ層での線形変換とスケーリング，そして活性化関数の適用\n",
        "                x = x @ (W.t() / h ** 0.5)  # 重み行列との積（次元スケーリング）\n",
        "                x = self.act(x + B)  # バイアス項を加えた後，活性化関数を適用\n",
        "            else:\n",
        "                # 最終層での線形変換（出力はスカラー値）\n",
        "                x = x @ (W.t() / h ** 0.5) + B  # スカラー出力\n",
        "\n",
        "        # 出力を 1 次元のテンソルに変換して返す\n",
        "        return x.view(-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = FC(d=10, h=100, L=2, act=nn.Sigmoid(), bias=False)"
      ],
      "metadata": {
        "id": "TcRKOJ_1b7FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b.forward(x=torch.randn(5, 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnfQJymKcLsk",
        "outputId": "bc748235-f8c5-4a5d-96c2-e59006a6b621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0983, 0.0122, 0.1126, 0.0241, 0.0249], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b.W0[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vUNhZPvih83",
        "outputId": "cc541fe6-1195-4083-cb70-daf6c73e9a4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ダイナミクス"
      ],
      "metadata": {
        "id": "1RilangZmSpN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kgfnwRl4udyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Dynamics that compares the angle of the gradient between steps and keep it small\n",
        "\n",
        "- マージンに達したときに停止\n",
        "\n",
        "2つの実装：\n",
        "1. `train_regular` - 任意のモデルに対応\n",
        "2. `train_kernel` - 線形モデル専用\n",
        "\"\"\"\n",
        "\n",
        "import copy\n",
        "import itertools\n",
        "import math\n",
        "from time import perf_counter\n",
        "import torch\n",
        "\n",
        "\n",
        "def gradient(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False):\n",
        "    '''\n",
        "    `outputs` に対する `inputs` の勾配を計算する関数\n",
        "    使用例:\n",
        "    ```\n",
        "    gradient(x.sum(), x)          # x の合計に対する勾配\n",
        "    gradient((x * y).sum(), [x, y])  # x と y の要素ごとの積の合計に対する勾配\n",
        "    ```\n",
        "\n",
        "    :param outputs: 勾配を計算する対象の出力テンソル\n",
        "    :param inputs: 勾配を計算したい入力テンソルのリストまたは単一テンソル\n",
        "    :param grad_outputs: 出力テンソルの勾配を指定するためのオプション（通常は None で良い）\n",
        "    :param retain_graph: 計算グラフを保持するかどうかを指定するフラグ（デフォルトは None）\n",
        "    :param create_graph: 勾配の計算グラフを作成するかどうかを指定するフラグ（デフォルトは False）\n",
        "    :return: 入力テンソルに対する勾配をフラットなテンソルとして返す\n",
        "    '''\n",
        "\n",
        "    # `inputs` がテンソルの場合はリストに変換\n",
        "    if torch.is_tensor(inputs):\n",
        "        inputs = [inputs]\n",
        "    else:\n",
        "        inputs = list(inputs)\n",
        "\n",
        "    # `torch.autograd.grad` 関数を使用して勾配を計算\n",
        "    grads = torch.autograd.grad(outputs, inputs, grad_outputs,\n",
        "                                allow_unused=True, # 計算に使用されないテンソルには勾配が計算されない\n",
        "                                retain_graph=retain_graph, # 計算グラフを保持するかどうか\n",
        "                                create_graph=create_graph) # 勾配の計算グラフを作成するかどうか\n",
        "\n",
        "    # 勾配が None の場合は，同じサイズのゼロテンソルを代わりに使用\n",
        "    grads = [x if x is not None else torch.zeros_like(y) for x, y in zip(grads, inputs)]\n",
        "\n",
        "    # 勾配テンソルをフラットな形状に変換して連結\n",
        "    return torch.cat([x.contiguous().view(-1) for x in grads])\n",
        "\n",
        "\n",
        "def loglinspace(rate, step, end=None):\n",
        "    \"\"\"\n",
        "    対数線形間隔での数値を生成するジェネレーター関数\n",
        "    対数的に変化する間隔で数値を生成\n",
        "\n",
        "    `rate` と `step` のパラメータを使って，新しい値を計算\n",
        "    `end` が指定されていない場合は無限に数値を生成\n",
        "\n",
        "    Arguments:\n",
        "        rate (float): 対数的な変化の速度を制御するパラメータ\n",
        "        step (float): 各ステップでの間隔の大きさ\n",
        "        end (float, optional): 生成を停止する条件となる最大値．指定されない場合は無限に生成\n",
        "\n",
        "    Yields:\n",
        "        float: 現在の時間 `t` の値を生成\n",
        "    \"\"\"\n",
        "    t = 0\n",
        "    while end is None or t <= end:\n",
        "        yield t  # 現在の時間 `t` の値を生成\n",
        "        # 次の `t` を計算．ここで，`math.exp(-t * rate / step)` は指数関数的な減衰を表す\n",
        "        t = int(t + 1 + step * (1 - math.exp(-t * rate / step)))\n",
        "\n",
        "\n",
        "class ContinuousMomentum(torch.optim.Optimizer):\n",
        "    \"\"\"連続的なモーメンタムを実装\n",
        "\n",
        "    - d/dt velocity = -1/tau (velocity + grad)\n",
        "    - または\n",
        "    - d/dt velocity = -mu/t (velocity + grad)\n",
        "\n",
        "    - d/dt parameters = velocity\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, dt, tau):\n",
        "        \"\"\"\n",
        "        初期化メソッド\n",
        "\n",
        "        Arguments:\n",
        "            params (iterable): 最適化するパラメータのリスト\n",
        "            dt (float): 時間ステップのサイズ\n",
        "            tau (float): モーメンタムのタイムコンスタント\n",
        "        \"\"\"\n",
        "        defaults = dict(dt=dt, tau=tau)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"単一の最適化ステップを実行\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): モデルを再評価し，損失を返すクロージャ．多くの最適化器にはオプショナル．\n",
        "\n",
        "        Returns:\n",
        "            loss (Tensor or None): 損失の値．クロージャが指定された場合はその損失値を返す．\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            tau = group['tau']\n",
        "            dt = group['dt']\n",
        "\n",
        "            for p in group['params']:\n",
        "                # 勾配がないパラメータはスキップ\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                param_state = self.state[p]\n",
        "                # パラメータの状態が初めてのときは，時間 t を 0 に設定\n",
        "                if 't' not in param_state:\n",
        "                    t = param_state['t'] = 0\n",
        "                else:\n",
        "                    t = param_state['t']\n",
        "\n",
        "                # モーメンタムの状態（速度）を初期化\n",
        "                if tau != 0:\n",
        "                    if 'velocity' not in param_state:\n",
        "                        v = param_state['velocity'] = torch.zeros_like(p.data)\n",
        "                    else:\n",
        "                        v = param_state['velocity']\n",
        "\n",
        "                # モーメンタムの計算\n",
        "                if tau > 0:\n",
        "                    # tau > 0 の場合の連続モーメンタムの計算\n",
        "                    x = math.exp(-dt / tau)  # 時間の経過とともに減衰する係数\n",
        "                    v.mul_(x).add_(-(1 - x), p.grad.data) # 速度に勾配を加える\n",
        "                elif tau < 0:\n",
        "                    # tau < 0 の場合の連続モーメンタムの計算\n",
        "                    mu = -tau\n",
        "                    x = (t / (t + dt)) ** mu  # 時間の経過に伴う係数\n",
        "                    v.mul_(x).add_(-(1 - x), p.grad.data)  # 速度に勾配を加える\n",
        "                else:\n",
        "                    # tau = 0 の場合のシンプルな勾配降下\n",
        "                    v = -p.grad.data\n",
        "\n",
        "                # パラメータの更新\n",
        "                p.data.add_(dt, v)    # パラメータに速度を加える\n",
        "                param_state['t'] += dt    # 時間を進める\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "def make_step(f, optimizer, dt, grad):\n",
        "    \"\"\"\n",
        "    指定された勾配 `grad` を使用して，最適化ステップを実行\n",
        "\n",
        "    Arguments:\n",
        "        f (torch.nn.Module): トレーニングするモデル\n",
        "        optimizer (torch.optim.Optimizer): 使用する最適化器\n",
        "        dt (float): 時間刻み\n",
        "        grad (torch.Tensor): 勾配テンソル\n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    # モデルの全パラメータに対してループ\n",
        "    for p in f.parameters():\n",
        "        # パラメータの総要素数を取得\n",
        "        n = p.numel()\n",
        "        # 勾配テンソルを対応するパラメータに合わせてリシェイプし，割り当て\n",
        "        p.grad = grad[i: i + n].view_as(p)\n",
        "        i += n  # インデックスを次のパラメータに進める\n",
        "\n",
        "    # 各パラメータグループに対して時間刻み `dt` を設定\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['dt'] = dt\n",
        "\n",
        "    # 最適化ステップを実行\n",
        "    optimizer.step()\n",
        "\n",
        "    # 勾配をリセット（計算グラフから切り離し）\n",
        "    for p in f.parameters():\n",
        "        p.grad = None\n",
        "\n",
        "\n",
        "def train_regular(f0, x, y, tau, max_walltime, alpha, loss, subf0, max_dgrad=math.inf, max_dout=math.inf):\n",
        "    \"\"\"\n",
        "    一般的なモデルのトレーニング関数\n",
        "\n",
        "    Arguments:\n",
        "        f0 (torch.nn.Module): 初期モデル\n",
        "        x (torch.Tensor): 入力データ\n",
        "        y (torch.Tensor): 出力ラベル\n",
        "        tau (float): モーメンタムパラメータ\n",
        "        max_walltime (float): 最大経過時間\n",
        "        alpha (float): 出力変化に対する閾値\n",
        "        loss (callable): ロス関数\n",
        "        subf0 (bool): 初期モデルの出力を使用するかどうか\n",
        "        max_dgrad (float): 勾配の変化の最大許容値\n",
        "        max_dout (float): 出力の変化の最大許容値\n",
        "    \"\"\"\n",
        "\n",
        "    # 初期モデルのコピーを作成\n",
        "    f = copy.deepcopy(f0)\n",
        "\n",
        "    # モデルの出力を計算（必要に応じて初期モデルの出力を使用）\n",
        "    with torch.no_grad():\n",
        "        out0 = f0(x) if subf0 else 0\n",
        "\n",
        "    # 時間刻みとモーメンタムのパラメータを初期化\n",
        "    dt = 1\n",
        "    step_change_dt = 0\n",
        "\n",
        "    # ContinuousMomentum オプティマイザを初期化\n",
        "    optimizer = ContinuousMomentum(f.parameters(), dt=dt, tau=tau)\n",
        "\n",
        "    # ログ-線形間隔のチェックポイント生成器を作成\n",
        "    checkpoint_generator = loglinspace(0.01, 100)\n",
        "    checkpoint = next(checkpoint_generator)\n",
        "\n",
        "    # 経過時間を計測するためのタイマーを開始\n",
        "    wall = perf_counter()\n",
        "    t = 0\n",
        "    converged = False\n",
        "\n",
        "    # 最初のモデルの出力と勾配を計算\n",
        "    out = f(x)\n",
        "    grad = gradient(loss((out - out0) * y).mean(), f.parameters())\n",
        "\n",
        "    # トレーニングループ\n",
        "    for step in itertools.count():\n",
        "        # 現在のモデルとオプティマイザの状態を保存\n",
        "        state = copy.deepcopy((f.state_dict(), optimizer.state_dict(), t))\n",
        "\n",
        "        while True:\n",
        "            # 最適化ステップを実行\n",
        "            make_step(f, optimizer, dt, grad)\n",
        "            t += dt\n",
        "            current_dt = dt\n",
        "\n",
        "            # モデルの新しい出力と勾配を計算\n",
        "            new_out = f(x)\n",
        "            new_grad = gradient(loss((new_out - out0) * y).mean(), f.parameters())\n",
        "\n",
        "            # 出力の変化量を計算\n",
        "            dout = (out - new_out).mul(alpha).abs().max().item()\n",
        "\n",
        "            # 勾配の変化量を計算\n",
        "            if grad.norm() == 0 or new_grad.norm() == 0:\n",
        "                dgrad = 0\n",
        "            else:\n",
        "                dgrad = (grad - new_grad).norm().pow(2).div(grad.norm() * new_grad.norm()).item()\n",
        "\n",
        "            # 勾配の変化量と出力の変化量が閾値以下であれば，時間刻みを増加\n",
        "            if dgrad < max_dgrad and dout < max_dout:\n",
        "                if dgrad < 0.5 * max_dgrad and dout < 0.5 * max_dout:\n",
        "                    dt *= 1.1\n",
        "                break\n",
        "            # そうでない場合は，時間刻みを減少\n",
        "            dt /= 10\n",
        "\n",
        "            # 現在の状態を出力\n",
        "            print(\"[{} +{}] [dt={:.1e} dgrad={:.1e} dout={:.1e}]\".format(step, step - step_change_dt, dt, dgrad, dout), flush=True)\n",
        "\n",
        "            # モデルとオプティマイザの状態をリストア\n",
        "            step_change_dt = step\n",
        "            f.load_state_dict(state[0])\n",
        "            optimizer.load_state_dict(state[1])\n",
        "            t = state[2]\n",
        "\n",
        "        # 新しい出力と勾配を保存\n",
        "        out = new_out\n",
        "        grad = new_grad\n",
        "\n",
        "        save = False\n",
        "\n",
        "        # チェックポイントに達した場合，または収束した場合に状態を保存\n",
        "        if step == checkpoint:\n",
        "            checkpoint = next(checkpoint_generator)\n",
        "            assert checkpoint > step\n",
        "            save = True\n",
        "\n",
        "        # 出力が閾値を超え，収束していない場合に収束を宣言\n",
        "        if (alpha * (out - out0) * y >= 1).all() and not converged:\n",
        "            converged = True\n",
        "            save = True\n",
        "\n",
        "        if save:\n",
        "            state = {\n",
        "                'step': step,\n",
        "                'wall': perf_counter() - wall,\n",
        "                't': t,\n",
        "                'dt': current_dt,\n",
        "                'dgrad': dgrad,\n",
        "                'dout': dout,\n",
        "                'norm': sum(p.norm().pow(2) for p in f.parameters()).sqrt().item(),\n",
        "                'dnorm': sum((p0 - p).norm().pow(2) for p0, p in zip(f0.parameters(), f.parameters())).sqrt().item(),\n",
        "                'grad_norm': grad.norm().item(),\n",
        "            }\n",
        "\n",
        "            yield f, state, converged\n",
        "\n",
        "        # 収束した場合はトレーニングを終了\n",
        "        if converged:\n",
        "            break\n",
        "\n",
        "        # 経過時間が最大経過時間を超えた場合にトレーニングを終了\n",
        "        if perf_counter() > wall + max_walltime:\n",
        "            break\n",
        "\n",
        "        # 出力に NaN が含まれている場合，トレーニングを終了\n",
        "        if torch.isnan(out).any():\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "def train_kernel(ktrtr, ytr, tau, max_walltime, alpha, loss_prim, max_dgrad=math.inf, max_dout=math.inf):\n",
        "    \"\"\"\n",
        "    線形モデル専用のトレーニング関数\n",
        "\n",
        "    Arguments:\n",
        "        ktrtr (torch.Tensor): カーネル行列\n",
        "        ytr (torch.Tensor): 出力ラベル\n",
        "        tau (float): モーメンタムパラメータ\n",
        "        max_walltime (float): 最大経過時間\n",
        "        alpha (float): 出力変化に対する閾値\n",
        "        loss_prim (callable): プライムロス関数\n",
        "        max_dgrad (float): 勾配の変化の最大許容値\n",
        "        max_dout (float): 出力の変化の最大許容値\n",
        "    \"\"\"\n",
        "    # 初期出力と速度ベクトルをゼロで初期化\n",
        "    otr = ktrtr.new_zeros(len(ytr))\n",
        "    velo = otr.clone()\n",
        "\n",
        "    # 時間刻みとステップ変更のタイミングを初期化\n",
        "    dt = 1\n",
        "    step_change_dt = 0\n",
        "\n",
        "    # ログ-線形間隔のチェックポイント生成器を作成\n",
        "    checkpoint_generator = loglinspace(0.01, 100)\n",
        "    checkpoint = next(checkpoint_generator)\n",
        "\n",
        "    # 経過時間を計測するためのタイマーを開始\n",
        "    wall = perf_counter()\n",
        "    t = 0\n",
        "    converged = False\n",
        "\n",
        "    # 初期のプライムロスを計算\n",
        "    lprim = loss_prim(otr * ytr) * ytr\n",
        "    grad = ktrtr @ lprim / len(ytr)\n",
        "\n",
        "    # トレーニングループ\n",
        "    for step in itertools.count():\n",
        "\n",
        "        # 現在の出力，速度，時間を保存\n",
        "        state = copy.deepcopy((otr, velo, t))\n",
        "\n",
        "        while True:\n",
        "            # モーメンタムパラメータに基づいて速度を更新\n",
        "            if tau > 0:\n",
        "                x = math.exp(-dt / tau)\n",
        "                velo.mul_(x).add_(-(1 - x), grad)\n",
        "            elif tau < 0:\n",
        "                mu = -tau\n",
        "                x = (t / (t + dt)) ** mu\n",
        "                velo.mul_(x).add_(-(1 - x), grad)\n",
        "            else:\n",
        "                velo.copy_(-grad)\n",
        "\n",
        "            # 出力を更新\n",
        "            otr.add_(dt, velo)\n",
        "\n",
        "            t += dt\n",
        "            current_dt = dt\n",
        "\n",
        "            # 新しいプライムロスを計算し，勾配を更新\n",
        "            lprim = loss_prim(otr * ytr) * ytr\n",
        "            new_grad = ktrtr @ lprim / len(ytr)\n",
        "\n",
        "            # 出力の変化量を計算\n",
        "            dout = velo.mul(dt * alpha).abs().max().item()\n",
        "\n",
        "            # 勾配の変化量を計算\n",
        "            if grad.norm() == 0 or new_grad.norm() == 0:\n",
        "                dgrad = 0\n",
        "            else:\n",
        "                dgrad = (grad - new_grad).norm().pow(2).div(grad.norm() * new_grad.norm()).item()\n",
        "\n",
        "            # 勾配と出力の変化量が許容範囲内であれば，時間刻みを増加\n",
        "            if dgrad < max_dgrad and dout < max_dout:\n",
        "                if dgrad < 0.1 * max_dgrad and dout < 0.1 * max_dout:\n",
        "                    dt *= 1.1\n",
        "                break\n",
        "\n",
        "            # そうでない場合は，時間刻みを減少\n",
        "            dt /= 10\n",
        "\n",
        "            # 現在の状態を出力\n",
        "            print(\"[{} +{}] [dt={:.1e} dgrad={:.1e} dout={:.1e}]\".format(step, step - step_change_dt, dt, dgrad, dout), flush=True)\n",
        "            step_change_dt = step\n",
        "\n",
        "            # 保存した状態をリストア\n",
        "            otr.copy_(state[0])\n",
        "            velo.copy_(state[1])\n",
        "            t = state[2]\n",
        "\n",
        "        # 勾配を新しい値で更新\n",
        "        grad = new_grad\n",
        "\n",
        "        save = False\n",
        "\n",
        "        # チェックポイントに達した場合，または収束した場合に状態を保存\n",
        "        if step == checkpoint:\n",
        "            checkpoint = next(checkpoint_generator)\n",
        "            assert checkpoint > step\n",
        "            save = True\n",
        "\n",
        "        # 出力が閾値を超え，収束していない場合に収束を宣言\n",
        "        if (alpha * otr * ytr >= 1).all() and not converged:\n",
        "            converged = True\n",
        "            save = True\n",
        "\n",
        "        if save:\n",
        "            state = {\n",
        "                'step': step,\n",
        "                'wall': perf_counter() - wall,\n",
        "                't': t,\n",
        "                'dt': current_dt,\n",
        "                'dgrad': dgrad,\n",
        "                'dout': dout,\n",
        "                'grad_norm': grad.norm().item(),\n",
        "            }\n",
        "\n",
        "            yield otr, velo, grad, state, converged\n",
        "\n",
        "        # 収束した場合はトレーニングを終了\n",
        "        if converged:\n",
        "            break\n",
        "\n",
        "        # 経過時間が最大経過時間を超えた場合にトレーニングを終了\n",
        "        if perf_counter() > wall + max_walltime:\n",
        "            break\n",
        "\n",
        "        # 出力に NaN が含まれている場合，トレーニングを終了\n",
        "        if torch.isnan(otr).any():\n",
        "            break"
      ],
      "metadata": {
        "id": "gJZk7JSQdxx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# カーネル"
      ],
      "metadata": {
        "id": "YqlHUas5yfew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`compute_kernels` 関数は，与えられたモデル $ f $ の入力データ $ x_{\\text{tr}} $ と $ x_{\\text{te}} $ に基づいて，カーネル行列（Gram 行列）を計算．\n",
        "\n",
        "- $ K_{\\text{trtr}} $：トレーニングデータ間のカーネル行列\n",
        "- $ K_{\\text{tetr}} $：テストデータとトレーニングデータ間のカーネル行列\n",
        "- $ K_{\\text{tete}} $：テストデータ間のカーネル行列\n",
        "\n",
        "これらの行列は，モデルのパラメータに関する勾配を使って計算．\n",
        "\n",
        "<br>\n",
        "\n",
        "### モデルの勾配計算\n",
        "\n",
        "モデル $ f $ が入力 $ x $ に対して出力を生成し，その勾配を計算．勾配の計算は以下のように行う：\n",
        "\n",
        "1. 入力 $ x $ に対するモデルの出力を $ f(x) $ とする．\n",
        "2. この出力に対するパラメータ $ \\theta $ の勾配を求める：$ \\nabla_\\theta f(x) $\n",
        "\n",
        "  ここで，$ \\nabla_\\theta f(x) $ は $ x $ に対する勾配であり，モデルのパラメータ $ \\theta $ に関する勾配ベクトル．\n",
        "\n",
        "<br>\n",
        "\n",
        "### Gram 行列の計算\n",
        "\n",
        "1. $ K_{\\text{trtr}} $\n",
        "\n",
        "  トレーニングデータ $ x_{\\text{tr}} $ に対するカーネル行列 $ K_{\\text{trtr}} $ は，各トレーニングデータポイント $ x_i $ と $ x_j $ の勾配ベクトルに基づいて計算：\n",
        "\n",
        "  $ K_{\\text{trtr}} = J_{\\text{tr}} J_{\\text{tr}}^T $\n",
        "\n",
        "  ここで，$ J_{\\text{tr}} $ はトレーニングデータ $ x_{\\text{tr}} $ に対する勾配ベクトルを列に持つ行列．$ J_{\\text{tr}} $ の $ i $-th 行は，入力 $ x_i $ に対する勾配ベクトル．\n",
        "\n",
        "<br>\n",
        "\n",
        "2. $ K_{\\text{tetr}} $\n",
        "\n",
        "  テストデータ $ x_{\\text{te}} $ とトレーニングデータ $ x_{\\text{tr}} $ とのカーネル行列 $ K_{\\text{tetr}} $ は次のように計算：\n",
        "\n",
        "  $ K_{\\text{tetr}} = J_{\\text{te}} J_{\\text{tr}}^T $\n",
        "\n",
        "  ここで，$ J_{\\text{te}} $ はテストデータ $ x_{\\text{te}} $ に対する勾配ベクトルを列に持つ行列．\n",
        "\n",
        "<br>\n",
        "\n",
        "3. $ K_{\\text{tete}} $\n",
        "\n",
        "  テストデータ $ x_{\\text{te}} $ に対するカーネル行列 $ K_{\\text{tete}} $ は、テストデータポイント $ x_i $ と $ x_j $ の勾配ベクトルに基づいて計算：\n",
        "\n",
        "  $ K_{\\text{tete}} = J_{\\text{te}} J_{\\text{te}}^T $\n",
        "\n",
        "  ここで，$ J_{\\text{te}} $ の $ i $-th 行は，入力 $ x_i $ に対する勾配ベクトル．"
      ],
      "metadata": {
        "id": "iL4Pua0dm4DN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Computes the Gram matrix of a given model\n",
        "\"\"\"\n",
        "\n",
        "def compute_kernels(f, xtr, xte):\n",
        "    # from hessian import gradient\n",
        "\n",
        "    # 新しいゼロ行列を作成\n",
        "    # ktrtr: トレーニングデータ間のカーネル行列\n",
        "    # ktetr: テストデータとトレーニングデータ間のカーネル行列\n",
        "    # ktete: テストデータ間のカーネル行列\n",
        "    ktrtr = xtr.new_zeros(len(xtr), len(xtr))\n",
        "    ktetr = xtr.new_zeros(len(xte), len(xtr))\n",
        "    ktete = xtr.new_zeros(len(xte), len(xte))\n",
        "\n",
        "    params = []\n",
        "    current = []\n",
        "\n",
        "    # モデルのパラメータをサイズで降順にソートし，メモリ制限に基づいて分割\n",
        "    for p in sorted(f.parameters(), key=lambda p: p.numel(), reverse=True):\n",
        "        current.append(p)\n",
        "        # メモリ制限に基づき，パラメータを分割\n",
        "        if sum(p.numel() for p in current) > 2e9 // (8 * (len(xtr) + len(xte))):\n",
        "            if len(current) > 1:\n",
        "                params.append(current[:-1])\n",
        "                current = current[-1:]\n",
        "            else:\n",
        "                params.append(current)\n",
        "                current = []\n",
        "    if len(current) > 0:\n",
        "        params.append(current)\n",
        "\n",
        "    # 各パラメータグループについてカーネル行列を計算\n",
        "    for i, p in enumerate(params):\n",
        "        print(\"[{}/{}] [len={} numel={}]\".format(i, len(params), len(p), sum(x.numel() for x in p)), flush=True)\n",
        "\n",
        "        # 勾配行列を初期化\n",
        "        jtr = xtr.new_empty(len(xtr), sum(u.numel() for u in p))  # (P, N~) # (トレーニングデータ数, パラメータ数の合計)\n",
        "        jte = xte.new_empty(len(xte), sum(u.numel() for u in p))  # (P, N~) # (テストデータ数, パラメータ数の合計)\n",
        "\n",
        "        # トレーニングデータに対する勾配行列を計算\n",
        "        for j, x in enumerate(xtr):\n",
        "            jtr[j] = gradient(f(x[None]), p)  # (N~) # (パラメータ数の合計)\n",
        "\n",
        "        # テストデータに対する勾配行列を計算\n",
        "        for j, x in enumerate(xte):\n",
        "            jte[j] = gradient(f(x[None]), p)  # (N~) # (パラメータ数の合計)\n",
        "\n",
        "        # カーネル行列を更新\n",
        "        ktrtr.add_(jtr @ jtr.t())  # トレーニングデータ間のカーネル行列\n",
        "        ktetr.add_(jte @ jtr.t())  # テストデータとトレーニングデータ間のカーネル行列\n",
        "        ktete.add_(jte @ jte.t())  # テストデータ間のカーネル行列\n",
        "        del jtr, jte  # 不要になった勾配行列を削除\n",
        "\n",
        "    return ktrtr, ktetr, ktete"
      ],
      "metadata": {
        "id": "KofuVIsHm5Qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 損失関連"
      ],
      "metadata": {
        "id": "HquikxBO7SjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "def loss_func(hyper, fy):\n",
        "    \"\"\"\n",
        "    損失関数を計算．指定された損失関数のタイプに応じて異なる計算を行う．\n",
        "\n",
        "    Parameters:\n",
        "    - hyper: 設定を含むオブジェクト\n",
        "    - fy: モデルの予測値と実際のラベルを用いた計算結果\n",
        "\n",
        "    Returns:\n",
        "    - 損失値: 指定された損失関数に基づいて計算された損失\n",
        "    \"\"\"\n",
        "    if hyper.loss == 'softhinge':\n",
        "        sp = partial(torch.nn.functional.softplus, beta=hyper.lossbeta)\n",
        "        # 損失を計算し，alpha で正規化\n",
        "        return sp(1 - hyper.alpha * fy) / hyper.alpha\n",
        "    if hyper.loss == 'qhinge':\n",
        "        return 0.5 * (1 - hyper.alpha * fy).relu().pow(2) / hyper.alpha\n",
        "\n",
        "\n",
        "def loss_func_prime(hyper, fy):\n",
        "    \"\"\"\n",
        "    損失関数の勾配（微分）を計算．指定された損失関数のタイプに応じて異なる計算を行う．\n",
        "\n",
        "    Parameters:\n",
        "    - hyper: 設定を含むオブジェクト\n",
        "    - fy: モデルの予測値と実際のラベルを用いた計算結果\n",
        "\n",
        "    Returns:\n",
        "    - 損失の勾配: 損失関数の微分\n",
        "    \"\"\"\n",
        "    if hyper.loss == 'softhinge':\n",
        "        return -torch.sigmoid(hyper.lossbeta * (1 - hyper.alpha * fy)).mul(hyper.lossbeta)\n",
        "    if hyper.loss == 'qhinge':\n",
        "        return -(1 - hyper.alpha * fy).relu()"
      ],
      "metadata": {
        "id": "2WKWCbZI7RxG"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run_kernel と run_regular"
      ],
      "metadata": {
        "id": "lvM2H10K73Ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_kernel(hyper, ktrtr, ktetr, ktete, f, xtr, ytr, xte, yte):\n",
        "    \"\"\"\n",
        "    カーネル回帰の実行と評価を行う関数．トレーニングデータに対するカーネル回帰を実行し，トレーニングとテストデータに対する出力を計算\n",
        "\n",
        "    Parameters:\n",
        "    - hyper: 設定やパラメータが格納されたオブジェクト\n",
        "    - ktrtr: トレーニングデータに対するカーネル行列\n",
        "    - ktetr: トレーニングデータとテストデータ間のカーネル行列\n",
        "    - ktete: テストデータに対するカーネル行列\n",
        "    - f: モデル関数\n",
        "    - xtr: トレーニングデータの入力\n",
        "    - ytr: トレーニングデータのラベル\n",
        "    - xte: テストデータの入力\n",
        "    - yte: テストデータのラベル\n",
        "\n",
        "    Returns:\n",
        "    - out: トレーニングおよびテストの結果とカーネル行列の統計量を含む辞書\n",
        "    \"\"\"\n",
        "\n",
        "    # 引数の f0 が 1 であることを確認\n",
        "    assert hyper.f0 == 1\n",
        "\n",
        "    # 結果を保存するためのリストを初期化\n",
        "    dynamics = []\n",
        "\n",
        "    # 時間スケール tau を設定\n",
        "    tau = hyper.tau_over_h * hyper.h\n",
        "    # alpha に基づいて tau を調整\n",
        "    if hyper.tau_alpha_crit is not None:\n",
        "        tau *= min(1, hyper.tau_alpha_crit / hyper.alpha)\n",
        "\n",
        "    # カーネル回帰のトレーニングを実行し，各ステップの状態を取得\n",
        "    for otr, _velo, _grad, state, _converged in train_kernel(ktrtr, ytr, tau, hyper.train_time, hyper.alpha, partial(loss_func_prime, hyper), hyper.max_dgrad, hyper.max_dout):\n",
        "\n",
        "        # トレーニングの結果を状態に保存\n",
        "        state['train'] = {\n",
        "            'loss': loss_func(hyper, otr * ytr).mean().item(),\n",
        "            'aloss': hyper.alpha * loss_func(hyper, otr * ytr).mean().item(),\n",
        "            'err': (otr * ytr <= 0).double().mean().item(),\n",
        "            'nd': (hyper.alpha * otr * ytr < 1).long().sum().item(),\n",
        "            'dfnorm': otr.pow(2).mean().sqrt(),\n",
        "            'outputs': otr if hyper.save_outputs else None,\n",
        "            'labels': ytr if hyper.save_outputs else None,\n",
        "        }\n",
        "\n",
        "        print(\"[i={d[step]:d} t={d[t]:.2e} wall={d[wall]:.0f}] [dt={d[dt]:.1e} dgrad={d[dgrad]:.1e} dout={d[dout]:.1e}] [train aL={d[train][aloss]:.2e} err={d[train][err]:.2f} nd={d[train][nd]}]\".format(d=state), flush=True)\n",
        "        dynamics.append(state)\n",
        "\n",
        "    # トレーニングデータに対するカーネル回帰の係数を計算\n",
        "    c = torch.lstsq(otr.view(-1, 1), ktrtr).solution.flatten()\n",
        "\n",
        "    # テストデータの出力を計算\n",
        "    if len(xte) > len(xtr):\n",
        "        # from hessian import gradient\n",
        "        # テストデータがトレーニングデータより多い場合，勾配を用いて出力を計算\n",
        "        a = gradient(f(xtr) @ c, f.parameters())\n",
        "        ote = torch.stack([gradient(f(x[None]), f.parameters()) @ a for x in xte])\n",
        "    else:\n",
        "        # それ以外の場合，事前に計算されたカーネル行列を使用\n",
        "        ote = ktetr @ c\n",
        "\n",
        "    # 結果を辞書としてまとめて返す\n",
        "    out = {\n",
        "        'dynamics': dynamics,\n",
        "        'train': {\n",
        "            'outputs': otr,\n",
        "            'labels': ytr,\n",
        "        },\n",
        "        'test': {\n",
        "            'outputs': ote,\n",
        "            'labels': yte,\n",
        "        },\n",
        "        'kernel': {\n",
        "            'train': {\n",
        "                'value': ktrtr.cpu() if hyper.store_kernel == 1 else None,\n",
        "                'diag': ktrtr.diag(),\n",
        "                'mean': ktrtr.mean(),\n",
        "                'std': ktrtr.std(),\n",
        "                'norm': ktrtr.norm(),\n",
        "            },\n",
        "            'test': {\n",
        "                'value': ktete.cpu() if hyper.store_kernel == 1 else None,\n",
        "                'diag': ktete.diag(),\n",
        "                'mean': ktete.mean(),\n",
        "                'std': ktete.std(),\n",
        "                'norm': ktete.norm(),\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def run_regular(hyper, f0, xtr, ytr, xte, yte):\n",
        "    \"\"\"\n",
        "    通常のトレーニングプロセスを実行し，トレーニングとテストの結果を収集する関数\n",
        "\n",
        "    Parameters:\n",
        "    - hyper: 設定やパラメータが格納されたオブジェクト\n",
        "    - f0: 初期モデル関数\n",
        "    - xtr: トレーニングデータの入力\n",
        "    - ytr: トレーニングデータのラベル\n",
        "    - xte: テストデータの入力\n",
        "    - yte: テストデータのラベル\n",
        "\n",
        "    Yields:\n",
        "    - f: トレーニング後のモデル関数\n",
        "    - out: トレーニングとテストの結果を含む辞書\n",
        "    \"\"\"\n",
        "\n",
        "    # 勾配計算を無効にし，初期モデルによる出力を計算\n",
        "    with torch.no_grad():\n",
        "        otr0 = f0(xtr)\n",
        "        ote0 = f0(xte)\n",
        "\n",
        "    # 初期モデルが無い場合は，出力をゼロに設定\n",
        "    if hyper.f0 == 0:\n",
        "        otr0 = torch.zeros_like(otr0)\n",
        "        ote0 = torch.zeros_like(ote0)\n",
        "\n",
        "    # ランダムにサンプルインデックスを選択し，部分データを取得\n",
        "    j = torch.randperm(min(len(xte), len(xtr)))[:10 * hyper.chunk]\n",
        "    ytrj = ytr[j]\n",
        "    ytej = yte[j]\n",
        "\n",
        "    # トレーニング開始時間を記録\n",
        "    t = perf_counter()\n",
        "\n",
        "    # 時間スケール tau を設定\n",
        "    tau = hyper.tau_over_h * hyper.h\n",
        "    if hyper.tau_alpha_crit is not None:\n",
        "        tau *= min(1, hyper.tau_alpha_crit / hyper.alpha)\n",
        "\n",
        "    # トレーニングのダイナミクスを保存するリスト\n",
        "    dynamics = []\n",
        "\n",
        "    # トレーニングを実行し，各ステップの状態を取得\n",
        "    for f, state, done in train_regular(f0, xtr, ytr, tau, hyper.train_time, hyper.alpha, partial(loss_func, hyper), bool(hyper.f0), hyper.max_dgrad, hyper.max_dout):\n",
        "        with torch.no_grad():\n",
        "            # 勾配計算を無効にし，トレーニングとテストデータの出力を計算\n",
        "            otr = f(xtr[j]) - otr0[j]\n",
        "            ote = f(xte[j]) - ote0[j]\n",
        "\n",
        "        # モデルアーキテクチャが 'fc' の場合，重みのノルムを計算\n",
        "        if hyper.arch.split('_')[0] == 'fc':\n",
        "            def getw(f, i):\n",
        "                return torch.cat(list(getattr(f.f, \"W{}\".format(i))))\n",
        "            state['wnorm'] = [getw(f, i).norm().item() for i in range(f.f.L + 1)]\n",
        "            state['dwnorm'] = [(getw(f, i) - getw(f0, i)).norm().item() for i in range(f.f.L + 1)]\n",
        "\n",
        "        # トレーニングデータの損失，エラー，その他の情報を計算\n",
        "        state['train'] = {\n",
        "            'loss': loss_func(hyper, otr * ytrj).mean().item(),\n",
        "            'aloss': hyper.alpha * loss_func(hyper, otr * ytrj).mean().item(),\n",
        "            'err': (otr * ytr[j] <= 0).double().mean().item(),\n",
        "            'nd': (hyper.alpha * otr * ytr[j] < 1).long().sum().item(),\n",
        "            'dfnorm': otr.pow(2).mean().sqrt(),\n",
        "            'fnorm': (otr + otr0[j]).pow(2).mean().sqrt(),\n",
        "            'outputs': otr if hyper.save_outputs else None,\n",
        "            'labels': ytrj if hyper.save_outputs else None,\n",
        "        }\n",
        "\n",
        "        # テストデータの損失，エラー，その他の情報を計算\n",
        "        state['test'] = {\n",
        "            'loss': loss_func(hyper, ote * ytej).mean().item(),\n",
        "            'aloss': hyper.alpha * loss_func(hyper, ote * ytej).mean().item(),\n",
        "            'err': (ote * yte[j] <= 0).double().mean().item(),\n",
        "            'nd': (hyper.alpha * ote * yte[j] < 1).long().sum().item(),\n",
        "            'dfnorm': ote.pow(2).mean().sqrt(),\n",
        "            'fnorm': (ote + ote0[j]).pow(2).mean().sqrt(),\n",
        "            'outputs': ote if hyper.save_outputs else None,\n",
        "            'labels': ytej if hyper.save_outputs else None,\n",
        "        }\n",
        "\n",
        "        print(\"[i={d[step]:d} t={d[t]:.2e} wall={d[wall]:.0f}] [dt={d[dt]:.1e} dgrad={d[dgrad]:.1e} dout={d[dout]:.1e}] [train aL={d[train][aloss]:.2e} err={d[train][err]:.2f} nd={d[train][nd]}/{p}] [test aL={d[test][aloss]:.2e} err={d[test][err]:.2f}]\".format(d=state, p=len(j)), flush=True)\n",
        "\n",
        "        # ダイナミクスリストに現在のステートを追加\n",
        "        dynamics.append(state)\n",
        "\n",
        "        # トレーニングが完了した場合または実行時間が 6000秒 を超えた場合に結果を出力\n",
        "        if done or perf_counter() - t > 6000:\n",
        "            t = perf_counter()\n",
        "\n",
        "            # 勾配計算を無効にし，最終的なトレーニングとテストの出力を計算\n",
        "            with torch.no_grad():\n",
        "                otr = f(xtr) - otr0\n",
        "                ote = f(xte) - ote0\n",
        "\n",
        "            # 結果を辞書としてまとめて返す\n",
        "            out = {\n",
        "                'dynamics': dynamics,\n",
        "                'train': {\n",
        "                    'f0': otr0,\n",
        "                    'outputs': otr,\n",
        "                    'labels': ytr,\n",
        "                },\n",
        "                'test': {\n",
        "                    'f0': ote0,\n",
        "                    'outputs': ote,\n",
        "                    'labels': yte,\n",
        "                }\n",
        "            }\n",
        "            yield f, out"
      ],
      "metadata": {
        "id": "WNUmIC4h71lQ"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# メイン"
      ],
      "metadata": {
        "id": "X3le98ocmswR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "class SplitEval(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    モデルを分割して評価するためのクラス．入力データを指定されたサイズに分割し，それぞれの分割に対してモデルを評価．\n",
        "    最後に，評価結果を結合して返す．\n",
        "\n",
        "    Attributes:\n",
        "    - f: 評価対象のモデル\n",
        "    - size: 入力データを分割するサイズ\n",
        "    \"\"\"\n",
        "    def __init__(self, f, size):\n",
        "        \"\"\"\n",
        "        コンストラクタ．モデルと分割サイズを初期化\n",
        "\n",
        "        Parameters:\n",
        "        - f: 評価対象のモデル\n",
        "        - size: 入力データを分割するサイズ\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.f = f    # モデルを保存\n",
        "        self.size = size    # 分割サイズを保存\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        フォワードパス．入力データを指定されたサイズに分割し，各分割に対してモデルを適用し，結果を結合して返す．\n",
        "\n",
        "        Parameters:\n",
        "        - x: 入力データ（バッチ）\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: 各分割に対してモデルを適用した結果を結合したテンソル\n",
        "        \"\"\"\n",
        "        return torch.cat([self.f(x[i: i + self.size]) for i in range(0, len(x), self.size)])\n",
        "\n",
        "def run_exp(hyper, f0, xtr, ytr, xte, yte):\n",
        "    \"\"\"\n",
        "    実験を実行し，トレーニングとテストの結果を収集する関数\n",
        "\n",
        "    Parameters:\n",
        "    - hyper: 設定やパラメータが格納されたオブジェクト\n",
        "    - f0: 初期モデル関数\n",
        "    - xtr: トレーニングデータの入力\n",
        "    - ytr: トレーニングデータのラベル\n",
        "    - xte: テストデータの入力\n",
        "    - yte: テストデータのラベル\n",
        "\n",
        "    Yields:\n",
        "    - run: 実験の結果を含む辞書\n",
        "    \"\"\"\n",
        "    run = {\n",
        "        'hyper': hyper,\n",
        "        'N': sum(p.numel() for p in f0.parameters()),  # モデルのパラメータの総数\n",
        "    }\n",
        "\n",
        "    if hyper.delta_kernel == 1 or hyper.init_kernel == 1:\n",
        "        init_kernel = compute_kernels(f0, xtr, xte[:len(xtr)])\n",
        "\n",
        "    if hyper.init_kernel == 1:\n",
        "        run['init_kernel'] = run_kernel(hyper, *init_kernel, f0, xtr, ytr, xte, yte)\n",
        "\n",
        "    if hyper.delta_kernel == 1:\n",
        "        init_kernel = (init_kernel[0].cpu(), init_kernel[2].cpu())\n",
        "    elif hyper.init_kernel == 1:\n",
        "        del init_kernel\n",
        "\n",
        "    if hyper.regular == 1:\n",
        "        for f, out in run_regular(hyper, f0, xtr, ytr, xte, yte):\n",
        "            run['regular'] = out\n",
        "            yield run\n",
        "\n",
        "        if hyper.delta_kernel == 1 or hyper.final_kernel == 1:\n",
        "            final_kernel = compute_kernels(f, xtr, xte[:len(xtr)])\n",
        "\n",
        "        if hyper.final_kernel == 1:\n",
        "            run['final_kernel'] = run_kernel(hyper, *final_kernel, f, xtr, ytr, xte, yte)\n",
        "\n",
        "        if hyper.delta_kernel == 1:\n",
        "            final_kernel = (final_kernel[0].cpu(), final_kernel[2].cpu())\n",
        "            run['delta_kernel'] = {\n",
        "                'train': (init_kernel[0] - final_kernel[0]).norm().item(),\n",
        "                'test': (init_kernel[1] - final_kernel[1]).norm().item(),\n",
        "            }\n",
        "\n",
        "    yield run\n",
        "\n",
        "def execute(hyper):\n",
        "    \"\"\"\n",
        "    実験を実行するためのメイン関数．データセットの準備，モデルの設定，実験の実行\n",
        "\n",
        "    Parameters:\n",
        "    - hyper: 実験の設定を含むオブジェクト\n",
        "    \"\"\"\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        if hyper.dtype == 'float64':\n",
        "            torch.set_default_dtype(torch.float64)\n",
        "        elif hyper.dtype == 'float32':\n",
        "            torch.set_default_dtype(torch.float32)\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"CUDA is not available. Running on CPU.\")\n",
        "        if hyper.dtype == 'float64':\n",
        "            torch.set_default_dtype(torch.float64)\n",
        "        elif hyper.dtype == 'float32':\n",
        "            torch.set_default_dtype(torch.float32)\n",
        "\n",
        "    (xtr, ytr), (xte, yte) = get_binary_dataset(hyper.n, hyper.k, hyper.train_size, hyper.test_size, hyper.data_seed, hyper.normalize, device)\n",
        "    xtr = xtr.type(torch.get_default_dtype())\n",
        "    xte = xte.type(torch.get_default_dtype())\n",
        "    ytr = ytr.type(torch.get_default_dtype())\n",
        "    yte = yte.type(torch.get_default_dtype())\n",
        "\n",
        "    torch.manual_seed(hyper.init_seed + hash(hyper.alpha))\n",
        "\n",
        "    arch, act = hyper.arch.split('_')\n",
        "    if act == 'relu':\n",
        "        act = lambda x: 2 ** 0.5 * torch.relu(x)\n",
        "    elif act == 'tanh':\n",
        "        act = torch.tanh\n",
        "    elif act == 'softplus':\n",
        "        factor = torch.nn.functional.softplus(torch.randn(100000, dtype=torch.float64), hyper.spbeta).pow(2).mean().rsqrt().item()\n",
        "        act = lambda x: torch.nn.functional.softplus(x, beta=hyper.spbeta).mul(factor)\n",
        "    else:\n",
        "        raise ValueError('act not specified')\n",
        "\n",
        "    if arch == 'fc':\n",
        "        assert hyper.L is not None\n",
        "        xtr = xtr.flatten(1)\n",
        "        xte = xte.flatten(1)\n",
        "        f = FC(xtr.size(1), hyper.h, hyper.L, act, hyper.bias).to(device)\n",
        "    else:\n",
        "        raise ValueError('arch not specified')\n",
        "\n",
        "    f = SplitEval(f, hyper.chunk)\n",
        "\n",
        "    torch.manual_seed(hyper.batch_seed)\n",
        "\n",
        "    for run in run_exp(hyper, f, xtr, ytr, xte, yte):\n",
        "        yield run\n",
        "\n",
        "class HyperParams:\n",
        "    def __init__(self):\n",
        "        self.device = 'cuda'\n",
        "        self.dtype = 'float64'\n",
        "        self.init_seed = 0\n",
        "        self.batch_seed = 0\n",
        "        self.n = 30\n",
        "        self.k = 3\n",
        "        self.train_size = 700\n",
        "        self.test_size = 400\n",
        "        self.normalize = True\n",
        "        self.data_seed = 0\n",
        "        self.alpha = 1e-4   # [例：1e-4]\n",
        "        self.f0 = 1\n",
        "        self.tau_over_h = 1e-3\n",
        "        self.tau_alpha_crit = 1e3\n",
        "        self.L = 2\n",
        "        self.h = 100\n",
        "        self.arch = 'fc_softplus'\n",
        "        self.spbeta = 5\n",
        "        self.bias = True\n",
        "        self.max_dgrad = 1e-4\n",
        "        self.max_dout = 0.1\n",
        "        self.loss = 'softhinge'\n",
        "        self.lossbeta = 20\n",
        "        self.train_time = 18000\n",
        "        self.chunk = 100\n",
        "        self.init_kernel = 0\n",
        "        self.delta_kernel = 0\n",
        "        self.final_kernel = 0\n",
        "        self.store_kernel = 0\n",
        "        self.save_outputs = 0\n",
        "        self.regular = 1\n",
        "        self.directory = 'F10k3Lsp_h_init'\n",
        "        self.pickle = 'F10k3Lsp_h_init.pickle'\n",
        "\n",
        "# 実験の実行と結果を保存する関数\n",
        "import pickle\n",
        "\n",
        "def run_and_save_experiment(hyper):\n",
        "    # ディレクトリの作成\n",
        "    if not os.path.exists(hyper.directory):\n",
        "        os.makedirs(hyper.directory)\n",
        "\n",
        "    # pickle ファイルのパスをディレクトリに基づいて変更\n",
        "    pickle_path = os.path.join(hyper.directory, hyper.pickle)\n",
        "\n",
        "    # test_size または chunk が None の場合は train_size の値で初期化\n",
        "    if hyper.test_size is None:\n",
        "        hyper.test_size = hyper.train_size\n",
        "\n",
        "    if hyper.chunk is None:\n",
        "        hyper.chunk = hyper.train_size\n",
        "\n",
        "    try:\n",
        "        # 引数を pickle ファイルに保存\n",
        "        with open(pickle_path, 'wb') as f:\n",
        "            pickle.dump(hyper, f)\n",
        "\n",
        "        with open(pickle_path, 'ab') as f:\n",
        "            for res in execute(hyper):\n",
        "                # 結果を pickle ファイルに追加で保存\n",
        "                pickle.dump(res, f)\n",
        "    except Exception as e:\n",
        "        if os.path.exists(pickle_path):\n",
        "            os.remove(pickle_path)\n",
        "        print(f\"An error occurred during saving: {e}\")\n",
        "        raise e\n",
        "\n",
        "######## 実験の実行と結果の保存\n",
        "hyper = HyperParams()\n",
        "run_and_save_experiment(hyper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk-AWvkGJOFk",
        "outputId": "f7e39dd3-b6e6-4311-865a-745c8dcceae7"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available. Running on CPU.\n",
            "[0 +0] [dt=1.0e-01 dgrad=1.1e-04 dout=3.1e-06]\n",
            "[i=0 t=1.00e-01 wall=0] [dt=1.0e-01 dgrad=4.6e-07 dout=1.9e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=1 t=2.10e-01 wall=0] [dt=1.1e-01 dgrad=1.1e-06 dout=3.0e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=2 t=3.31e-01 wall=0] [dt=1.2e-01 dgrad=1.6e-06 dout=3.6e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=3 t=4.64e-01 wall=0] [dt=1.3e-01 dgrad=2.0e-06 dout=4.0e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=4 t=6.11e-01 wall=0] [dt=1.5e-01 dgrad=2.4e-06 dout=4.5e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=5 t=7.72e-01 wall=0] [dt=1.6e-01 dgrad=3.0e-06 dout=4.9e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=6 t=9.49e-01 wall=0] [dt=1.8e-01 dgrad=3.6e-06 dout=5.4e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=7 t=1.14e+00 wall=0] [dt=1.9e-01 dgrad=4.3e-06 dout=6.0e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=8 t=1.36e+00 wall=0] [dt=2.1e-01 dgrad=5.3e-06 dout=6.6e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=9 t=1.59e+00 wall=0] [dt=2.4e-01 dgrad=6.3e-06 dout=7.3e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=10 t=1.85e+00 wall=0] [dt=2.6e-01 dgrad=7.7e-06 dout=8.0e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=11 t=2.14e+00 wall=0] [dt=2.9e-01 dgrad=9.3e-06 dout=8.8e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=12 t=2.45e+00 wall=0] [dt=3.1e-01 dgrad=1.1e-05 dout=9.7e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=13 t=2.80e+00 wall=0] [dt=3.5e-01 dgrad=1.4e-05 dout=1.1e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=14 t=3.18e+00 wall=0] [dt=3.8e-01 dgrad=1.6e-05 dout=1.2e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=15 t=3.59e+00 wall=0] [dt=4.2e-01 dgrad=2.0e-05 dout=1.3e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=16 t=4.05e+00 wall=0] [dt=4.6e-01 dgrad=2.4e-05 dout=1.4e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=17 t=4.56e+00 wall=0] [dt=5.1e-01 dgrad=2.9e-05 dout=1.6e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=18 t=5.12e+00 wall=1] [dt=5.6e-01 dgrad=3.5e-05 dout=1.7e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=19 t=5.73e+00 wall=1] [dt=6.1e-01 dgrad=4.2e-05 dout=1.9e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=20 t=6.40e+00 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.1e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=21 t=7.07e+00 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.1e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=22 t=7.75e+00 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.1e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=23 t=8.42e+00 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.2e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=24 t=9.09e+00 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.2e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=25 t=9.76e+00 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.2e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=26 t=1.04e+01 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.2e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=27 t=1.11e+01 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.2e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=28 t=1.18e+01 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.2e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=29 t=1.25e+01 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.3e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=30 t=1.31e+01 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.3e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=31 t=1.38e+01 wall=1] [dt=6.7e-01 dgrad=5.2e-05 dout=2.3e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=32 t=1.45e+01 wall=1] [dt=6.7e-01 dgrad=5.2e-05 dout=2.3e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=33 t=1.51e+01 wall=1] [dt=6.7e-01 dgrad=5.2e-05 dout=2.3e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=34 t=1.58e+01 wall=1] [dt=6.7e-01 dgrad=5.2e-05 dout=2.4e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=35 t=1.65e+01 wall=1] [dt=6.7e-01 dgrad=5.2e-05 dout=2.4e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=36 t=1.72e+01 wall=1] [dt=6.7e-01 dgrad=5.2e-05 dout=2.4e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=37 t=1.78e+01 wall=1] [dt=6.7e-01 dgrad=5.3e-05 dout=2.4e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=38 t=1.85e+01 wall=1] [dt=6.7e-01 dgrad=5.3e-05 dout=2.4e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=39 t=1.92e+01 wall=1] [dt=6.7e-01 dgrad=5.3e-05 dout=2.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=40 t=1.99e+01 wall=1] [dt=6.7e-01 dgrad=5.3e-05 dout=2.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=41 t=2.05e+01 wall=1] [dt=6.7e-01 dgrad=5.3e-05 dout=2.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=42 t=2.12e+01 wall=1] [dt=6.7e-01 dgrad=5.3e-05 dout=2.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=43 t=2.19e+01 wall=1] [dt=6.7e-01 dgrad=5.4e-05 dout=2.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=44 t=2.25e+01 wall=1] [dt=6.7e-01 dgrad=5.4e-05 dout=2.6e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=45 t=2.32e+01 wall=1] [dt=6.7e-01 dgrad=5.4e-05 dout=2.6e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=46 t=2.39e+01 wall=1] [dt=6.7e-01 dgrad=5.4e-05 dout=2.6e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=47 t=2.46e+01 wall=1] [dt=6.7e-01 dgrad=5.4e-05 dout=2.6e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=48 t=2.52e+01 wall=1] [dt=6.7e-01 dgrad=5.4e-05 dout=2.6e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=49 t=2.59e+01 wall=1] [dt=6.7e-01 dgrad=5.4e-05 dout=2.7e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=50 t=2.66e+01 wall=1] [dt=6.7e-01 dgrad=5.4e-05 dout=2.7e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=51 t=2.73e+01 wall=1] [dt=6.7e-01 dgrad=5.4e-05 dout=2.7e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=52 t=2.79e+01 wall=1] [dt=6.7e-01 dgrad=5.5e-05 dout=2.7e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=53 t=2.86e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=2.8e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=54 t=2.93e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=2.8e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=55 t=2.99e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=2.8e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=56 t=3.06e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=2.8e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=57 t=3.13e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=2.8e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=58 t=3.20e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=2.9e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=59 t=3.26e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=2.9e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=60 t=3.33e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=2.9e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=61 t=3.40e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=2.9e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=62 t=3.47e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=3.0e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=63 t=3.53e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=3.0e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=64 t=3.60e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=3.0e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=65 t=3.67e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=3.1e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=66 t=3.73e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=3.1e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=67 t=3.80e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=3.1e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=68 t=3.87e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=3.1e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=69 t=3.94e+01 wall=2] [dt=6.7e-01 dgrad=5.6e-05 dout=3.2e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=70 t=4.00e+01 wall=2] [dt=6.7e-01 dgrad=5.6e-05 dout=3.2e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=71 t=4.07e+01 wall=2] [dt=6.7e-01 dgrad=5.6e-05 dout=3.2e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=72 t=4.14e+01 wall=2] [dt=6.7e-01 dgrad=5.6e-05 dout=3.3e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=73 t=4.21e+01 wall=2] [dt=6.7e-01 dgrad=5.6e-05 dout=3.3e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=74 t=4.27e+01 wall=2] [dt=6.7e-01 dgrad=5.6e-05 dout=3.3e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=75 t=4.34e+01 wall=2] [dt=6.7e-01 dgrad=5.6e-05 dout=3.4e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=76 t=4.41e+01 wall=2] [dt=6.7e-01 dgrad=5.6e-05 dout=3.4e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=77 t=4.47e+01 wall=2] [dt=6.7e-01 dgrad=5.6e-05 dout=3.4e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=78 t=4.54e+01 wall=2] [dt=6.7e-01 dgrad=5.6e-05 dout=3.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=79 t=4.61e+01 wall=2] [dt=6.7e-01 dgrad=5.7e-05 dout=3.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=80 t=4.68e+01 wall=2] [dt=6.7e-01 dgrad=5.7e-05 dout=3.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=81 t=4.74e+01 wall=2] [dt=6.7e-01 dgrad=5.7e-05 dout=3.6e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=82 t=4.81e+01 wall=2] [dt=6.7e-01 dgrad=5.7e-05 dout=3.6e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=83 t=4.88e+01 wall=2] [dt=6.7e-01 dgrad=5.7e-05 dout=3.6e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=84 t=4.95e+01 wall=2] [dt=6.7e-01 dgrad=5.7e-05 dout=3.7e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=85 t=5.01e+01 wall=2] [dt=6.7e-01 dgrad=5.7e-05 dout=3.7e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=86 t=5.08e+01 wall=2] [dt=6.7e-01 dgrad=5.8e-05 dout=3.8e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=87 t=5.15e+01 wall=2] [dt=6.7e-01 dgrad=5.8e-05 dout=3.8e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=88 t=5.21e+01 wall=2] [dt=6.7e-01 dgrad=5.8e-05 dout=3.8e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=89 t=5.28e+01 wall=3] [dt=6.7e-01 dgrad=5.8e-05 dout=3.9e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=90 t=5.35e+01 wall=3] [dt=6.7e-01 dgrad=5.8e-05 dout=3.9e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=91 t=5.42e+01 wall=3] [dt=6.7e-01 dgrad=5.9e-05 dout=4.0e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=92 t=5.48e+01 wall=3] [dt=6.7e-01 dgrad=5.9e-05 dout=4.0e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=93 t=5.55e+01 wall=3] [dt=6.7e-01 dgrad=5.9e-05 dout=4.1e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=94 t=5.62e+01 wall=3] [dt=6.7e-01 dgrad=5.9e-05 dout=4.1e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=95 t=5.69e+01 wall=3] [dt=6.7e-01 dgrad=6.0e-05 dout=4.2e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=96 t=5.75e+01 wall=3] [dt=6.7e-01 dgrad=6.0e-05 dout=4.2e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=97 t=5.82e+01 wall=3] [dt=6.7e-01 dgrad=6.0e-05 dout=4.2e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=98 t=5.89e+01 wall=3] [dt=6.7e-01 dgrad=6.0e-05 dout=4.3e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=99 t=5.95e+01 wall=3] [dt=6.7e-01 dgrad=6.1e-05 dout=4.3e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=100 t=6.02e+01 wall=3] [dt=6.7e-01 dgrad=6.1e-05 dout=4.4e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=101 t=6.09e+01 wall=3] [dt=6.7e-01 dgrad=6.1e-05 dout=4.4e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=103 t=6.22e+01 wall=3] [dt=6.7e-01 dgrad=6.2e-05 dout=4.6e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=105 t=6.36e+01 wall=3] [dt=6.7e-01 dgrad=6.3e-05 dout=4.7e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=107 t=6.49e+01 wall=3] [dt=6.7e-01 dgrad=6.3e-05 dout=4.8e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=109 t=6.63e+01 wall=3] [dt=6.7e-01 dgrad=6.4e-05 dout=4.9e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=111 t=6.76e+01 wall=3] [dt=6.7e-01 dgrad=6.4e-05 dout=5.0e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=113 t=6.90e+01 wall=3] [dt=6.7e-01 dgrad=6.5e-05 dout=5.1e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=115 t=7.03e+01 wall=3] [dt=6.7e-01 dgrad=6.6e-05 dout=5.3e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=117 t=7.17e+01 wall=3] [dt=6.7e-01 dgrad=6.6e-05 dout=5.4e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=119 t=7.30e+01 wall=3] [dt=6.7e-01 dgrad=6.7e-05 dout=5.5e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=121 t=7.43e+01 wall=3] [dt=6.7e-01 dgrad=6.7e-05 dout=5.7e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=123 t=7.57e+01 wall=3] [dt=6.7e-01 dgrad=6.8e-05 dout=5.8e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=125 t=7.70e+01 wall=3] [dt=6.7e-01 dgrad=6.9e-05 dout=6.0e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=127 t=7.84e+01 wall=3] [dt=6.7e-01 dgrad=6.9e-05 dout=6.1e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=129 t=7.97e+01 wall=3] [dt=6.7e-01 dgrad=7.0e-05 dout=6.3e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=131 t=8.11e+01 wall=4] [dt=6.7e-01 dgrad=7.0e-05 dout=6.4e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=133 t=8.24e+01 wall=4] [dt=6.7e-01 dgrad=7.1e-05 dout=6.6e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=135 t=8.38e+01 wall=4] [dt=6.7e-01 dgrad=7.1e-05 dout=6.8e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=137 t=8.51e+01 wall=4] [dt=6.7e-01 dgrad=7.2e-05 dout=6.9e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=139 t=8.65e+01 wall=4] [dt=6.7e-01 dgrad=7.3e-05 dout=7.1e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=141 t=8.78e+01 wall=4] [dt=6.7e-01 dgrad=7.3e-05 dout=7.3e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=143 t=8.91e+01 wall=4] [dt=6.7e-01 dgrad=7.4e-05 dout=7.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=145 t=9.05e+01 wall=4] [dt=6.7e-01 dgrad=7.5e-05 dout=7.7e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=147 t=9.18e+01 wall=4] [dt=6.7e-01 dgrad=7.6e-05 dout=7.9e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=149 t=9.32e+01 wall=4] [dt=6.7e-01 dgrad=7.6e-05 dout=8.1e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=151 t=9.45e+01 wall=4] [dt=6.7e-01 dgrad=7.7e-05 dout=8.3e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=153 t=9.59e+01 wall=4] [dt=6.7e-01 dgrad=7.8e-05 dout=8.6e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=155 t=9.72e+01 wall=4] [dt=6.7e-01 dgrad=7.9e-05 dout=8.8e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=157 t=9.86e+01 wall=4] [dt=6.7e-01 dgrad=8.0e-05 dout=9.0e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=159 t=9.99e+01 wall=4] [dt=6.7e-01 dgrad=8.1e-05 dout=9.3e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=161 t=1.01e+02 wall=4] [dt=6.7e-01 dgrad=8.2e-05 dout=9.6e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=163 t=1.03e+02 wall=4] [dt=6.7e-01 dgrad=8.2e-05 dout=9.8e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=165 t=1.04e+02 wall=4] [dt=6.7e-01 dgrad=8.3e-05 dout=1.0e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=167 t=1.05e+02 wall=4] [dt=6.7e-01 dgrad=8.4e-05 dout=1.0e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=169 t=1.07e+02 wall=4] [dt=6.7e-01 dgrad=8.6e-05 dout=1.1e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=171 t=1.08e+02 wall=5] [dt=6.7e-01 dgrad=8.7e-05 dout=1.1e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=173 t=1.09e+02 wall=5] [dt=6.7e-01 dgrad=8.8e-05 dout=1.1e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=175 t=1.11e+02 wall=5] [dt=6.7e-01 dgrad=8.9e-05 dout=1.2e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=177 t=1.12e+02 wall=5] [dt=6.7e-01 dgrad=9.0e-05 dout=1.2e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=179 t=1.13e+02 wall=5] [dt=6.7e-01 dgrad=9.1e-05 dout=1.3e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=181 t=1.15e+02 wall=5] [dt=6.7e-01 dgrad=9.3e-05 dout=1.3e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=183 t=1.16e+02 wall=5] [dt=6.7e-01 dgrad=9.4e-05 dout=1.3e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=185 t=1.17e+02 wall=5] [dt=6.7e-01 dgrad=9.6e-05 dout=1.4e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=187 t=1.19e+02 wall=5] [dt=6.7e-01 dgrad=9.7e-05 dout=1.4e-05] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=189 t=1.20e+02 wall=5] [dt=6.7e-01 dgrad=9.8e-05 dout=1.5e-05] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=191 t=1.21e+02 wall=5] [dt=6.7e-01 dgrad=1.0e-04 dout=1.5e-05] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[192 +192] [dt=6.7e-02 dgrad=1.0e-04 dout=1.6e-05]\n",
            "[i=193 t=1.22e+02 wall=5] [dt=7.4e-02 dgrad=1.2e-06 dout=1.7e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=195 t=1.22e+02 wall=5] [dt=9.0e-02 dgrad=1.8e-06 dout=2.1e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=197 t=1.22e+02 wall=5] [dt=1.1e-01 dgrad=2.6e-06 dout=2.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=199 t=1.22e+02 wall=5] [dt=1.3e-01 dgrad=3.9e-06 dout=3.1e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=201 t=1.23e+02 wall=5] [dt=1.6e-01 dgrad=5.7e-06 dout=3.8e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=203 t=1.23e+02 wall=5] [dt=1.9e-01 dgrad=8.3e-06 dout=4.6e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=206 t=1.24e+02 wall=6] [dt=2.6e-01 dgrad=1.5e-05 dout=6.2e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=209 t=1.25e+02 wall=6] [dt=3.4e-01 dgrad=2.7e-05 dout=8.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=212 t=1.26e+02 wall=6] [dt=4.5e-01 dgrad=4.8e-05 dout=1.2e-05] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=215 t=1.27e+02 wall=6] [dt=5.0e-01 dgrad=5.9e-05 dout=1.3e-05] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=218 t=1.29e+02 wall=6] [dt=5.0e-01 dgrad=6.0e-05 dout=1.4e-05] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=221 t=1.30e+02 wall=6] [dt=5.0e-01 dgrad=6.1e-05 dout=1.4e-05] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.43]\n",
            "[i=224 t=1.32e+02 wall=6] [dt=5.0e-01 dgrad=6.2e-05 dout=1.5e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=227 t=1.33e+02 wall=6] [dt=5.0e-01 dgrad=6.3e-05 dout=1.6e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=230 t=1.35e+02 wall=6] [dt=5.0e-01 dgrad=6.4e-05 dout=1.6e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=233 t=1.36e+02 wall=6] [dt=5.0e-01 dgrad=6.5e-05 dout=1.7e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=236 t=1.38e+02 wall=6] [dt=5.0e-01 dgrad=6.6e-05 dout=1.8e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=239 t=1.39e+02 wall=6] [dt=5.0e-01 dgrad=6.7e-05 dout=1.9e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=242 t=1.41e+02 wall=6] [dt=5.0e-01 dgrad=6.9e-05 dout=2.0e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=245 t=1.42e+02 wall=7] [dt=5.0e-01 dgrad=7.0e-05 dout=2.0e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=248 t=1.44e+02 wall=7] [dt=5.0e-01 dgrad=7.2e-05 dout=2.1e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=251 t=1.45e+02 wall=7] [dt=5.0e-01 dgrad=7.3e-05 dout=2.2e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=254 t=1.47e+02 wall=7] [dt=5.0e-01 dgrad=7.5e-05 dout=2.4e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=257 t=1.48e+02 wall=7] [dt=5.0e-01 dgrad=7.6e-05 dout=2.5e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=260 t=1.50e+02 wall=7] [dt=5.0e-01 dgrad=7.8e-05 dout=2.6e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=263 t=1.51e+02 wall=7] [dt=5.0e-01 dgrad=8.0e-05 dout=2.8e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=266 t=1.53e+02 wall=7] [dt=5.0e-01 dgrad=8.1e-05 dout=2.9e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=269 t=1.54e+02 wall=7] [dt=5.0e-01 dgrad=8.3e-05 dout=3.1e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=272 t=1.56e+02 wall=7] [dt=5.0e-01 dgrad=8.5e-05 dout=3.2e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=275 t=1.57e+02 wall=7] [dt=5.0e-01 dgrad=8.7e-05 dout=3.4e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=278 t=1.59e+02 wall=7] [dt=5.0e-01 dgrad=8.9e-05 dout=3.6e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=281 t=1.60e+02 wall=7] [dt=5.0e-01 dgrad=9.1e-05 dout=3.8e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=284 t=1.62e+02 wall=7] [dt=5.0e-01 dgrad=9.3e-05 dout=4.1e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=287 t=1.63e+02 wall=7] [dt=5.0e-01 dgrad=9.5e-05 dout=4.3e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=290 t=1.65e+02 wall=7] [dt=5.0e-01 dgrad=9.7e-05 dout=4.6e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=293 t=1.66e+02 wall=7] [dt=5.0e-01 dgrad=1.0e-04 dout=4.8e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[294 +102] [dt=5.0e-02 dgrad=1.0e-04 dout=4.9e-05]\n",
            "[i=296 t=1.66e+02 wall=8] [dt=6.0e-02 dgrad=1.5e-06 dout=5.9e-06] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=299 t=1.66e+02 wall=8] [dt=8.0e-02 dgrad=2.6e-06 dout=8.0e-06] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=302 t=1.67e+02 wall=8] [dt=1.1e-01 dgrad=4.7e-06 dout=1.1e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=305 t=1.67e+02 wall=8] [dt=1.4e-01 dgrad=8.3e-06 dout=1.5e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=309 t=1.68e+02 wall=8] [dt=2.1e-01 dgrad=1.8e-05 dout=2.2e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=313 t=1.69e+02 wall=8] [dt=3.0e-01 dgrad=3.9e-05 dout=3.3e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=317 t=1.70e+02 wall=8] [dt=3.7e-01 dgrad=5.9e-05 dout=4.3e-05] [train aL=1.00e+00 err=0.40 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=321 t=1.72e+02 wall=8] [dt=3.7e-01 dgrad=6.0e-05 dout=4.5e-05] [train aL=1.00e+00 err=0.40 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=325 t=1.73e+02 wall=8] [dt=3.7e-01 dgrad=6.2e-05 dout=4.8e-05] [train aL=1.00e+00 err=0.40 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=329 t=1.75e+02 wall=8] [dt=3.7e-01 dgrad=6.3e-05 dout=5.1e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=333 t=1.76e+02 wall=8] [dt=3.7e-01 dgrad=6.5e-05 dout=5.5e-05] [train aL=1.00e+00 err=0.40 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=337 t=1.78e+02 wall=8] [dt=3.7e-01 dgrad=6.7e-05 dout=5.8e-05] [train aL=1.00e+00 err=0.40 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=341 t=1.79e+02 wall=8] [dt=3.7e-01 dgrad=6.9e-05 dout=6.2e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=345 t=1.81e+02 wall=8] [dt=3.7e-01 dgrad=7.1e-05 dout=6.6e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=349 t=1.82e+02 wall=8] [dt=3.7e-01 dgrad=7.3e-05 dout=7.1e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=353 t=1.84e+02 wall=9] [dt=3.7e-01 dgrad=7.5e-05 dout=7.6e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=357 t=1.85e+02 wall=9] [dt=3.7e-01 dgrad=7.7e-05 dout=8.2e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=361 t=1.87e+02 wall=9] [dt=3.7e-01 dgrad=8.0e-05 dout=8.8e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=365 t=1.88e+02 wall=9] [dt=3.7e-01 dgrad=8.2e-05 dout=9.4e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=369 t=1.90e+02 wall=9] [dt=3.7e-01 dgrad=8.5e-05 dout=1.0e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=373 t=1.91e+02 wall=9] [dt=3.7e-01 dgrad=8.8e-05 dout=1.1e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=377 t=1.92e+02 wall=9] [dt=3.7e-01 dgrad=9.2e-05 dout=1.2e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=381 t=1.94e+02 wall=9] [dt=3.7e-01 dgrad=9.5e-05 dout=1.3e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=385 t=1.95e+02 wall=9] [dt=3.7e-01 dgrad=9.9e-05 dout=1.4e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[386 +92] [dt=3.7e-02 dgrad=1.0e-04 dout=1.4e-04]\n",
            "[i=389 t=1.96e+02 wall=9] [dt=4.9e-02 dgrad=1.8e-06 dout=1.9e-05] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=393 t=1.96e+02 wall=9] [dt=7.2e-02 dgrad=3.8e-06 dout=2.8e-05] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=397 t=1.96e+02 wall=9] [dt=1.1e-01 dgrad=8.3e-06 dout=4.2e-05] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=401 t=1.97e+02 wall=9] [dt=1.5e-01 dgrad=1.8e-05 dout=6.3e-05] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=405 t=1.98e+02 wall=10] [dt=2.3e-01 dgrad=4.0e-05 dout=9.6e-05] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=409 t=1.99e+02 wall=10] [dt=2.7e-01 dgrad=6.0e-05 dout=1.2e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=414 t=2.00e+02 wall=10] [dt=2.7e-01 dgrad=6.2e-05 dout=1.3e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=419 t=2.01e+02 wall=10] [dt=2.7e-01 dgrad=6.5e-05 dout=1.4e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=424 t=2.03e+02 wall=10] [dt=2.7e-01 dgrad=6.7e-05 dout=1.6e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=429 t=2.04e+02 wall=10] [dt=2.7e-01 dgrad=7.0e-05 dout=1.7e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=434 t=2.05e+02 wall=10] [dt=2.7e-01 dgrad=7.3e-05 dout=1.9e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=439 t=2.07e+02 wall=10] [dt=2.7e-01 dgrad=7.6e-05 dout=2.0e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=444 t=2.08e+02 wall=10] [dt=2.7e-01 dgrad=7.9e-05 dout=2.2e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=449 t=2.09e+02 wall=10] [dt=2.7e-01 dgrad=8.2e-05 dout=2.4e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=454 t=2.11e+02 wall=10] [dt=2.7e-01 dgrad=8.6e-05 dout=2.7e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=459 t=2.12e+02 wall=10] [dt=2.7e-01 dgrad=9.0e-05 dout=2.9e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=464 t=2.14e+02 wall=11] [dt=2.7e-01 dgrad=9.4e-05 dout=3.3e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=469 t=2.15e+02 wall=11] [dt=2.7e-01 dgrad=9.9e-05 dout=3.6e-04] [train aL=9.98e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[471 +85] [dt=2.7e-02 dgrad=1.0e-04 dout=3.7e-04]\n",
            "[i=474 t=2.15e+02 wall=11] [dt=3.6e-02 dgrad=1.8e-06 dout=5.0e-05] [train aL=9.98e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=479 t=2.16e+02 wall=11] [dt=5.8e-02 dgrad=4.7e-06 dout=8.2e-05] [train aL=9.98e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=484 t=2.16e+02 wall=11] [dt=9.4e-02 dgrad=1.2e-05 dout=1.4e-04] [train aL=9.98e-01 err=0.42 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=489 t=2.17e+02 wall=11] [dt=1.5e-01 dgrad=3.2e-05 dout=2.3e-04] [train aL=9.98e-01 err=0.42 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=494 t=2.18e+02 wall=11] [dt=2.0e-01 dgrad=5.9e-05 dout=3.3e-04] [train aL=9.98e-01 err=0.42 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=499 t=2.19e+02 wall=11] [dt=2.0e-01 dgrad=6.2e-05 dout=3.5e-04] [train aL=9.98e-01 err=0.42 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=504 t=2.20e+02 wall=11] [dt=2.0e-01 dgrad=6.4e-05 dout=3.8e-04] [train aL=9.98e-01 err=0.42 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=509 t=2.21e+02 wall=11] [dt=2.0e-01 dgrad=6.7e-05 dout=4.2e-04] [train aL=9.98e-01 err=0.42 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=514 t=2.22e+02 wall=11] [dt=2.0e-01 dgrad=7.0e-05 dout=4.5e-04] [train aL=9.98e-01 err=0.42 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=520 t=2.23e+02 wall=11] [dt=2.0e-01 dgrad=7.3e-05 dout=5.0e-04] [train aL=9.98e-01 err=0.42 nd=400/400] [test aL=9.98e-01 err=0.46]\n",
            "[i=526 t=2.24e+02 wall=12] [dt=2.0e-01 dgrad=7.7e-05 dout=5.6e-04] [train aL=9.97e-01 err=0.42 nd=400/400] [test aL=9.98e-01 err=0.46]\n",
            "[i=532 t=2.25e+02 wall=12] [dt=2.0e-01 dgrad=8.2e-05 dout=6.3e-04] [train aL=9.97e-01 err=0.42 nd=400/400] [test aL=9.98e-01 err=0.46]\n",
            "[i=538 t=2.26e+02 wall=12] [dt=2.0e-01 dgrad=8.6e-05 dout=7.1e-04] [train aL=9.97e-01 err=0.42 nd=400/400] [test aL=9.98e-01 err=0.46]\n",
            "[i=544 t=2.28e+02 wall=12] [dt=2.0e-01 dgrad=9.1e-05 dout=7.9e-04] [train aL=9.97e-01 err=0.42 nd=400/400] [test aL=9.98e-01 err=0.46]\n",
            "[i=550 t=2.29e+02 wall=12] [dt=2.0e-01 dgrad=9.6e-05 dout=9.0e-04] [train aL=9.96e-01 err=0.42 nd=400/400] [test aL=9.98e-01 err=0.46]\n",
            "[554 +83] [dt=2.0e-02 dgrad=1.0e-04 dout=9.7e-04]\n",
            "[i=556 t=2.30e+02 wall=12] [dt=2.4e-02 dgrad=1.5e-06 dout=1.2e-04] [train aL=9.96e-01 err=0.42 nd=400/400] [test aL=9.97e-01 err=0.46]\n",
            "[i=562 t=2.30e+02 wall=12] [dt=4.3e-02 dgrad=4.7e-06 dout=2.1e-04] [train aL=9.96e-01 err=0.42 nd=400/400] [test aL=9.97e-01 err=0.46]\n",
            "[i=568 t=2.30e+02 wall=12] [dt=7.7e-02 dgrad=1.5e-05 dout=3.9e-04] [train aL=9.96e-01 err=0.42 nd=400/400] [test aL=9.97e-01 err=0.46]\n",
            "[i=574 t=2.31e+02 wall=12] [dt=1.4e-01 dgrad=4.8e-05 dout=7.4e-04] [train aL=9.96e-01 err=0.42 nd=400/400] [test aL=9.97e-01 err=0.46]\n",
            "[i=580 t=2.32e+02 wall=12] [dt=1.5e-01 dgrad=6.1e-05 dout=8.9e-04] [train aL=9.96e-01 err=0.42 nd=400/400] [test aL=9.97e-01 err=0.46]\n",
            "[i=586 t=2.33e+02 wall=13] [dt=1.5e-01 dgrad=6.4e-05 dout=9.8e-04] [train aL=9.95e-01 err=0.42 nd=400/400] [test aL=9.97e-01 err=0.46]\n",
            "[i=592 t=2.33e+02 wall=13] [dt=1.5e-01 dgrad=6.7e-05 dout=1.1e-03] [train aL=9.95e-01 err=0.42 nd=400/400] [test aL=9.97e-01 err=0.46]\n",
            "[i=598 t=2.34e+02 wall=13] [dt=1.5e-01 dgrad=7.1e-05 dout=1.2e-03] [train aL=9.94e-01 err=0.42 nd=400/400] [test aL=9.96e-01 err=0.46]\n",
            "[i=604 t=2.35e+02 wall=13] [dt=1.5e-01 dgrad=7.4e-05 dout=1.3e-03] [train aL=9.94e-01 err=0.42 nd=400/400] [test aL=9.96e-01 err=0.46]\n",
            "[i=610 t=2.36e+02 wall=13] [dt=1.5e-01 dgrad=7.9e-05 dout=1.5e-03] [train aL=9.94e-01 err=0.43 nd=400/400] [test aL=9.96e-01 err=0.47]\n",
            "[i=616 t=2.37e+02 wall=13] [dt=1.5e-01 dgrad=8.3e-05 dout=1.7e-03] [train aL=9.93e-01 err=0.42 nd=400/400] [test aL=9.95e-01 err=0.47]\n",
            "[i=622 t=2.38e+02 wall=13] [dt=1.5e-01 dgrad=8.8e-05 dout=1.9e-03] [train aL=9.92e-01 err=0.42 nd=400/400] [test aL=9.95e-01 err=0.46]\n",
            "[i=629 t=2.39e+02 wall=13] [dt=1.5e-01 dgrad=9.4e-05 dout=2.1e-03] [train aL=9.92e-01 err=0.42 nd=400/400] [test aL=9.94e-01 err=0.46]\n",
            "[635 +81] [dt=1.5e-02 dgrad=1.0e-04 dout=2.4e-03]\n",
            "[i=636 t=2.40e+02 wall=13] [dt=1.6e-02 dgrad=1.2e-06 dout=2.6e-04] [train aL=9.91e-01 err=0.42 nd=400/400] [test aL=9.94e-01 err=0.46]\n",
            "[i=643 t=2.40e+02 wall=14] [dt=3.2e-02 dgrad=4.6e-06 dout=5.3e-04] [train aL=9.91e-01 err=0.42 nd=400/400] [test aL=9.94e-01 err=0.46]\n",
            "[i=650 t=2.40e+02 wall=14] [dt=6.2e-02 dgrad=1.8e-05 dout=1.1e-03] [train aL=9.91e-01 err=0.43 nd=400/400] [test aL=9.93e-01 err=0.46]\n",
            "[i=657 t=2.41e+02 wall=14] [dt=1.1e-01 dgrad=5.9e-05 dout=2.1e-03] [train aL=9.90e-01 err=0.43 nd=400/400] [test aL=9.93e-01 err=0.46]\n",
            "[i=664 t=2.42e+02 wall=14] [dt=1.1e-01 dgrad=6.2e-05 dout=2.3e-03] [train aL=9.89e-01 err=0.43 nd=400/400] [test aL=9.92e-01 err=0.46]\n",
            "[i=671 t=2.42e+02 wall=14] [dt=1.1e-01 dgrad=6.6e-05 dout=2.6e-03] [train aL=9.88e-01 err=0.43 nd=400/400] [test aL=9.92e-01 err=0.46]\n",
            "[i=678 t=2.43e+02 wall=14] [dt=1.1e-01 dgrad=7.0e-05 dout=2.9e-03] [train aL=9.87e-01 err=0.43 nd=400/400] [test aL=9.91e-01 err=0.46]\n",
            "[i=685 t=2.44e+02 wall=14] [dt=1.1e-01 dgrad=7.4e-05 dout=3.3e-03] [train aL=9.86e-01 err=0.44 nd=400/400] [test aL=9.90e-01 err=0.46]\n",
            "[i=692 t=2.45e+02 wall=14] [dt=1.1e-01 dgrad=7.8e-05 dout=3.8e-03] [train aL=9.85e-01 err=0.44 nd=400/400] [test aL=9.89e-01 err=0.46]\n",
            "[i=699 t=2.46e+02 wall=15] [dt=1.1e-01 dgrad=8.3e-05 dout=4.3e-03] [train aL=9.83e-01 err=0.44 nd=400/400] [test aL=9.88e-01 err=0.46]\n",
            "[i=706 t=2.46e+02 wall=15] [dt=1.1e-01 dgrad=8.9e-05 dout=4.9e-03] [train aL=9.82e-01 err=0.43 nd=400/400] [test aL=9.87e-01 err=0.46]\n",
            "[i=713 t=2.47e+02 wall=15] [dt=1.1e-01 dgrad=9.5e-05 dout=5.6e-03] [train aL=9.80e-01 err=0.43 nd=400/400] [test aL=9.85e-01 err=0.45]\n",
            "[718 +83] [dt=1.1e-02 dgrad=1.0e-04 dout=6.2e-03]\n",
            "[i=720 t=2.48e+02 wall=15] [dt=1.3e-02 dgrad=1.5e-06 dout=7.5e-04] [train aL=9.78e-01 err=0.43 nd=400/400] [test aL=9.84e-01 err=0.45]\n",
            "[i=727 t=2.48e+02 wall=15] [dt=2.6e-02 dgrad=5.6e-06 dout=1.5e-03] [train aL=9.78e-01 err=0.43 nd=400/400] [test aL=9.84e-01 err=0.45]\n",
            "[i=735 t=2.48e+02 wall=15] [dt=5.6e-02 dgrad=2.7e-05 dout=3.4e-03] [train aL=9.77e-01 err=0.43 nd=400/400] [test aL=9.83e-01 err=0.45]\n",
            "[i=743 t=2.49e+02 wall=15] [dt=8.2e-02 dgrad=6.0e-05 dout=5.6e-03] [train aL=9.75e-01 err=0.43 nd=400/400] [test aL=9.81e-01 err=0.45]\n",
            "[i=751 t=2.49e+02 wall=15] [dt=8.2e-02 dgrad=6.4e-05 dout=6.4e-03] [train aL=9.73e-01 err=0.43 nd=400/400] [test aL=9.79e-01 err=0.45]\n",
            "[i=759 t=2.50e+02 wall=16] [dt=8.2e-02 dgrad=6.9e-05 dout=7.3e-03] [train aL=9.70e-01 err=0.43 nd=400/400] [test aL=9.77e-01 err=0.45]\n",
            "[i=767 t=2.51e+02 wall=16] [dt=8.2e-02 dgrad=7.4e-05 dout=8.4e-03] [train aL=9.67e-01 err=0.43 nd=400/400] [test aL=9.75e-01 err=0.45]\n",
            "[i=775 t=2.51e+02 wall=16] [dt=8.2e-02 dgrad=7.9e-05 dout=9.7e-03] [train aL=9.63e-01 err=0.44 nd=400/400] [test aL=9.72e-01 err=0.45]\n",
            "[i=783 t=2.52e+02 wall=16] [dt=8.2e-02 dgrad=8.1e-05 dout=1.1e-02] [train aL=9.59e-01 err=0.44 nd=400/400] [test aL=9.68e-01 err=0.45]\n",
            "[i=791 t=2.53e+02 wall=16] [dt=8.2e-02 dgrad=6.7e-05 dout=1.3e-02] [train aL=9.54e-01 err=0.45 nd=400/400] [test aL=9.64e-01 err=0.45]\n",
            "[i=799 t=2.53e+02 wall=16] [dt=9.9e-02 dgrad=7.7e-05 dout=1.8e-02] [train aL=9.48e-01 err=0.45 nd=400/400] [test aL=9.60e-01 err=0.45]\n",
            "[800 +82] [dt=9.9e-03 dgrad=1.2e-04 dout=1.8e-02]\n",
            "[i=807 t=2.53e+02 wall=16] [dt=1.9e-02 dgrad=6.1e-06 dout=3.5e-03] [train aL=9.47e-01 err=0.45 nd=400/400] [test aL=9.59e-01 err=0.45]\n",
            "[i=815 t=2.54e+02 wall=17] [dt=3.8e-02 dgrad=6.5e-05 dout=6.7e-03] [train aL=9.45e-01 err=0.45 nd=400/400] [test aL=9.58e-01 err=0.45]\n",
            "[821 +21] [dt=3.8e-03 dgrad=1.1e-04 dout=6.4e-03]\n",
            "[i=823 t=2.54e+02 wall=17] [dt=4.5e-03 dgrad=1.6e-06 dout=7.8e-04] [train aL=9.44e-01 err=0.45 nd=398/400] [test aL=9.57e-01 err=0.45]\n",
            "[i=831 t=2.54e+02 wall=17] [dt=9.7e-03 dgrad=8.4e-06 dout=1.6e-03] [train aL=9.43e-01 err=0.45 nd=398/400] [test aL=9.56e-01 err=0.45]\n",
            "[i=839 t=2.54e+02 wall=17] [dt=2.1e-02 dgrad=5.8e-05 dout=3.4e-03] [train aL=9.42e-01 err=0.45 nd=396/400] [test aL=9.56e-01 err=0.45]\n",
            "[i=848 t=2.54e+02 wall=17] [dt=2.3e-02 dgrad=5.4e-05 dout=3.4e-03] [train aL=9.41e-01 err=0.44 nd=395/400] [test aL=9.55e-01 err=0.45]\n",
            "[i=857 t=2.54e+02 wall=18] [dt=2.3e-02 dgrad=7.1e-05 dout=3.2e-03] [train aL=9.40e-01 err=0.43 nd=394/400] [test aL=9.55e-01 err=0.45]\n",
            "[i=866 t=2.55e+02 wall=18] [dt=2.3e-02 dgrad=6.1e-05 dout=2.9e-03] [train aL=9.38e-01 err=0.43 nd=393/400] [test aL=9.54e-01 err=0.46]\n",
            "[i=875 t=2.55e+02 wall=18] [dt=2.5e-02 dgrad=7.2e-05 dout=2.9e-03] [train aL=9.37e-01 err=0.43 nd=393/400] [test aL=9.54e-01 err=0.45]\n",
            "[i=884 t=2.55e+02 wall=18] [dt=2.5e-02 dgrad=8.4e-05 dout=2.7e-03] [train aL=9.36e-01 err=0.44 nd=392/400] [test aL=9.54e-01 err=0.46]\n",
            "[i=893 t=2.55e+02 wall=18] [dt=2.5e-02 dgrad=7.1e-05 dout=2.5e-03] [train aL=9.35e-01 err=0.43 nd=391/400] [test aL=9.54e-01 err=0.46]\n",
            "[895 +74] [dt=2.5e-03 dgrad=1.1e-04 dout=2.4e-03]\n",
            "[i=902 t=2.55e+02 wall=19] [dt=4.9e-03 dgrad=4.6e-06 dout=4.7e-04] [train aL=9.34e-01 err=0.43 nd=391/400] [test aL=9.54e-01 err=0.46]\n",
            "[i=911 t=2.55e+02 wall=19] [dt=1.2e-02 dgrad=2.3e-05 dout=1.1e-03] [train aL=9.34e-01 err=0.43 nd=391/400] [test aL=9.54e-01 err=0.46]\n",
            "[i=920 t=2.56e+02 wall=19] [dt=2.1e-02 dgrad=5.8e-05 dout=1.8e-03] [train aL=9.33e-01 err=0.43 nd=391/400] [test aL=9.55e-01 err=0.46]\n",
            "[i=929 t=2.56e+02 wall=19] [dt=2.5e-02 dgrad=8.3e-05 dout=2.1e-03] [train aL=9.32e-01 err=0.43 nd=389/400] [test aL=9.55e-01 err=0.46]\n",
            "[930 +35] [dt=2.5e-03 dgrad=1.2e-04 dout=2.0e-03]\n",
            "[i=938 t=2.56e+02 wall=20] [dt=5.3e-03 dgrad=7.5e-06 dout=4.4e-04] [train aL=9.32e-01 err=0.43 nd=389/400] [test aL=9.55e-01 err=0.46]\n",
            "[i=947 t=2.56e+02 wall=20] [dt=1.3e-02 dgrad=2.1e-05 dout=1.0e-03] [train aL=9.32e-01 err=0.43 nd=389/400] [test aL=9.55e-01 err=0.47]\n",
            "[i=957 t=2.56e+02 wall=20] [dt=2.2e-02 dgrad=4.7e-05 dout=1.8e-03] [train aL=9.31e-01 err=0.42 nd=388/400] [test aL=9.55e-01 err=0.47]\n",
            "[959 +29] [dt=2.5e-03 dgrad=1.0e-04 dout=2.0e-03]\n",
            "[i=967 t=2.56e+02 wall=20] [dt=5.3e-03 dgrad=4.1e-06 dout=4.3e-04] [train aL=9.31e-01 err=0.42 nd=388/400] [test aL=9.55e-01 err=0.47]\n",
            "[i=977 t=2.56e+02 wall=20] [dt=1.1e-02 dgrad=6.0e-05 dout=9.3e-04] [train aL=9.30e-01 err=0.42 nd=388/400] [test aL=9.56e-01 err=0.47]\n",
            "[i=987 t=2.56e+02 wall=21] [dt=2.4e-02 dgrad=5.4e-05 dout=1.9e-03] [train aL=9.30e-01 err=0.41 nd=386/400] [test aL=9.56e-01 err=0.47]\n",
            "[993 +34] [dt=2.4e-03 dgrad=1.1e-04 dout=1.9e-03]\n",
            "[i=997 t=2.57e+02 wall=21] [dt=3.5e-03 dgrad=2.4e-06 dout=2.8e-04] [train aL=9.29e-01 err=0.41 nd=386/400] [test aL=9.56e-01 err=0.47]\n",
            "[i=1007 t=2.57e+02 wall=21] [dt=9.2e-03 dgrad=2.1e-05 dout=7.5e-04] [train aL=9.29e-01 err=0.42 nd=385/400] [test aL=9.56e-01 err=0.47]\n",
            "[i=1017 t=2.57e+02 wall=21] [dt=2.4e-02 dgrad=6.5e-05 dout=2.2e-03] [train aL=9.28e-01 err=0.41 nd=384/400] [test aL=9.57e-01 err=0.47]\n",
            "[1018 +25] [dt=2.4e-03 dgrad=1.6e-04 dout=2.2e-03]\n",
            "[i=1027 t=2.57e+02 wall=21] [dt=5.6e-03 dgrad=4.0e-06 dout=5.3e-04] [train aL=9.28e-01 err=0.42 nd=384/400] [test aL=9.57e-01 err=0.47]\n",
            "[i=1037 t=2.57e+02 wall=21] [dt=1.5e-02 dgrad=3.9e-05 dout=1.5e-03] [train aL=9.28e-01 err=0.42 nd=384/400] [test aL=9.57e-01 err=0.47]\n",
            "[1042 +24] [dt=2.1e-03 dgrad=1.5e-04 dout=2.2e-03]\n",
            "[i=1047 t=2.57e+02 wall=22] [dt=3.4e-03 dgrad=4.2e-06 dout=3.5e-04] [train aL=9.27e-01 err=0.42 nd=384/400] [test aL=9.57e-01 err=0.47]\n",
            "[i=1057 t=2.57e+02 wall=22] [dt=8.9e-03 dgrad=3.3e-05 dout=9.1e-04] [train aL=9.27e-01 err=0.42 nd=384/400] [test aL=9.57e-01 err=0.47]\n",
            "[1062 +20] [dt=1.1e-03 dgrad=1.1e-04 dout=1.1e-03]\n",
            "[i=1068 t=2.57e+02 wall=22] [dt=1.9e-03 dgrad=3.6e-06 dout=1.9e-04] [train aL=9.27e-01 err=0.42 nd=383/400] [test aL=9.57e-01 err=0.47]\n",
            "[i=1079 t=2.57e+02 wall=22] [dt=5.4e-03 dgrad=1.3e-05 dout=5.3e-04] [train aL=9.27e-01 err=0.42 nd=383/400] [test aL=9.57e-01 err=0.47]\n",
            "[i=1090 t=2.57e+02 wall=22] [dt=1.6e-02 dgrad=4.4e-05 dout=1.4e-03] [train aL=9.26e-01 err=0.42 nd=383/400] [test aL=9.58e-01 err=0.47]\n",
            "[i=1101 t=2.57e+02 wall=22] [dt=2.3e-02 dgrad=6.8e-05 dout=2.0e-03] [train aL=9.26e-01 err=0.42 nd=382/400] [test aL=9.58e-01 err=0.47]\n",
            "[1103 +41] [dt=2.3e-03 dgrad=1.3e-04 dout=2.0e-03]\n",
            "[i=1112 t=2.58e+02 wall=23] [dt=5.4e-03 dgrad=8.2e-06 dout=4.8e-04] [train aL=9.25e-01 err=0.42 nd=382/400] [test aL=9.58e-01 err=0.47]\n",
            "[i=1123 t=2.58e+02 wall=23] [dt=1.5e-02 dgrad=1.7e-05 dout=1.5e-03] [train aL=9.25e-01 err=0.42 nd=382/400] [test aL=9.58e-01 err=0.47]\n",
            "[1128 +25] [dt=2.2e-03 dgrad=1.0e-04 dout=2.2e-03]\n",
            "[i=1134 t=2.58e+02 wall=23] [dt=4.0e-03 dgrad=2.9e-06 dout=3.8e-04] [train aL=9.25e-01 err=0.43 nd=382/400] [test aL=9.59e-01 err=0.47]\n",
            "[i=1145 t=2.58e+02 wall=23] [dt=1.1e-02 dgrad=1.1e-05 dout=1.1e-03] [train aL=9.24e-01 err=0.43 nd=382/400] [test aL=9.59e-01 err=0.47]\n",
            "[i=1156 t=2.58e+02 wall=23] [dt=1.8e-02 dgrad=8.1e-05 dout=1.5e-03] [train aL=9.23e-01 err=0.43 nd=382/400] [test aL=9.59e-01 err=0.47]\n",
            "[1162 +34] [dt=2.0e-03 dgrad=1.1e-04 dout=1.6e-03]\n",
            "[i=1167 t=2.58e+02 wall=24] [dt=3.2e-03 dgrad=2.9e-06 dout=2.5e-04] [train aL=9.23e-01 err=0.43 nd=382/400] [test aL=9.59e-01 err=0.47]\n",
            "[i=1179 t=2.58e+02 wall=24] [dt=1.0e-02 dgrad=1.2e-05 dout=7.7e-04] [train aL=9.23e-01 err=0.43 nd=382/400] [test aL=9.59e-01 err=0.48]\n",
            "[i=1191 t=2.58e+02 wall=24] [dt=2.6e-02 dgrad=5.7e-05 dout=2.1e-03] [train aL=9.22e-01 err=0.43 nd=381/400] [test aL=9.60e-01 err=0.47]\n",
            "[i=1203 t=2.59e+02 wall=24] [dt=3.5e-02 dgrad=5.9e-05 dout=3.0e-03] [train aL=9.20e-01 err=0.42 nd=379/400] [test aL=9.60e-01 err=0.47]\n",
            "[i=1215 t=2.59e+02 wall=24] [dt=4.2e-02 dgrad=4.4e-05 dout=4.0e-03] [train aL=9.18e-01 err=0.43 nd=378/400] [test aL=9.61e-01 err=0.47]\n",
            "[1218 +56] [dt=4.7e-03 dgrad=1.4e-04 dout=4.5e-03]\n",
            "[i=1227 t=2.59e+02 wall=24] [dt=1.1e-02 dgrad=3.6e-06 dout=1.1e-03] [train aL=9.18e-01 err=0.43 nd=378/400] [test aL=9.61e-01 err=0.48]\n",
            "[i=1239 t=2.60e+02 wall=25] [dt=3.4e-02 dgrad=2.4e-05 dout=3.5e-03] [train aL=9.16e-01 err=0.44 nd=378/400] [test aL=9.61e-01 err=0.47]\n",
            "[1250 +32] [dt=5.6e-03 dgrad=1.3e-04 dout=6.4e-03]\n",
            "[i=1251 t=2.60e+02 wall=25] [dt=6.1e-03 dgrad=1.5e-06 dout=7.0e-04] [train aL=9.14e-01 err=0.44 nd=377/400] [test aL=9.62e-01 err=0.47]\n",
            "[i=1263 t=2.60e+02 wall=25] [dt=1.9e-02 dgrad=1.9e-05 dout=2.3e-03] [train aL=9.13e-01 err=0.44 nd=377/400] [test aL=9.62e-01 err=0.48]\n",
            "[i=1275 t=2.61e+02 wall=25] [dt=4.5e-02 dgrad=5.0e-05 dout=5.9e-03] [train aL=9.11e-01 err=0.43 nd=377/400] [test aL=9.63e-01 err=0.46]\n",
            "[1280 +30] [dt=5.5e-03 dgrad=1.1e-04 dout=7.6e-03]\n",
            "[i=1287 t=2.61e+02 wall=26] [dt=1.1e-02 dgrad=3.4e-06 dout=1.5e-03] [train aL=9.09e-01 err=0.43 nd=376/400] [test aL=9.63e-01 err=0.47]\n",
            "[i=1300 t=2.61e+02 wall=26] [dt=3.7e-02 dgrad=3.2e-05 dout=5.5e-03] [train aL=9.08e-01 err=0.43 nd=376/400] [test aL=9.64e-01 err=0.47]\n",
            "[1310 +30] [dt=4.9e-03 dgrad=1.1e-04 dout=8.2e-03]\n",
            "[i=1313 t=2.62e+02 wall=26] [dt=6.5e-03 dgrad=1.9e-06 dout=1.1e-03] [train aL=9.05e-01 err=0.43 nd=376/400] [test aL=9.64e-01 err=0.46]\n",
            "[i=1326 t=2.62e+02 wall=26] [dt=2.2e-02 dgrad=1.2e-05 dout=3.9e-03] [train aL=9.04e-01 err=0.43 nd=376/400] [test aL=9.65e-01 err=0.47]\n",
            "[i=1339 t=2.62e+02 wall=26] [dt=5.3e-02 dgrad=6.3e-05 dout=1.0e-02] [train aL=9.00e-01 err=0.42 nd=377/400] [test aL=9.66e-01 err=0.47]\n",
            "[1348 +38] [dt=7.1e-03 dgrad=1.0e-04 dout=1.5e-02]\n",
            "[i=1352 t=2.63e+02 wall=27] [dt=1.0e-02 dgrad=2.3e-06 dout=2.2e-03] [train aL=8.96e-01 err=0.42 nd=378/400] [test aL=9.67e-01 err=0.47]\n",
            "[i=1365 t=2.63e+02 wall=27] [dt=3.6e-02 dgrad=5.3e-05 dout=7.7e-03] [train aL=8.94e-01 err=0.42 nd=378/400] [test aL=9.68e-01 err=0.47]\n",
            "[1369 +21] [dt=3.6e-03 dgrad=1.1e-04 dout=7.5e-03]\n",
            "[i=1378 t=2.63e+02 wall=27] [dt=8.4e-03 dgrad=6.7e-06 dout=1.8e-03] [train aL=8.93e-01 err=0.43 nd=378/400] [test aL=9.68e-01 err=0.47]\n",
            "[i=1391 t=2.64e+02 wall=27] [dt=2.2e-02 dgrad=6.9e-05 dout=4.4e-03] [train aL=8.91e-01 err=0.43 nd=374/400] [test aL=9.69e-01 err=0.47]\n",
            "[1400 +31] [dt=2.2e-03 dgrad=1.0e-04 dout=4.1e-03]\n",
            "[i=1404 t=2.64e+02 wall=27] [dt=3.2e-03 dgrad=2.2e-06 dout=6.0e-04] [train aL=8.90e-01 err=0.43 nd=373/400] [test aL=9.69e-01 err=0.46]\n",
            "[i=1418 t=2.64e+02 wall=28] [dt=1.2e-02 dgrad=3.7e-05 dout=2.2e-03] [train aL=8.90e-01 err=0.43 nd=371/400] [test aL=9.70e-01 err=0.46]\n",
            "[i=1432 t=2.64e+02 wall=28] [dt=1.5e-02 dgrad=6.3e-05 dout=2.3e-03] [train aL=8.89e-01 err=0.43 nd=370/400] [test aL=9.70e-01 err=0.46]\n",
            "[i=1446 t=2.64e+02 wall=28] [dt=1.5e-02 dgrad=6.7e-05 dout=2.0e-03] [train aL=8.88e-01 err=0.43 nd=370/400] [test aL=9.71e-01 err=0.47]\n",
            "[i=1460 t=2.65e+02 wall=28] [dt=1.5e-02 dgrad=6.4e-05 dout=1.8e-03] [train aL=8.87e-01 err=0.43 nd=369/400] [test aL=9.72e-01 err=0.47]\n",
            "[i=1474 t=2.65e+02 wall=28] [dt=1.5e-02 dgrad=5.6e-05 dout=1.5e-03] [train aL=8.87e-01 err=0.43 nd=367/400] [test aL=9.72e-01 err=0.47]\n",
            "[i=1488 t=2.65e+02 wall=29] [dt=1.5e-02 dgrad=5.3e-05 dout=1.3e-03] [train aL=8.86e-01 err=0.43 nd=363/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1502 t=2.65e+02 wall=29] [dt=1.8e-02 dgrad=5.8e-05 dout=1.3e-03] [train aL=8.85e-01 err=0.43 nd=362/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1516 t=2.65e+02 wall=29] [dt=2.0e-02 dgrad=7.9e-05 dout=1.2e-03] [train aL=8.85e-01 err=0.42 nd=358/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1531 t=2.66e+02 wall=29] [dt=2.4e-02 dgrad=5.2e-05 dout=1.2e-03] [train aL=8.84e-01 err=0.42 nd=356/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1546 t=2.66e+02 wall=29] [dt=2.6e-02 dgrad=5.6e-05 dout=1.2e-03] [train aL=8.83e-01 err=0.42 nd=353/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1561 t=2.67e+02 wall=30] [dt=2.6e-02 dgrad=7.9e-05 dout=1.1e-03] [train aL=8.83e-01 err=0.42 nd=352/400] [test aL=9.73e-01 err=0.47]\n",
            "[1568 +168] [dt=2.6e-03 dgrad=1.1e-04 dout=1.1e-03]\n",
            "[i=1576 t=2.67e+02 wall=30] [dt=5.6e-03 dgrad=6.0e-06 dout=2.5e-04] [train aL=8.82e-01 err=0.42 nd=352/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1591 t=2.67e+02 wall=30] [dt=1.9e-02 dgrad=5.0e-05 dout=9.6e-04] [train aL=8.82e-01 err=0.42 nd=352/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1606 t=2.67e+02 wall=31] [dt=2.3e-02 dgrad=5.8e-05 dout=1.2e-03] [train aL=8.81e-01 err=0.42 nd=351/400] [test aL=9.73e-01 err=0.47]\n",
            "[1616 +48] [dt=2.3e-03 dgrad=1.0e-04 dout=1.2e-03]\n",
            "[i=1621 t=2.67e+02 wall=31] [dt=3.8e-03 dgrad=2.8e-06 dout=1.9e-04] [train aL=8.81e-01 err=0.42 nd=351/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1636 t=2.68e+02 wall=31] [dt=1.6e-02 dgrad=4.2e-05 dout=8.1e-04] [train aL=8.80e-01 err=0.42 nd=351/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1652 t=2.68e+02 wall=32] [dt=1.9e-02 dgrad=5.7e-05 dout=9.5e-04] [train aL=8.80e-01 err=0.42 nd=352/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1668 t=2.68e+02 wall=32] [dt=2.1e-02 dgrad=5.3e-05 dout=1.1e-03] [train aL=8.79e-01 err=0.42 nd=350/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1684 t=2.69e+02 wall=32] [dt=2.8e-02 dgrad=5.6e-05 dout=1.4e-03] [train aL=8.78e-01 err=0.42 nd=351/400] [test aL=9.73e-01 err=0.47]\n",
            "[1688 +72] [dt=2.8e-03 dgrad=1.0e-04 dout=1.4e-03]\n",
            "[i=1700 t=2.69e+02 wall=33] [dt=8.7e-03 dgrad=7.9e-06 dout=4.6e-04] [train aL=8.78e-01 err=0.42 nd=351/400] [test aL=9.73e-01 err=0.47]\n",
            "[1712 +24] [dt=2.3e-03 dgrad=1.0e-04 dout=1.2e-03]\n",
            "[i=1716 t=2.69e+02 wall=33] [dt=3.3e-03 dgrad=2.2e-06 dout=1.8e-04] [train aL=8.78e-01 err=0.42 nd=350/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1732 t=2.69e+02 wall=33] [dt=1.5e-02 dgrad=3.0e-05 dout=7.7e-04] [train aL=8.77e-01 err=0.42 nd=350/400] [test aL=9.73e-01 err=0.47]\n",
            "[1743 +31] [dt=2.4e-03 dgrad=1.1e-04 dout=1.2e-03]\n",
            "[i=1748 t=2.69e+02 wall=33] [dt=3.9e-03 dgrad=3.1e-06 dout=1.9e-04] [train aL=8.77e-01 err=0.42 nd=350/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1765 t=2.70e+02 wall=34] [dt=2.0e-02 dgrad=6.2e-05 dout=9.8e-04] [train aL=8.77e-01 err=0.42 nd=350/400] [test aL=9.73e-01 err=0.47]\n",
            "[1776 +33] [dt=2.7e-03 dgrad=1.0e-04 dout=1.2e-03]\n",
            "[i=1782 t=2.70e+02 wall=34] [dt=4.7e-03 dgrad=3.5e-06 dout=2.1e-04] [train aL=8.76e-01 err=0.43 nd=349/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1799 t=2.70e+02 wall=34] [dt=2.4e-02 dgrad=5.8e-05 dout=1.2e-03] [train aL=8.76e-01 err=0.43 nd=347/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1816 t=2.70e+02 wall=34] [dt=3.2e-02 dgrad=6.9e-05 dout=1.3e-03] [train aL=8.75e-01 err=0.43 nd=347/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1833 t=2.71e+02 wall=35] [dt=3.8e-02 dgrad=4.9e-05 dout=1.6e-03] [train aL=8.74e-01 err=0.42 nd=347/400] [test aL=9.74e-01 err=0.47]\n",
            "[1838 +62] [dt=4.2e-03 dgrad=1.1e-04 dout=1.7e-03]\n",
            "[i=1850 t=2.71e+02 wall=35] [dt=1.3e-02 dgrad=9.2e-06 dout=5.3e-04] [train aL=8.73e-01 err=0.42 nd=347/400] [test aL=9.74e-01 err=0.47]\n",
            "[i=1867 t=2.72e+02 wall=35] [dt=4.1e-02 dgrad=6.4e-05 dout=1.7e-03] [train aL=8.72e-01 err=0.42 nd=347/400] [test aL=9.74e-01 err=0.47]\n",
            "[1878 +40] [dt=4.6e-03 dgrad=1.2e-04 dout=1.9e-03]\n",
            "[i=1885 t=2.72e+02 wall=35] [dt=8.9e-03 dgrad=5.2e-06 dout=3.6e-04] [train aL=8.71e-01 err=0.41 nd=347/400] [test aL=9.75e-01 err=0.47]\n",
            "[i=1903 t=2.73e+02 wall=36] [dt=3.7e-02 dgrad=5.1e-05 dout=1.7e-03] [train aL=8.70e-01 err=0.41 nd=347/400] [test aL=9.75e-01 err=0.47]\n",
            "[i=1921 t=2.73e+02 wall=36] [dt=4.1e-02 dgrad=5.9e-05 dout=1.7e-03] [train aL=8.69e-01 err=0.41 nd=348/400] [test aL=9.76e-01 err=0.47]\n",
            "[i=1939 t=2.74e+02 wall=36] [dt=5.4e-02 dgrad=5.5e-05 dout=2.1e-03] [train aL=8.68e-01 err=0.41 nd=347/400] [test aL=9.76e-01 err=0.47]\n",
            "[i=1957 t=2.75e+02 wall=36] [dt=6.6e-02 dgrad=5.4e-05 dout=2.4e-03] [train aL=8.66e-01 err=0.41 nd=345/400] [test aL=9.77e-01 err=0.47]\n",
            "[i=1975 t=2.77e+02 wall=37] [dt=6.6e-02 dgrad=7.9e-05 dout=2.2e-03] [train aL=8.63e-01 err=0.41 nd=343/400] [test aL=9.78e-01 err=0.46]\n",
            "[i=1993 t=2.78e+02 wall=37] [dt=6.6e-02 dgrad=7.0e-05 dout=2.3e-03] [train aL=8.61e-01 err=0.41 nd=342/400] [test aL=9.79e-01 err=0.46]\n",
            "[1998 +120] [dt=6.6e-03 dgrad=1.1e-04 dout=2.4e-03]\n",
            "[i=2012 t=2.78e+02 wall=37] [dt=2.5e-02 dgrad=1.2e-05 dout=9.2e-04] [train aL=8.59e-01 err=0.42 nd=342/400] [test aL=9.79e-01 err=0.46]\n",
            "[i=2031 t=2.79e+02 wall=38] [dt=6.5e-02 dgrad=7.3e-05 dout=2.4e-03] [train aL=8.57e-01 err=0.42 nd=342/400] [test aL=9.80e-01 err=0.46]\n",
            "[2040 +42] [dt=6.5e-03 dgrad=1.0e-04 dout=2.6e-03]\n",
            "[i=2050 t=2.80e+02 wall=38] [dt=1.7e-02 dgrad=7.4e-06 dout=6.9e-04] [train aL=8.55e-01 err=0.41 nd=340/400] [test aL=9.80e-01 err=0.46]\n",
            "[i=2069 t=2.81e+02 wall=39] [dt=5.8e-02 dgrad=5.7e-05 dout=2.5e-03] [train aL=8.53e-01 err=0.41 nd=341/400] [test aL=9.81e-01 err=0.46]\n",
            "[i=2088 t=2.82e+02 wall=39] [dt=7.0e-02 dgrad=6.1e-05 dout=2.9e-03] [train aL=8.50e-01 err=0.41 nd=341/400] [test aL=9.81e-01 err=0.46]\n",
            "[i=2107 t=2.83e+02 wall=39] [dt=7.0e-02 dgrad=5.4e-05 dout=2.6e-03] [train aL=8.45e-01 err=0.41 nd=339/400] [test aL=9.81e-01 err=0.46]\n",
            "[i=2126 t=2.85e+02 wall=40] [dt=7.0e-02 dgrad=8.4e-05 dout=2.4e-03] [train aL=8.41e-01 err=0.40 nd=339/400] [test aL=9.80e-01 err=0.46]\n",
            "[2128 +88] [dt=7.0e-03 dgrad=1.0e-04 dout=2.4e-03]\n",
            "[i=2146 t=2.85e+02 wall=40] [dt=3.9e-02 dgrad=2.1e-05 dout=1.3e-03] [train aL=8.39e-01 err=0.40 nd=339/400] [test aL=9.80e-01 err=0.46]\n",
            "[2155 +27] [dt=6.9e-03 dgrad=1.1e-04 dout=2.3e-03]\n",
            "[i=2166 t=2.86e+02 wall=40] [dt=2.0e-02 dgrad=7.6e-06 dout=6.5e-04] [train aL=8.37e-01 err=0.40 nd=337/400] [test aL=9.80e-01 err=0.46]\n",
            "[i=2186 t=2.87e+02 wall=41] [dt=7.5e-02 dgrad=6.9e-05 dout=2.7e-03] [train aL=8.33e-01 err=0.40 nd=336/400] [test aL=9.79e-01 err=0.46]\n",
            "[i=2206 t=2.88e+02 wall=41] [dt=8.2e-02 dgrad=8.3e-05 dout=3.1e-03] [train aL=8.26e-01 err=0.40 nd=333/400] [test aL=9.77e-01 err=0.46]\n",
            "[i=2226 t=2.90e+02 wall=41] [dt=8.2e-02 dgrad=8.1e-05 dout=3.1e-03] [train aL=8.18e-01 err=0.39 nd=333/400] [test aL=9.73e-01 err=0.45]\n",
            "[2230 +75] [dt=8.2e-03 dgrad=1.1e-04 dout=3.3e-03]\n",
            "[i=2246 t=2.90e+02 wall=41] [dt=3.8e-02 dgrad=2.2e-05 dout=1.6e-03] [train aL=8.14e-01 err=0.39 nd=332/400] [test aL=9.72e-01 err=0.46]\n",
            "[2256 +26] [dt=5.5e-03 dgrad=1.3e-04 dout=2.5e-03]\n",
            "[i=2267 t=2.91e+02 wall=42] [dt=1.6e-02 dgrad=6.4e-06 dout=7.3e-04] [train aL=8.11e-01 err=0.39 nd=332/400] [test aL=9.70e-01 err=0.45]\n",
            "[2281 +25] [dt=5.5e-03 dgrad=1.0e-04 dout=2.8e-03]\n",
            "[i=2288 t=2.92e+02 wall=42] [dt=1.1e-02 dgrad=4.2e-06 dout=5.5e-04] [train aL=8.08e-01 err=0.39 nd=330/400] [test aL=9.69e-01 err=0.45]\n",
            "[i=2309 t=2.92e+02 wall=42] [dt=4.4e-02 dgrad=8.8e-05 dout=2.6e-03] [train aL=8.03e-01 err=0.39 nd=328/400] [test aL=9.67e-01 err=0.46]\n",
            "[2325 +44] [dt=5.9e-03 dgrad=1.1e-04 dout=3.4e-03]\n",
            "[i=2330 t=2.93e+02 wall=43] [dt=9.5e-03 dgrad=3.0e-06 dout=5.5e-04] [train aL=7.97e-01 err=0.38 nd=328/400] [test aL=9.63e-01 err=0.45]\n",
            "[i=2351 t=2.94e+02 wall=43] [dt=4.0e-02 dgrad=6.7e-05 dout=2.8e-03] [train aL=7.93e-01 err=0.38 nd=327/400] [test aL=9.61e-01 err=0.45]\n",
            "[2359 +34] [dt=4.8e-03 dgrad=1.0e-04 dout=3.9e-03]\n",
            "[i=2372 t=2.94e+02 wall=44] [dt=1.7e-02 dgrad=1.0e-05 dout=1.3e-03] [train aL=7.90e-01 err=0.38 nd=326/400] [test aL=9.59e-01 err=0.45]\n",
            "[2386 +27] [dt=5.2e-03 dgrad=1.1e-04 dout=4.6e-03]\n",
            "[i=2394 t=2.94e+02 wall=44] [dt=1.1e-02 dgrad=9.2e-06 dout=1.0e-03] [train aL=7.85e-01 err=0.38 nd=325/400] [test aL=9.56e-01 err=0.46]\n",
            "[i=2416 t=2.95e+02 wall=44] [dt=5.7e-02 dgrad=5.4e-05 dout=5.5e-03] [train aL=7.78e-01 err=0.38 nd=326/400] [test aL=9.52e-01 err=0.46]\n",
            "[2426 +40] [dt=6.2e-03 dgrad=1.7e-04 dout=6.1e-03]\n",
            "[i=2438 t=2.96e+02 wall=45] [dt=2.0e-02 dgrad=1.1e-05 dout=1.9e-03] [train aL=7.72e-01 err=0.38 nd=323/400] [test aL=9.47e-01 err=0.46]\n",
            "[2453 +27] [dt=4.6e-03 dgrad=1.4e-04 dout=4.8e-03]\n",
            "[i=2460 t=2.96e+02 wall=45] [dt=9.0e-03 dgrad=9.8e-06 dout=9.5e-04] [train aL=7.66e-01 err=0.37 nd=322/400] [test aL=9.44e-01 err=0.45]\n",
            "[i=2482 t=2.97e+02 wall=46] [dt=2.8e-02 dgrad=5.4e-05 dout=3.8e-03] [train aL=7.60e-01 err=0.36 nd=322/400] [test aL=9.40e-01 err=0.45]\n",
            "[2488 +35] [dt=3.4e-03 dgrad=1.2e-04 dout=4.7e-03]\n",
            "[i=2504 t=2.97e+02 wall=46] [dt=1.6e-02 dgrad=1.4e-05 dout=2.3e-03] [train aL=7.57e-01 err=0.35 nd=320/400] [test aL=9.38e-01 err=0.45]\n",
            "[2522 +34] [dt=3.4e-03 dgrad=1.1e-04 dout=4.9e-03]\n",
            "[i=2527 t=2.98e+02 wall=46] [dt=5.4e-03 dgrad=3.0e-06 dout=7.8e-04] [train aL=7.50e-01 err=0.34 nd=321/400] [test aL=9.33e-01 err=0.45]\n",
            "[i=2550 t=2.98e+02 wall=47] [dt=3.6e-02 dgrad=8.2e-05 dout=5.4e-03] [train aL=7.44e-01 err=0.34 nd=321/400] [test aL=9.28e-01 err=0.45]\n",
            "[2551 +29] [dt=3.6e-03 dgrad=1.6e-04 dout=5.4e-03]\n",
            "[i=2573 t=2.98e+02 wall=47] [dt=2.7e-02 dgrad=5.9e-05 dout=4.1e-03] [train aL=7.40e-01 err=0.34 nd=320/400] [test aL=9.25e-01 err=0.44]\n",
            "[2591 +40] [dt=3.3e-03 dgrad=2.1e-04 dout=5.2e-03]\n",
            "[i=2596 t=2.99e+02 wall=47] [dt=5.2e-03 dgrad=6.0e-06 dout=8.5e-04] [train aL=7.31e-01 err=0.33 nd=320/400] [test aL=9.19e-01 err=0.44]\n",
            "[i=2619 t=2.99e+02 wall=48] [dt=3.2e-02 dgrad=5.6e-05 dout=5.2e-03] [train aL=7.24e-01 err=0.33 nd=318/400] [test aL=9.13e-01 err=0.44]\n",
            "[2623 +32] [dt=3.5e-03 dgrad=1.1e-04 dout=5.7e-03]\n",
            "[i=2643 t=3.00e+02 wall=48] [dt=2.4e-02 dgrad=3.2e-05 dout=3.9e-03] [train aL=7.18e-01 err=0.32 nd=318/400] [test aL=9.09e-01 err=0.43]\n",
            "[2655 +32] [dt=3.2e-03 dgrad=1.1e-04 dout=5.5e-03]\n",
            "[i=2667 t=3.00e+02 wall=48] [dt=9.9e-03 dgrad=1.2e-05 dout=1.7e-03] [train aL=7.11e-01 err=0.31 nd=316/400] [test aL=9.03e-01 err=0.43]\n",
            "[2681 +26] [dt=3.4e-03 dgrad=1.0e-04 dout=6.5e-03]\n",
            "[i=2691 t=3.00e+02 wall=49] [dt=8.9e-03 dgrad=6.7e-06 dout=1.7e-03] [train aL=7.04e-01 err=0.30 nd=313/400] [test aL=8.97e-01 err=0.42]\n",
            "[2708 +27] [dt=3.1e-03 dgrad=1.2e-04 dout=6.3e-03]\n",
            "[i=2715 t=3.01e+02 wall=49] [dt=6.0e-03 dgrad=3.9e-06 dout=1.2e-03] [train aL=6.97e-01 err=0.29 nd=313/400] [test aL=8.91e-01 err=0.41]\n",
            "[2732 +24] [dt=2.7e-03 dgrad=1.2e-04 dout=5.8e-03]\n",
            "[i=2739 t=3.01e+02 wall=50] [dt=5.3e-03 dgrad=9.3e-06 dout=1.1e-03] [train aL=6.92e-01 err=0.28 nd=315/400] [test aL=8.86e-01 err=0.41]\n",
            "[2759 +27] [dt=2.7e-03 dgrad=1.6e-04 dout=6.0e-03]\n",
            "[i=2763 t=3.01e+02 wall=50] [dt=4.0e-03 dgrad=3.6e-06 dout=8.8e-04] [train aL=6.85e-01 err=0.29 nd=315/400] [test aL=8.80e-01 err=0.41]\n",
            "[i=2788 t=3.02e+02 wall=50] [dt=2.2e-02 dgrad=5.1e-05 dout=5.3e-03] [train aL=6.77e-01 err=0.29 nd=315/400] [test aL=8.73e-01 err=0.40]\n",
            "[2790 +31] [dt=2.2e-03 dgrad=1.7e-04 dout=5.3e-03]\n",
            "[i=2813 t=3.02e+02 wall=51] [dt=1.8e-02 dgrad=6.9e-05 dout=4.1e-03] [train aL=6.72e-01 err=0.29 nd=314/400] [test aL=8.68e-01 err=0.41]\n",
            "[2820 +30] [dt=2.4e-03 dgrad=1.0e-04 dout=5.4e-03]\n",
            "[i=2838 t=3.02e+02 wall=51] [dt=1.3e-02 dgrad=3.6e-05 dout=2.9e-03] [train aL=6.66e-01 err=0.28 nd=311/400] [test aL=8.63e-01 err=0.41]\n",
            "[2843 +23] [dt=1.9e-03 dgrad=1.1e-04 dout=4.4e-03]\n",
            "[i=2863 t=3.02e+02 wall=51] [dt=1.1e-02 dgrad=4.9e-05 dout=2.5e-03] [train aL=6.61e-01 err=0.28 nd=311/400] [test aL=8.58e-01 err=0.40]\n",
            "[2870 +27] [dt=1.7e-03 dgrad=1.6e-04 dout=4.1e-03]\n",
            "[i=2888 t=3.03e+02 wall=52] [dt=9.7e-03 dgrad=3.9e-05 dout=2.3e-03] [train aL=6.56e-01 err=0.28 nd=311/400] [test aL=8.54e-01 err=0.40]\n",
            "[2902 +32] [dt=2.3e-03 dgrad=1.7e-04 dout=5.5e-03]\n",
            "[i=2914 t=3.03e+02 wall=52] [dt=7.1e-03 dgrad=4.5e-06 dout=1.7e-03] [train aL=6.49e-01 err=0.28 nd=310/400] [test aL=8.47e-01 err=0.40]\n",
            "[2927 +25] [dt=1.9e-03 dgrad=2.0e-04 dout=4.5e-03]\n",
            "[i=2940 t=3.03e+02 wall=53] [dt=6.4e-03 dgrad=9.2e-06 dout=1.5e-03] [train aL=6.43e-01 err=0.28 nd=311/400] [test aL=8.41e-01 err=0.39]\n",
            "[2954 +27] [dt=1.8e-03 dgrad=1.0e-04 dout=4.6e-03]\n",
            "[i=2966 t=3.03e+02 wall=53] [dt=5.7e-03 dgrad=1.0e-05 dout=1.5e-03] [train aL=6.37e-01 err=0.28 nd=310/400] [test aL=8.35e-01 err=0.38]\n",
            "[2977 +23] [dt=1.6e-03 dgrad=1.0e-04 dout=4.1e-03]\n",
            "[i=2992 t=3.03e+02 wall=53] [dt=6.8e-03 dgrad=5.3e-06 dout=1.8e-03] [train aL=6.32e-01 err=0.27 nd=309/400] [test aL=8.30e-01 err=0.38]\n",
            "[3012 +35] [dt=1.5e-03 dgrad=1.4e-04 dout=4.0e-03]\n",
            "[i=3018 t=3.04e+02 wall=54] [dt=2.6e-03 dgrad=6.6e-06 dout=7.1e-04] [train aL=6.24e-01 err=0.27 nd=306/400] [test aL=8.22e-01 err=0.38]\n",
            "[3032 +20] [dt=9.8e-04 dgrad=1.2e-04 dout=3.0e-03]\n",
            "[i=3045 t=3.04e+02 wall=54] [dt=3.4e-03 dgrad=1.4e-05 dout=1.1e-03] [train aL=6.21e-01 err=0.27 nd=305/400] [test aL=8.19e-01 err=0.38]\n",
            "[3063 +31] [dt=1.4e-03 dgrad=1.3e-04 dout=4.8e-03]\n",
            "[i=3072 t=3.04e+02 wall=54] [dt=3.3e-03 dgrad=2.7e-05 dout=1.1e-03] [train aL=6.16e-01 err=0.27 nd=304/400] [test aL=8.13e-01 err=0.37]\n",
            "[3094 +31] [dt=1.4e-03 dgrad=1.3e-04 dout=4.8e-03]\n",
            "[i=3099 t=3.04e+02 wall=55] [dt=2.3e-03 dgrad=3.9e-06 dout=7.7e-04] [train aL=6.08e-01 err=0.27 nd=303/400] [test aL=8.06e-01 err=0.37]\n",
            "[3114 +20] [dt=8.6e-04 dgrad=1.4e-04 dout=3.2e-03]\n",
            "[i=3126 t=3.04e+02 wall=55] [dt=2.7e-03 dgrad=2.4e-05 dout=1.0e-03] [train aL=6.05e-01 err=0.27 nd=303/400] [test aL=8.02e-01 err=0.38]\n",
            "[3150 +36] [dt=1.1e-03 dgrad=1.1e-04 dout=4.5e-03]\n",
            "[i=3153 t=3.04e+02 wall=56] [dt=1.5e-03 dgrad=2.4e-06 dout=6.0e-04] [train aL=5.98e-01 err=0.26 nd=301/400] [test aL=7.96e-01 err=0.37]\n",
            "[3178 +28] [dt=1.6e-03 dgrad=1.6e-04 dout=6.9e-03]\n",
            "[i=3181 t=3.04e+02 wall=56] [dt=2.2e-03 dgrad=2.4e-06 dout=9.1e-04] [train aL=5.91e-01 err=0.26 nd=301/400] [test aL=7.89e-01 err=0.36]\n",
            "[3205 +27] [dt=1.6e-03 dgrad=1.0e-04 dout=6.6e-03]\n",
            "[i=3209 t=3.05e+02 wall=57] [dt=2.3e-03 dgrad=2.4e-06 dout=9.7e-04] [train aL=5.83e-01 err=0.25 nd=298/400] [test aL=7.80e-01 err=0.36]\n",
            "[3230 +25] [dt=1.4e-03 dgrad=1.0e-04 dout=5.9e-03]\n",
            "[i=3237 t=3.05e+02 wall=58] [dt=2.8e-03 dgrad=4.2e-06 dout=1.1e-03] [train aL=5.76e-01 err=0.25 nd=295/400] [test aL=7.72e-01 err=0.34]\n",
            "[3263 +33] [dt=1.5e-03 dgrad=1.5e-04 dout=6.1e-03]\n",
            "[i=3265 t=3.05e+02 wall=58] [dt=1.9e-03 dgrad=1.5e-06 dout=7.4e-04] [train aL=5.65e-01 err=0.24 nd=293/400] [test aL=7.61e-01 err=0.33]\n",
            "[3291 +28] [dt=1.5e-03 dgrad=1.8e-04 dout=6.0e-03]\n",
            "[i=3293 t=3.05e+02 wall=59] [dt=1.8e-03 dgrad=2.7e-06 dout=7.2e-04] [train aL=5.55e-01 err=0.23 nd=291/400] [test aL=7.52e-01 err=0.32]\n",
            "[3316 +25] [dt=1.1e-03 dgrad=1.1e-04 dout=4.5e-03]\n",
            "[i=3322 t=3.05e+02 wall=59] [dt=2.0e-03 dgrad=3.6e-06 dout=7.9e-04] [train aL=5.48e-01 err=0.23 nd=289/400] [test aL=7.45e-01 err=0.31]\n",
            "[3341 +25] [dt=1.0e-03 dgrad=1.8e-04 dout=3.8e-03]\n",
            "[i=3351 t=3.06e+02 wall=59] [dt=2.6e-03 dgrad=7.9e-06 dout=9.9e-04] [train aL=5.42e-01 err=0.23 nd=289/400] [test aL=7.39e-01 err=0.31]\n",
            "[3369 +28] [dt=1.5e-03 dgrad=1.5e-04 dout=5.4e-03]\n",
            "[i=3380 t=3.06e+02 wall=60] [dt=4.1e-03 dgrad=2.4e-06 dout=1.5e-03] [train aL=5.35e-01 err=0.22 nd=287/400] [test aL=7.31e-01 err=0.30]\n",
            "[i=3409 t=3.06e+02 wall=60] [dt=1.7e-02 dgrad=6.5e-05 dout=5.0e-03] [train aL=5.18e-01 err=0.21 nd=285/400] [test aL=7.15e-01 err=0.29]\n",
            "[3415 +46] [dt=2.1e-03 dgrad=1.3e-04 dout=6.2e-03]\n",
            "[3437 +22] [dt=1.7e-03 dgrad=1.0e-04 dout=5.1e-03]\n",
            "[i=3438 t=3.06e+02 wall=61] [dt=1.9e-03 dgrad=8.5e-07 dout=5.6e-04] [train aL=5.06e-01 err=0.21 nd=280/400] [test aL=7.02e-01 err=0.29]\n",
            "[i=3468 t=3.07e+02 wall=61] [dt=2.2e-02 dgrad=4.1e-05 dout=6.8e-03] [train aL=4.91e-01 err=0.20 nd=278/400] [test aL=6.87e-01 err=0.28]\n",
            "[3478 +41] [dt=2.7e-03 dgrad=1.6e-04 dout=8.5e-03]\n",
            "[i=3498 t=3.07e+02 wall=62] [dt=1.8e-02 dgrad=4.3e-05 dout=6.0e-03] [train aL=4.70e-01 err=0.20 nd=269/400] [test aL=6.66e-01 err=0.26]\n",
            "[3507 +29] [dt=2.7e-03 dgrad=1.3e-04 dout=9.0e-03]\n",
            "[i=3528 t=3.07e+02 wall=62] [dt=2.0e-02 dgrad=6.0e-05 dout=6.9e-03] [train aL=4.50e-01 err=0.19 nd=257/400] [test aL=6.46e-01 err=0.25]\n",
            "[3551 +44] [dt=3.5e-03 dgrad=1.5e-04 dout=1.3e-02]\n",
            "[i=3558 t=3.08e+02 wall=63] [dt=6.8e-03 dgrad=9.6e-06 dout=2.5e-03] [train aL=4.15e-01 err=0.17 nd=238/400] [test aL=6.09e-01 err=0.23]\n",
            "[3580 +29] [dt=3.4e-03 dgrad=1.2e-04 dout=1.3e-02]\n",
            "[i=3588 t=3.08e+02 wall=63] [dt=7.4e-03 dgrad=1.1e-05 dout=2.8e-03] [train aL=3.89e-01 err=0.16 nd=232/400] [test aL=5.82e-01 err=0.22]\n",
            "[3597 +17] [dt=1.7e-03 dgrad=1.0e-04 dout=6.7e-03]\n",
            "[i=3619 t=3.09e+02 wall=63] [dt=1.4e-02 dgrad=1.3e-05 dout=5.5e-03] [train aL=3.77e-01 err=0.15 nd=229/400] [test aL=5.69e-01 err=0.21]\n",
            "[3645 +48] [dt=2.8e-03 dgrad=1.4e-04 dout=1.2e-02]\n",
            "[i=3650 t=3.09e+02 wall=64] [dt=4.4e-03 dgrad=4.1e-06 dout=1.9e-03] [train aL=3.43e-01 err=0.14 nd=219/400] [test aL=5.31e-01 err=0.18]\n",
            "[3666 +21] [dt=2.0e-03 dgrad=2.9e-04 dout=8.6e-03]\n",
            "[i=3681 t=3.10e+02 wall=64] [dt=8.5e-03 dgrad=2.9e-05 dout=3.7e-03] [train aL=3.31e-01 err=0.14 nd=215/400] [test aL=5.17e-01 err=0.17]\n",
            "[3696 +30] [dt=1.8e-03 dgrad=1.1e-04 dout=7.6e-03]\n",
            "[i=3712 t=3.10e+02 wall=65] [dt=8.4e-03 dgrad=1.8e-05 dout=3.5e-03] [train aL=3.16e-01 err=0.14 nd=205/400] [test aL=5.01e-01 err=0.16]\n",
            "[3726 +30] [dt=2.0e-03 dgrad=1.0e-04 dout=8.1e-03]\n",
            "[i=3744 t=3.10e+02 wall=65] [dt=1.1e-02 dgrad=1.7e-05 dout=4.5e-03] [train aL=3.01e-01 err=0.13 nd=196/400] [test aL=4.83e-01 err=0.15]\n",
            "[3755 +29] [dt=1.8e-03 dgrad=1.1e-04 dout=6.8e-03]\n",
            "[i=3776 t=3.10e+02 wall=66] [dt=1.2e-02 dgrad=3.9e-05 dout=4.4e-03] [train aL=2.88e-01 err=0.13 nd=191/400] [test aL=4.66e-01 err=0.14]\n",
            "[3797 +42] [dt=1.7e-03 dgrad=1.1e-04 dout=5.8e-03]\n",
            "[i=3808 t=3.11e+02 wall=66] [dt=5.0e-03 dgrad=8.6e-06 dout=1.7e-03] [train aL=2.73e-01 err=0.12 nd=177/400] [test aL=4.46e-01 err=0.13]\n",
            "[3822 +25] [dt=1.7e-03 dgrad=1.4e-04 dout=5.4e-03]\n",
            "[i=3840 t=3.11e+02 wall=67] [dt=9.6e-03 dgrad=1.5e-05 dout=2.9e-03] [train aL=2.64e-01 err=0.12 nd=173/400] [test aL=4.34e-01 err=0.13]\n",
            "[3866 +44] [dt=2.1e-03 dgrad=1.4e-04 dout=5.2e-03]\n",
            "[i=3872 t=3.12e+02 wall=67] [dt=3.6e-03 dgrad=4.5e-06 dout=9.3e-04] [train aL=2.46e-01 err=0.10 nd=159/400] [test aL=4.09e-01 err=0.12]\n",
            "[3890 +24] [dt=1.5e-03 dgrad=1.4e-04 dout=3.6e-03]\n",
            "[i=3905 t=3.12e+02 wall=68] [dt=6.3e-03 dgrad=1.1e-05 dout=1.5e-03] [train aL=2.39e-01 err=0.09 nd=155/400] [test aL=3.99e-01 err=0.12]\n",
            "[i=3938 t=3.12e+02 wall=68] [dt=2.4e-02 dgrad=8.4e-05 dout=5.7e-03] [train aL=2.23e-01 err=0.09 nd=143/400] [test aL=3.74e-01 err=0.11]\n",
            "[3941 +51] [dt=2.4e-03 dgrad=1.3e-04 dout=5.7e-03]\n",
            "[3970 +29] [dt=2.4e-03 dgrad=1.1e-04 dout=5.6e-03]\n",
            "[i=3971 t=3.13e+02 wall=69] [dt=2.6e-03 dgrad=1.2e-06 dout=6.1e-04] [train aL=2.12e-01 err=0.08 nd=135/400] [test aL=3.58e-01 err=0.10]\n",
            "[3993 +23] [dt=1.9e-03 dgrad=1.2e-04 dout=4.4e-03]\n",
            "[i=4004 t=3.13e+02 wall=69] [dt=5.5e-03 dgrad=9.7e-06 dout=1.3e-03] [train aL=2.06e-01 err=0.08 nd=133/400] [test aL=3.48e-01 err=0.10]\n",
            "[4023 +30] [dt=1.9e-03 dgrad=2.1e-04 dout=4.4e-03]\n",
            "[i=4037 t=3.13e+02 wall=70] [dt=7.2e-03 dgrad=3.1e-05 dout=1.7e-03] [train aL=1.98e-01 err=0.07 nd=130/400] [test aL=3.36e-01 err=0.10]\n",
            "[4068 +45] [dt=2.3e-03 dgrad=1.3e-04 dout=4.7e-03]\n",
            "[i=4071 t=3.14e+02 wall=71] [dt=3.0e-03 dgrad=1.8e-06 dout=6.2e-04] [train aL=1.86e-01 err=0.07 nd=121/400] [test aL=3.17e-01 err=0.09]\n",
            "[i=4105 t=3.14e+02 wall=71] [dt=2.2e-02 dgrad=6.6e-05 dout=5.0e-03] [train aL=1.74e-01 err=0.06 nd=114/400] [test aL=2.98e-01 err=0.09]\n",
            "[4110 +42] [dt=2.2e-03 dgrad=4.8e-04 dout=5.1e-03]\n",
            "[4129 +19] [dt=9.3e-04 dgrad=1.0e-04 dout=2.0e-03]\n",
            "[i=4139 t=3.14e+02 wall=72] [dt=2.4e-03 dgrad=8.5e-06 dout=5.2e-04] [train aL=1.69e-01 err=0.05 nd=108/400] [test aL=2.91e-01 err=0.09]\n",
            "[4170 +41] [dt=1.6e-03 dgrad=1.2e-04 dout=3.6e-03]\n",
            "[i=4173 t=3.15e+02 wall=72] [dt=2.2e-03 dgrad=1.9e-06 dout=4.8e-04] [train aL=1.63e-01 err=0.05 nd=108/400] [test aL=2.81e-01 err=0.09]\n",
            "[4207 +37] [dt=1.8e-03 dgrad=2.2e-04 dout=3.9e-03]\n",
            "[i=4208 t=3.15e+02 wall=73] [dt=1.9e-03 dgrad=1.9e-06 dout=4.2e-04] [train aL=1.56e-01 err=0.05 nd=104/400] [test aL=2.69e-01 err=0.08]\n",
            "[i=4243 t=3.15e+02 wall=74] [dt=2.3e-02 dgrad=7.9e-05 dout=5.6e-03] [train aL=1.48e-01 err=0.04 nd=98/400] [test aL=2.57e-01 err=0.08]\n",
            "[4249 +42] [dt=2.3e-03 dgrad=1.1e-04 dout=5.7e-03]\n",
            "[4265 +16] [dt=1.1e-03 dgrad=1.2e-04 dout=2.6e-03]\n",
            "[i=4278 t=3.16e+02 wall=74] [dt=3.7e-03 dgrad=2.4e-06 dout=8.9e-04] [train aL=1.44e-01 err=0.04 nd=95/400] [test aL=2.50e-01 err=0.07]\n",
            "[4292 +27] [dt=1.3e-03 dgrad=1.2e-04 dout=2.9e-03]\n",
            "[i=4313 t=3.16e+02 wall=75] [dt=9.4e-03 dgrad=2.0e-05 dout=2.0e-03] [train aL=1.41e-01 err=0.03 nd=91/400] [test aL=2.44e-01 err=0.07]\n",
            "[4325 +33] [dt=1.7e-03 dgrad=1.7e-04 dout=3.5e-03]\n",
            "[4346 +21] [dt=9.2e-04 dgrad=1.2e-04 dout=1.8e-03]\n",
            "[i=4349 t=3.16e+02 wall=75] [dt=1.2e-03 dgrad=2.2e-06 dout=2.4e-04] [train aL=1.36e-01 err=0.03 nd=88/400] [test aL=2.37e-01 err=0.07]\n",
            "[i=4385 t=3.16e+02 wall=76] [dt=1.6e-02 dgrad=5.3e-05 dout=3.1e-03] [train aL=1.32e-01 err=0.02 nd=85/400] [test aL=2.30e-01 err=0.06]\n",
            "[4391 +45] [dt=1.8e-03 dgrad=1.0e-04 dout=3.4e-03]\n",
            "[4421 +30] [dt=1.4e-03 dgrad=1.1e-04 dout=2.7e-03]\n",
            "[i=4421 t=3.17e+02 wall=76] [dt=1.4e-03 dgrad=1.1e-06 dout=2.7e-04] [train aL=1.27e-01 err=0.02 nd=80/400] [test aL=2.22e-01 err=0.06]\n",
            "[4455 +34] [dt=2.1e-03 dgrad=1.9e-04 dout=3.7e-03]\n",
            "[i=4457 t=3.17e+02 wall=77] [dt=2.5e-03 dgrad=2.6e-06 dout=4.6e-04] [train aL=1.22e-01 err=0.01 nd=79/400] [test aL=2.14e-01 err=0.06]\n",
            "[i=4493 t=3.17e+02 wall=77] [dt=2.5e-02 dgrad=5.6e-05 dout=4.3e-03] [train aL=1.14e-01 err=0.00 nd=71/400] [test aL=2.03e-01 err=0.04]\n",
            "[4496 +41] [dt=2.5e-03 dgrad=2.4e-04 dout=4.3e-03]\n",
            "[i=4530 t=3.18e+02 wall=78] [dt=2.4e-02 dgrad=4.7e-05 dout=4.3e-03] [train aL=1.06e-01 err=0.00 nd=67/400] [test aL=1.93e-01 err=0.03]\n",
            "[i=4567 t=3.19e+02 wall=78] [dt=3.6e-02 dgrad=5.3e-05 dout=7.7e-03] [train aL=8.26e-02 err=0.00 nd=66/400] [test aL=1.69e-01 err=0.02]\n",
            "[4601 +105] [dt=3.9e-03 dgrad=1.3e-04 dout=1.3e-02]\n",
            "[i=4604 t=3.20e+02 wall=79] [dt=5.2e-03 dgrad=2.3e-06 dout=1.7e-03] [train aL=4.37e-02 err=0.00 nd=69/400] [test aL=1.39e-01 err=0.00]\n",
            "[4626 +25] [dt=2.9e-03 dgrad=1.0e-04 dout=1.0e-02]\n",
            "[i=4641 t=3.21e+02 wall=79] [dt=1.2e-02 dgrad=3.6e-05 dout=4.3e-03] [train aL=2.65e-02 err=0.00 nd=71/400] [test aL=1.27e-01 err=0.00]\n",
            "[4646 +20] [dt=1.5e-03 dgrad=1.0e-04 dout=5.1e-03]\n",
            "[4679 +33] [dt=8.2e-04 dgrad=1.0e-04 dout=2.5e-03]\n",
            "[i=4679 t=3.21e+02 wall=80] [dt=8.2e-04 dgrad=1.0e-06 dout=2.5e-04] [train aL=1.81e-02 err=0.00 nd=70/400] [test aL=1.21e-01 err=0.00]\n",
            "[i=4717 t=3.21e+02 wall=81] [dt=5.5e-03 dgrad=5.7e-05 dout=1.4e-03] [train aL=1.44e-02 err=0.00 nd=57/400] [test aL=1.17e-01 err=0.00]\n",
            "[i=4755 t=3.21e+02 wall=81] [dt=6.7e-03 dgrad=5.3e-05 dout=1.1e-03] [train aL=1.07e-02 err=0.00 nd=32/400] [test aL=1.12e-01 err=0.00]\n",
            "[i=4793 t=3.22e+02 wall=82] [dt=9.7e-03 dgrad=5.8e-05 dout=1.3e-03] [train aL=7.64e-03 err=0.00 nd=22/400] [test aL=1.06e-01 err=0.00]\n",
            "[i=4832 t=3.22e+02 wall=83] [dt=1.2e-02 dgrad=5.2e-05 dout=1.1e-03] [train aL=5.31e-03 err=0.00 nd=9/400] [test aL=9.90e-02 err=0.00]\n",
            "[i=4871 t=3.23e+02 wall=83] [dt=1.4e-02 dgrad=5.0e-05 dout=1.1e-03] [train aL=3.70e-03 err=0.00 nd=7/400] [test aL=9.29e-02 err=0.00]\n",
            "[i=4910 t=3.23e+02 wall=84] [dt=1.9e-02 dgrad=5.6e-05 dout=1.2e-03] [train aL=2.58e-03 err=0.00 nd=4/400] [test aL=8.75e-02 err=0.00]\n",
            "[i=4949 t=3.24e+02 wall=85] [dt=2.3e-02 dgrad=5.2e-05 dout=1.3e-03] [train aL=1.81e-03 err=0.00 nd=2/400] [test aL=8.25e-02 err=0.00]\n",
            "[i=4989 t=3.25e+02 wall=85] [dt=2.8e-02 dgrad=5.0e-05 dout=1.3e-03] [train aL=1.27e-03 err=0.00 nd=1/400] [test aL=7.79e-02 err=0.00]\n",
            "[i=5029 t=3.27e+02 wall=86] [dt=3.4e-02 dgrad=5.2e-05 dout=1.1e-03] [train aL=9.20e-04 err=0.00 nd=0/400] [test aL=7.38e-02 err=0.00]\n",
            "[i=5069 t=3.28e+02 wall=86] [dt=4.5e-02 dgrad=5.8e-05 dout=8.8e-04] [train aL=6.82e-04 err=0.00 nd=0/400] [test aL=7.02e-02 err=0.00]\n",
            "[i=5071 t=3.28e+02 wall=86] [dt=4.5e-02 dgrad=5.7e-05 dout=8.5e-04] [train aL=6.72e-04 err=0.00 nd=0/400] [test aL=7.00e-02 err=0.00]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# ANSI escape sequences for colors\n",
        "COLORS = {\n",
        "    'HEADER': '\\033[95m',  # Magenta\n",
        "    'OKCYAN': '\\033[96m',  # Cyan\n",
        "    'OKGREEN': '\\033[92m',  # Green\n",
        "    'WARNING': '\\033[93m',  # Yellow\n",
        "    'FAIL': '\\033[91m',    # Red\n",
        "    'ENDC': '\\033[0m',     # Reset to default\n",
        "    'BOLD': '\\033[1m',     # Bold\n",
        "    'UNDERLINE': '\\033[4m' # Underline\n",
        "}\n",
        "\n",
        "def color_text(text, color_name):\n",
        "    return f\"{COLORS[color_name]}{text}{COLORS['ENDC']}\"\n",
        "\n",
        "def load_and_print_summary(file_path, verbose=False):\n",
        "    \"\"\"\n",
        "    保存された pickle ファイルを読み込み，その中身の概要を出力する関数\n",
        "\n",
        "    Parameters:\n",
        "    - file_path: 読み込む pickle ファイルのパス\n",
        "    - verbose: データの詳細表示を行うかどうかのフラグ\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            # 初めに HyperParams オブジェクトを読み込む\n",
        "            hyper = pickle.load(f)\n",
        "\n",
        "            # ハイパーパラメータの概要表示\n",
        "            print(color_text(f\"Loaded HyperParams object:\", 'HEADER'))\n",
        "            print(f\"  Type: {type(hyper).__name__}\")\n",
        "            print(f\"  Attributes:\")\n",
        "            for attribute in dir(hyper):\n",
        "                if not attribute.startswith('__') and not callable(getattr(hyper, attribute)):\n",
        "                    value = getattr(hyper, attribute)\n",
        "                    print(f\"    {attribute}: {value} ({type(value).__name__})\")\n",
        "\n",
        "            # 次に，実験結果が続く場合\n",
        "            results = []\n",
        "            while True:\n",
        "                try:\n",
        "                    result = pickle.load(f)\n",
        "                    results.append(result)\n",
        "                except EOFError:\n",
        "                    break\n",
        "\n",
        "            # 実験結果の概要を表示\n",
        "            print(color_text(\"\\nExperiment results summary:\", 'OKCYAN'))\n",
        "            print(f\"  Number of results: {len(results)}\")\n",
        "            if len(results) > 0:\n",
        "                if isinstance(results[0], dict):\n",
        "                    print(\"  Sample keys from result dictionaries:\")\n",
        "                    sample_result = results[0]\n",
        "                    sample_keys = {}\n",
        "                    if isinstance(sample_result, dict):\n",
        "                        for key, value in sample_result.items():\n",
        "                            if isinstance(value, (list, dict)):\n",
        "                                entry_count = len(value)\n",
        "                                sample_keys[key] = (f\"{entry_count} entries\", type(value).__name__)\n",
        "                            else:\n",
        "                                sample_keys[key] = (f\"{value}\", type(value).__name__)\n",
        "                    print(f\"    Number of keys: {len(sample_keys)}\")\n",
        "                    for key, (entry_or_value, dtype) in sample_keys.items():\n",
        "                        print(f\"    {key}: {entry_or_value}, Type: {dtype}\")\n",
        "                else:\n",
        "                    print(f\"  Type of results: {type(results[0]).__name__}\")\n",
        "\n",
        "            # `regular` に含まれるキーとその数を表示\n",
        "            if len(results) > 0 and isinstance(results[0], dict):\n",
        "                regular_keys = set()\n",
        "                key_data_counts = {}\n",
        "\n",
        "                for result in results:\n",
        "                    if 'regular' in result and isinstance(result['regular'], dict):\n",
        "                        for key, value in result['regular'].items():\n",
        "                            if key not in key_data_counts:\n",
        "                                key_data_counts[key] = (0, None)\n",
        "                            count, dtype = key_data_counts[key]\n",
        "                            key_data_counts[key] = (count + 1, dtype if dtype else type(value).__name__)\n",
        "                            regular_keys.add(key)\n",
        "\n",
        "                print(color_text(f\"\\nKeys in 'regular' and their count:\", 'OKGREEN'))\n",
        "                print(f\"  Number of keys: {len(key_data_counts)}\")\n",
        "                for key, (count, dtype) in key_data_counts.items():\n",
        "                    print(f\"    {key}: {count} entries, Type: {dtype}\")\n",
        "\n",
        "                # 各キー (`dynamics`, `train`, `test`) の詳細を表示\n",
        "                for key in ['dynamics', 'train', 'test']:\n",
        "                    if key in regular_keys:\n",
        "                        if key == 'dynamics':\n",
        "                            for i, dynamics_list in enumerate([result['regular'][key] for result in results if 'regular' in result and key in result['regular']], 1):\n",
        "                                print(color_text(f\"\\nDetails of 'dynamics' list {i}:\", 'HEADER'))\n",
        "                                if len(dynamics_list) > 0 and isinstance(dynamics_list[0], dict):\n",
        "                                    entry_description = {subkey: type(value).__name__ for subkey, value in dynamics_list[0].items()}\n",
        "                                    print(f\"  Example entry data types in 'dynamics' list {i}: {entry_description}\")\n",
        "                                print(f\"  Number of entries in 'dynamics' list {i}: {len(dynamics_list)}\")\n",
        "                        else:\n",
        "                            key_data_counts = {}\n",
        "                            for result in results:\n",
        "                                if 'regular' in result and key in result['regular']:\n",
        "                                    if isinstance(result['regular'][key], dict):\n",
        "                                        for subkey, value in result['regular'][key].items():\n",
        "                                            if subkey not in key_data_counts:\n",
        "                                                key_data_counts[subkey] = (0, None)\n",
        "                                            count, dtype = key_data_counts[subkey]\n",
        "                                            key_data_counts[subkey] = (count + 1, dtype if dtype else type(value).__name__)\n",
        "\n",
        "                            print(color_text(f\"\\nKeys in 'regular[{key}]' and their count:\", 'OKGREEN'))\n",
        "                            print(f\"  Number of keys: {len(key_data_counts)}\")\n",
        "                            for subkey, (count, dtype) in key_data_counts.items():\n",
        "                                print(f\"    {subkey}: {count} entries, Type: {dtype}\")\n",
        "\n",
        "                # `dynamics` 内の `train` と `test` キーのサブキーを表示\n",
        "                if 'dynamics' in regular_keys:\n",
        "                    dynamics_train_keys = {}\n",
        "                    dynamics_test_keys = {}\n",
        "                    for result in results:\n",
        "                        if 'regular' in result and 'dynamics' in result['regular']:\n",
        "                            for entry in result['regular']['dynamics']:\n",
        "                                if isinstance(entry, dict):\n",
        "                                    if 'train' in entry and isinstance(entry['train'], dict):\n",
        "                                        for subkey, value in entry['train'].items():\n",
        "                                            if subkey not in dynamics_train_keys:\n",
        "                                                dynamics_train_keys[subkey] = (0, None)\n",
        "                                            count, dtype = dynamics_train_keys[subkey]\n",
        "                                            dynamics_train_keys[subkey] = (count + 1, dtype if dtype else type(value).__name__)\n",
        "                                    if 'test' in entry and isinstance(entry['test'], dict):\n",
        "                                        for subkey, value in entry['test'].items():\n",
        "                                            if subkey not in dynamics_test_keys:\n",
        "                                                dynamics_test_keys[subkey] = (0, None)\n",
        "                                            count, dtype = dynamics_test_keys[subkey]\n",
        "                                            dynamics_test_keys[subkey] = (count + 1, dtype if dtype else type(value).__name__)\n",
        "\n",
        "                    print(color_text(f\"\\nKeys in 'dynamics[train]' and their count:\", 'OKGREEN'))\n",
        "                    print(f\"  Number of keys: {len(dynamics_train_keys)}\")\n",
        "                    for subkey, (count, dtype) in dynamics_train_keys.items():\n",
        "                        print(f\"    {subkey}: {count} entries, Type: {dtype}\")\n",
        "\n",
        "                    print(color_text(f\"\\nKeys in 'dynamics[test]' and their count:\", 'OKGREEN'))\n",
        "                    print(f\"  Number of keys: {len(dynamics_test_keys)}\")\n",
        "                    for subkey, (count, dtype) in dynamics_test_keys.items():\n",
        "                        print(f\"    {subkey}: {count} entries, Type: {dtype}\")\n",
        "\n",
        "            # 詳細なデータを表示（verbose が True の場合のみ）\n",
        "            if verbose:\n",
        "                print(color_text(\"\\nLoaded experiment results:\", 'HEADER'))\n",
        "                for i, result in enumerate(results):\n",
        "                    print(color_text(f\"\\nResult {i+1}:\", 'HEADER'))\n",
        "                    if isinstance(result, dict):\n",
        "                        print(f\"  Type: dict\")\n",
        "                        print(f\"  Keys: {list(result.keys())}\")\n",
        "                        print(f\"  All Values:\")\n",
        "                        for key in result.keys():\n",
        "                            print(f\"    {key}: {result[key]}\")\n",
        "                    else:\n",
        "                        print(f\"  Type: {type(result).__name__}\")\n",
        "                        print(f\"  Content: {result}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the file: {e}\")\n",
        "\n",
        "# 使用例\n",
        "file_path = 'F10k3Lsp_h_init/F10k3Lsp_h_init.pickle'\n",
        "load_and_print_summary(file_path, verbose=False)  # 詳細表示を無効にする場合"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqrROxCU3v1m",
        "outputId": "c56e5c28-4b75-49f6-aeb5-d0cf1989dbd9"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[95mLoaded HyperParams object:\u001b[0m\n",
            "  Type: HyperParams\n",
            "  Attributes:\n",
            "    L: 2 (int)\n",
            "    alpha: 0.0001 (float)\n",
            "    arch: fc_softplus (str)\n",
            "    batch_seed: 0 (int)\n",
            "    bias: True (bool)\n",
            "    chunk: 100 (int)\n",
            "    data_seed: 0 (int)\n",
            "    delta_kernel: 0 (int)\n",
            "    device: cuda (str)\n",
            "    directory: F10k3Lsp_h_init (str)\n",
            "    dtype: float64 (str)\n",
            "    f0: 1 (int)\n",
            "    final_kernel: 0 (int)\n",
            "    h: 100 (int)\n",
            "    init_kernel: 0 (int)\n",
            "    init_seed: 0 (int)\n",
            "    k: 3 (int)\n",
            "    loss: softhinge (str)\n",
            "    lossbeta: 20 (int)\n",
            "    max_dgrad: 0.0001 (float)\n",
            "    max_dout: 0.1 (float)\n",
            "    n: 30 (int)\n",
            "    normalize: True (bool)\n",
            "    pickle: F10k3Lsp_h_init.pickle (str)\n",
            "    regular: 1 (int)\n",
            "    save_outputs: 0 (int)\n",
            "    spbeta: 5 (int)\n",
            "    store_kernel: 0 (int)\n",
            "    tau_alpha_crit: 1000.0 (float)\n",
            "    tau_over_h: 0.001 (float)\n",
            "    test_size: 400 (int)\n",
            "    train_size: 700 (int)\n",
            "    train_time: 18000 (int)\n",
            "\u001b[96m\n",
            "Experiment results summary:\u001b[0m\n",
            "  Number of results: 2\n",
            "  Sample keys from result dictionaries:\n",
            "    Number of keys: 3\n",
            "    hyper: <__main__.HyperParams object at 0x7cffaa08ada0>, Type: HyperParams\n",
            "    N: 13301, Type: int\n",
            "    regular: 3 entries, Type: dict\n",
            "\u001b[92m\n",
            "Keys in 'regular' and their count:\u001b[0m\n",
            "  Number of keys: 3\n",
            "    dynamics: 2 entries, Type: list\n",
            "    train: 2 entries, Type: dict\n",
            "    test: 2 entries, Type: dict\n",
            "\u001b[95m\n",
            "Details of 'dynamics' list 1:\u001b[0m\n",
            "  Example entry data types in 'dynamics' list 1: {'step': 'int', 'wall': 'float', 't': 'float', 'dt': 'float', 'dgrad': 'float', 'dout': 'float', 'norm': 'float', 'dnorm': 'float', 'grad_norm': 'float', 'wnorm': 'list', 'dwnorm': 'list', 'train': 'dict', 'test': 'dict'}\n",
            "  Number of entries in 'dynamics' list 1: 479\n",
            "\u001b[95m\n",
            "Details of 'dynamics' list 2:\u001b[0m\n",
            "  Example entry data types in 'dynamics' list 2: {'step': 'int', 'wall': 'float', 't': 'float', 'dt': 'float', 'dgrad': 'float', 'dout': 'float', 'norm': 'float', 'dnorm': 'float', 'grad_norm': 'float', 'wnorm': 'list', 'dwnorm': 'list', 'train': 'dict', 'test': 'dict'}\n",
            "  Number of entries in 'dynamics' list 2: 479\n",
            "\u001b[92m\n",
            "Keys in 'regular[train]' and their count:\u001b[0m\n",
            "  Number of keys: 3\n",
            "    f0: 2 entries, Type: Tensor\n",
            "    outputs: 2 entries, Type: Tensor\n",
            "    labels: 2 entries, Type: Tensor\n",
            "\u001b[92m\n",
            "Keys in 'regular[test]' and their count:\u001b[0m\n",
            "  Number of keys: 3\n",
            "    f0: 2 entries, Type: Tensor\n",
            "    outputs: 2 entries, Type: Tensor\n",
            "    labels: 2 entries, Type: Tensor\n",
            "\u001b[92m\n",
            "Keys in 'dynamics[train]' and their count:\u001b[0m\n",
            "  Number of keys: 8\n",
            "    loss: 958 entries, Type: float\n",
            "    aloss: 958 entries, Type: float\n",
            "    err: 958 entries, Type: float\n",
            "    nd: 958 entries, Type: int\n",
            "    dfnorm: 958 entries, Type: Tensor\n",
            "    fnorm: 958 entries, Type: Tensor\n",
            "    outputs: 958 entries, Type: NoneType\n",
            "    labels: 958 entries, Type: NoneType\n",
            "\u001b[92m\n",
            "Keys in 'dynamics[test]' and their count:\u001b[0m\n",
            "  Number of keys: 8\n",
            "    loss: 958 entries, Type: float\n",
            "    aloss: 958 entries, Type: float\n",
            "    err: 958 entries, Type: float\n",
            "    nd: 958 entries, Type: int\n",
            "    dfnorm: 958 entries, Type: Tensor\n",
            "    fnorm: 958 entries, Type: Tensor\n",
            "    outputs: 958 entries, Type: NoneType\n",
            "    labels: 958 entries, Type: NoneType\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def load_dynamics_data(file_path):\n",
        "    \"\"\"\n",
        "    Load dynamics data from a pickle file.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path: The path to the pickle file containing the data.\n",
        "\n",
        "    Returns:\n",
        "    - A list of dynamics data entries.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'rb') as f:\n",
        "        # Skip the HyperParams object\n",
        "        pickle.load(f)\n",
        "\n",
        "        # Read the remaining results\n",
        "        results = []\n",
        "        while True:\n",
        "            try:\n",
        "                result = pickle.load(f)\n",
        "                results.append(result)\n",
        "            except EOFError:\n",
        "                break\n",
        "\n",
        "    # Extract 'dynamics' from the results\n",
        "    for result in results:\n",
        "        if 'regular' in result and 'dynamics' in result['regular']:\n",
        "            return result['regular']['dynamics']\n",
        "\n",
        "    raise ValueError(\"No dynamics data found in the provided file.\")\n",
        "\n",
        "def plot_losses_and_errors(dynamics):\n",
        "    \"\"\"\n",
        "    Plot training and test losses and errors from dynamics data.\n",
        "\n",
        "    Parameters:\n",
        "    - dynamics: A list of dynamics data entries.\n",
        "    \"\"\"\n",
        "    if not dynamics:\n",
        "        print(\"No dynamics data available.\")\n",
        "        return\n",
        "\n",
        "    # Print the first and last entries\n",
        "    print(\"First entry:\", dynamics[0])\n",
        "    print(\"Last entry:\", dynamics[-1])\n",
        "\n",
        "    steps = []\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    train_errors = []\n",
        "    test_errors = []\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for entry in dynamics:\n",
        "        steps.append(entry['step'])\n",
        "\n",
        "        # Extract loss and error values\n",
        "        if 'train' in entry:\n",
        "            train_losses.append(entry['train'].get('loss', None))\n",
        "            train_errors.append(entry['train'].get('err', None))\n",
        "            # Calculate accuracy from error\n",
        "            train_accuracies.append(1 - entry['train'].get('err', 0))\n",
        "\n",
        "        if 'test' in entry:\n",
        "            test_losses.append(entry['test'].get('loss', None))\n",
        "            test_errors.append(entry['test'].get('err', None))\n",
        "            # Calculate accuracy from error\n",
        "            test_accuracies.append(1 - entry['test'].get('err', 0))\n",
        "\n",
        "    # Plot training and test losses\n",
        "    plt.figure(figsize=(8, 3))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(steps, train_losses, label='Train Loss', color='blue')\n",
        "    plt.plot(steps, test_losses, label='Test Loss', color='red')\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('Steps')\n",
        "    # plt.xlim(10 ** 2, np.max(steps))\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Test Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    print(f\"step len\", len(steps))\n",
        "\n",
        "    # Plot training and test accuracies\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(steps, train_accuracies, label='Train Accuracy', color='blue')\n",
        "    plt.plot(steps, test_accuracies, label='Test Accuracy', color='red')\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('Steps')\n",
        "    # plt.xlim(10 ** 2, np.max(steps))\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Test Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "file_path = 'F10k3Lsp_h_init/F10k3Lsp_h_init.pickle'\n",
        "dynamics_data = load_dynamics_data(file_path)\n",
        "plot_losses_and_errors(dynamics_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "pdNLh3umSL3u",
        "outputId": "f2dc9abb-4a70-48a7-a15d-a850151b0406"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First entry: {'step': 0, 'wall': 0.04397309300111374, 't': 0.1, 'dt': 0.1, 'dgrad': 4.5984031528972735e-07, 'dout': 1.9359676033938735e-07, 'norm': 115.27988388886278, 'dnorm': 0.003391136560863102, 'grad_norm': 0.05365238713061271, 'wnorm': [54.611653033120426, 101.09842643636655, 9.28047149571096], 'dwnorm': [0.001719496677742854, 0.0017362157918947938, 0.001483597927293187], 'train': {'loss': 9999.999831274337, 'aloss': 0.9999999831274338, 'err': 0.4275, 'nd': 400, 'dfnorm': tensor(0.0008), 'fnorm': tensor(0.6722), 'outputs': None, 'labels': None}, 'test': {'loss': 9999.999914374877, 'aloss': 0.9999999914374877, 'err': 0.44, 'nd': 400, 'dfnorm': tensor(0.0009), 'fnorm': tensor(0.6370), 'outputs': None, 'labels': None}}\n",
            "Last entry: {'step': 5071, 'wall': 86.38200499100094, 't': 328.13292968360673, 'dt': 0.04478806832704514, 'dgrad': 5.6975277481757904e-05, 'dout': 0.0008535093512889944, 'norm': 623.5565604715115, 'dnorm': 597.5151334708667, 'grad_norm': 1.1369009728517232, 'wnorm': [342.5815000862641, 366.5303127533323, 352.6765755750726], 'dwnorm': [327.2729449324227, 342.4755139744882, 346.2539729600019], 'train': {'loss': 6.721979269091887, 'aloss': 0.0006721979269091888, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(14305.7559), 'fnorm': tensor(14305.8171), 'outputs': None, 'labels': None}, 'test': {'loss': 700.378477644996, 'aloss': 0.0700378477644996, 'err': 0.0, 'nd': 118, 'dfnorm': tensor(12508.6158), 'fnorm': tensor(12508.6758), 'outputs': None, 'labels': None}}\n",
            "step len 479\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def load_dynamics_data(file_path):\n",
        "    \"\"\"\n",
        "    Load dynamics data from a pickle file.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path: The path to the pickle file containing the data.\n",
        "\n",
        "    Returns:\n",
        "    - A list of dynamics data entries.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'rb') as f:\n",
        "        # Skip the HyperParams object\n",
        "        pickle.load(f)\n",
        "\n",
        "        # Read the remaining results\n",
        "        results = []\n",
        "        while True:\n",
        "            try:\n",
        "                result = pickle.load(f)\n",
        "                results.append(result)\n",
        "            except EOFError:\n",
        "                break\n",
        "\n",
        "    # Extract 'dynamics' from the results\n",
        "    for result in results:\n",
        "        if 'regular' in result and 'dynamics' in result['regular']:\n",
        "            return result['regular']['dynamics']\n",
        "\n",
        "    raise ValueError(\"No dynamics data found in the provided file.\")\n",
        "\n",
        "def plot_losses_and_errors(dynamics):\n",
        "    \"\"\"\n",
        "    Plot training and test losses and errors from dynamics data.\n",
        "\n",
        "    Parameters:\n",
        "    - dynamics: A list of dynamics data entries.\n",
        "    \"\"\"\n",
        "    times = []\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    train_errors = []\n",
        "    test_errors = []\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for entry in dynamics:\n",
        "        times.append(entry['t'])\n",
        "\n",
        "        # Extract loss and error values\n",
        "        if 'train' in entry:\n",
        "            train_losses.append(entry['train'].get('loss', None))\n",
        "            train_errors.append(entry['train'].get('err', None))\n",
        "            # Calculate accuracy from error\n",
        "            train_accuracies.append(1 - entry['train'].get('err', 0))\n",
        "\n",
        "        if 'test' in entry:\n",
        "            test_losses.append(entry['test'].get('loss', None))\n",
        "            test_errors.append(entry['test'].get('err', None))\n",
        "            # Calculate accuracy from error\n",
        "            test_accuracies.append(1 - entry['test'].get('err', 0))\n",
        "\n",
        "    # Plot training and test losses\n",
        "    plt.figure(figsize=(8, 3))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.semilogx(times, train_losses, label='Train Loss', color='blue')\n",
        "    plt.semilogx(times, test_losses, label='Test Loss', color='red')\n",
        "    plt.xlabel('Time (t)')\n",
        "    # plt.xlim(10 ** 2, np.max(times))\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Test Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    print(f\"t\", len(times))\n",
        "    print(f\"t max\", np.max(times))\n",
        "\n",
        "    # Plot training and test accuracies\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.semilogx(times, train_accuracies, label='Train Accuracy', color='blue')\n",
        "    plt.semilogx(times, test_accuracies, label='Test Accuracy', color='red')\n",
        "    plt.xlabel('Time (t)')\n",
        "    # plt.xlim(10 ** 2, np.max(times))\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Test Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "file_path = 'F10k3Lsp_h_init/F10k3Lsp_h_init.pickle'\n",
        "dynamics_data = load_dynamics_data(file_path)\n",
        "plot_losses_and_errors(dynamics_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "zUmtWbsSOxN6",
        "outputId": "3581b44c-7723-4a92-dffa-7023b79aff5d"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t 479\n",
            "t max 328.13292968360673\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# メイン (Adam)"
      ],
      "metadata": {
        "id": "bNabgUuFqfUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "class SplitEval(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    モデルを分割して評価するためのクラス．入力データを指定されたサイズに分割し，それぞれの分割に対してモデルを評価．\n",
        "    最後に，評価結果を結合して返す．\n",
        "\n",
        "    Attributes:\n",
        "    - f: 評価対象のモデル\n",
        "    - size: 入力データを分割するサイズ\n",
        "    \"\"\"\n",
        "    def __init__(self, f, size):\n",
        "        \"\"\"\n",
        "        コンストラクタ．モデルと分割サイズを初期化\n",
        "\n",
        "        Parameters:\n",
        "        - f: 評価対象のモデル\n",
        "        - size: 入力データを分割するサイズ\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.f = f    # モデルを保存\n",
        "        self.size = size    # 分割サイズを保存\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        フォワードパス．入力データを指定されたサイズに分割し，各分割に対してモデルを適用し，結果を結合して返す．\n",
        "\n",
        "        Parameters:\n",
        "        - x: 入力データ（バッチ）\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: 各分割に対してモデルを適用した結果を結合したテンソル\n",
        "        \"\"\"\n",
        "        return torch.cat([self.f(x[i: i + self.size]) for i in range(0, len(x), self.size)])\n",
        "\n",
        "def hinge(out, y, alpha):\n",
        "    return (1 - alpha * out * y).relu().mean() / alpha\n",
        "\n",
        "def quad_hinge(out, y, alpha):\n",
        "    return 0.5 * (1 - alpha * out * y).relu().pow(2).mean() / alpha ** 2\n",
        "\n",
        "def mse(out, y, alpha):\n",
        "    return 0.5 * (1.1 - alpha * out * y).pow(2).mean() / alpha ** 2\n",
        "\n",
        "def softhinge(out, y, alpha, beta):\n",
        "    sp = partial(torch.nn.functional.softplus, beta=beta)\n",
        "    return sp(1 - alpha * out * y).mean() / alpha\n",
        "\n",
        "def run_regular(hyper, f0, loss, xtr, ytr, xte, yte):\n",
        "    \"\"\"\n",
        "    通常のトレーニングプロセスを実行し，トレーニングとテストの結果を収集する関数\n",
        "\n",
        "    Parameters:\n",
        "    - hyper: 設定やパラメータが格納されたオブジェクト\n",
        "    - f0: 初期モデル関数\n",
        "    - xtr: トレーニングデータの入力\n",
        "    - ytr: トレーニングデータのラベル\n",
        "    - xte: テストデータの入力\n",
        "    - yte: テストデータのラベル\n",
        "\n",
        "    Yields:\n",
        "    - f: トレーニング後のモデル関数\n",
        "    - out: トレーニングとテストの結果を含む辞書\n",
        "    \"\"\"\n",
        "\n",
        "    # 勾配計算を無効にし，初期モデルによる出力を計算\n",
        "    with torch.no_grad():\n",
        "        otr0 = f0(xtr)\n",
        "        ote0 = f0(xte)\n",
        "\n",
        "\n",
        "    f = copy.deepcopy(f0)\n",
        "    optimizer = torch.optim.Adam(f.parameters(), hyper.lr)\n",
        "\n",
        "    # トレーニングのダイナミクスを保存するリスト\n",
        "    dynamics = []\n",
        "    checkpoint_generator = loglinspace(0.1, 1000)\n",
        "    checkpoint = next(checkpoint_generator)\n",
        "    wall = perf_counter()\n",
        "\n",
        "    for step in itertools.count():\n",
        "\n",
        "        batch = torch.randperm(len(xtr))[:hyper.bs]\n",
        "        xb = xtr[batch]\n",
        "\n",
        "        loss_value = loss(f(xb) - otr0[batch], ytr[batch], hyper.alpha)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        save = False\n",
        "\n",
        "        if step == checkpoint:\n",
        "            checkpoint = next(checkpoint_generator)\n",
        "            assert checkpoint > step\n",
        "            save = True\n",
        "\n",
        "        if save:\n",
        "            assert len(xtr) < len(xte)\n",
        "            j = torch.randperm(len(xtr))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                otr = f(xtr[j]) - otr0[j]\n",
        "                ote = f(xte[j]) - ote0[j]\n",
        "\n",
        "            state = {\n",
        "                'step' : step,\n",
        "                'wall': perf_counter() - wall,\n",
        "                'batch_loss': loss_value.item(),\n",
        "                'norm': sum(p.norm().pow(2) for p in f.parameters()).sqrt().item(),\n",
        "                'dnorm': sum((p0 - p).norm().pow(2) for p0, p in zip(f0.parameters(), f.parameters())).sqrt().item(),\n",
        "                'train' : {\n",
        "                    'loss': loss(otr, ytr[j], hyper.alpha).item(),\n",
        "                    'aloss': hyper.alpha * loss(otr, ytr[j], hyper.alpha).item(),\n",
        "                    'aaloss': hyper.alpha ** 2 * loss(otr, ytr[j], hyper.alpha).item(),\n",
        "                    'err': (otr * ytr[j] <= 0).double().mean().item(),\n",
        "                    'nd': (hyper.alpha * otr * ytr[j] < 1).long().sum().item(),\n",
        "                    'dfnorm': otr.pow(2).mean().sqrt(),\n",
        "                    'fnorm': (otr + otr0[j]).pow(2).mean().sqrt(),\n",
        "                },\n",
        "                'test' : {\n",
        "                    'loss': loss(ote, yte[j], hyper.alpha).item(),\n",
        "                    'aloss': hyper.alpha * loss(ote, yte[j], hyper.alpha).item(),\n",
        "                    'aaloss': hyper.alpha ** 2 * loss(ote, yte[j], hyper.alpha).item(),\n",
        "                    'err': (ote * yte[j] <= 0).double().mean().item(),\n",
        "                    'nd': (hyper.alpha * ote * yte[j] < 1).long().sum().item(),\n",
        "                    'dfnorm': ote.pow(2).mean().sqrt(),\n",
        "                    'fnorm': (ote + ote0[j]).pow(2).mean().sqrt(),\n",
        "                },\n",
        "            }\n",
        "\n",
        "            if hyper.arch.split('_')[0] == 'fc':\n",
        "                def getw(f, i):\n",
        "                    return torch.cat(list(getattr(f.f, \"W{}\".format(i))))\n",
        "                state['wnorm'] = [getw(f, i).norm().item() for i in range(f.f.L + 1)]\n",
        "                state['dwnorm'] = [(getw(f, i) - getw(f0, i)).norm().item() for i in range(f.f.L + 1)]\n",
        "\n",
        "            print(\"[i={d[step]:d} wall={d[wall]:.0f}] [train aL={d[train][aloss]:.2e} err={d[train][err]:.2f} nd={d[train][nd]}/{p}] [test aL={d[test][aloss]:.2e} err={d[test][err]:.2f}]\".format(d=state, p=len(j)), flush=True)\n",
        "\n",
        "            dynamics.append(state)\n",
        "\n",
        "            if state['test']['nd'] == 0:\n",
        "                break\n",
        "\n",
        "        if perf_counter() > wall + hyper.train_time:\n",
        "            break\n",
        "\n",
        "    with torch.no_grad():\n",
        "        otr = f(xtr) - otr0\n",
        "        ote = f(xte) - ote0\n",
        "\n",
        "    out = {\n",
        "        'dynamics': dynamics,\n",
        "        'train': {\n",
        "            'f0': otr0,\n",
        "            'outputs': otr,\n",
        "            'labels': ytr,\n",
        "        },\n",
        "        'test': {\n",
        "            'f0': ote0,\n",
        "            'outputs': ote,\n",
        "            'labels': yte,\n",
        "        }\n",
        "    }\n",
        "    return f, out\n",
        "\n",
        "\n",
        "def run_exp(hyper, f0, xtr, ytr, xte, yte):\n",
        "    \"\"\"\n",
        "    実験を実行し，トレーニングとテストの結果を収集する関数\n",
        "\n",
        "    Parameters:\n",
        "    - hyper: 設定やパラメータが格納されたオブジェクト\n",
        "    - f0: 初期モデル関数\n",
        "    - xtr: トレーニングデータの入力\n",
        "    - ytr: トレーニングデータのラベル\n",
        "    - xte: テストデータの入力\n",
        "    - yte: テストデータのラベル\n",
        "\n",
        "    Yields:\n",
        "    - run: 実験の結果を含む辞書\n",
        "    \"\"\"\n",
        "    run = {\n",
        "        'hyper': hyper,\n",
        "        'N': sum(p.numel() for p in f0.parameters()),  # モデルのパラメータの総数\n",
        "    }\n",
        "\n",
        "    if hyper.loss == 'hinge':\n",
        "        loss = hinge\n",
        "    elif hyper.loss == 'quad_hinge':\n",
        "        loss = quad_hinge\n",
        "    elif hyper.loss == 'mse':\n",
        "        loss = mse\n",
        "    elif hyper.loss == 'softhinge':\n",
        "        loss = partial(softhinge, beta=hyper.lossbeta)\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid loss function: {hyper.loss}\")\n",
        "\n",
        "    _f, out = run_regular(hyper, f0, loss, xtr, ytr, xte, yte)\n",
        "    run['regular'] = out\n",
        "\n",
        "    yield run\n",
        "\n",
        "def execute(hyper):\n",
        "    \"\"\"\n",
        "    実験を実行するためのメイン関数．データセットの準備，モデルの設定，実験の実行\n",
        "\n",
        "    Parameters:\n",
        "    - hyper: 実験の設定を含むオブジェクト\n",
        "    \"\"\"\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        if hyper.dtype == 'float64':\n",
        "            torch.set_default_dtype(torch.float64)\n",
        "        elif hyper.dtype == 'float32':\n",
        "            torch.set_default_dtype(torch.float32)\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"CUDA is not available. Running on CPU.\")\n",
        "        if hyper.dtype == 'float64':\n",
        "            torch.set_default_dtype(torch.float64)\n",
        "        elif hyper.dtype == 'float32':\n",
        "            torch.set_default_dtype(torch.float32)\n",
        "\n",
        "    (xtr, ytr), (xte, yte) = get_binary_dataset(hyper.n, hyper.k, hyper.train_size, hyper.test_size, hyper.data_seed, hyper.normalize, device)\n",
        "    xtr = xtr.type(torch.get_default_dtype())\n",
        "    xte = xte.type(torch.get_default_dtype())\n",
        "    ytr = ytr.type(torch.get_default_dtype())\n",
        "    yte = yte.type(torch.get_default_dtype())\n",
        "\n",
        "    torch.manual_seed(hyper.init_seed + hash(hyper.alpha))\n",
        "\n",
        "    arch, act = hyper.arch.split('_')\n",
        "    if act == 'relu':\n",
        "        act = lambda x: 2 ** 0.5 * torch.relu(x)\n",
        "    elif act == 'tanh':\n",
        "        act = torch.tanh\n",
        "    elif act == 'softplus':\n",
        "        factor = torch.nn.functional.softplus(torch.randn(100000, dtype=torch.float64), hyper.spbeta).pow(2).mean().rsqrt().item()\n",
        "        act = lambda x: torch.nn.functional.softplus(x, beta=hyper.spbeta).mul(factor)\n",
        "    else:\n",
        "        raise ValueError('act not specified')\n",
        "\n",
        "    if arch == 'fc':\n",
        "        assert hyper.L is not None\n",
        "        xtr = xtr.flatten(1)\n",
        "        xte = xte.flatten(1)\n",
        "        f = FC(xtr.size(1), hyper.h, hyper.L, act, hyper.bias).to(device)\n",
        "    else:\n",
        "        raise ValueError('arch not specified')\n",
        "\n",
        "    f = SplitEval(f, hyper.chunk)\n",
        "\n",
        "    torch.manual_seed(hyper.batch_seed)\n",
        "\n",
        "    for run in run_exp(hyper, f, xtr, ytr, xte, yte):\n",
        "        yield run\n",
        "\n",
        "class HyperParams:\n",
        "    def __init__(self):\n",
        "        self.device = 'cuda'\n",
        "        self.dtype = 'float64'\n",
        "        self.init_seed = 0\n",
        "        self.batch_seed = 0\n",
        "        self.n = 30\n",
        "        self.k = 3\n",
        "        self.train_size = 800\n",
        "        self.test_size = 900\n",
        "        self.normalize = True\n",
        "        self.data_seed = 0\n",
        "        self.alpha = 1e-4   # [例：1e-4]\n",
        "        self.f0 = 1\n",
        "        self.tau_over_h = 1e-3\n",
        "        self.tau_alpha_crit = 1e3\n",
        "        self.L = 3\n",
        "        self.h = 100\n",
        "        self.arch = 'fc_softplus'\n",
        "        self.spbeta = 5\n",
        "        self.bs = 32\n",
        "        self.lr = 1e-3\n",
        "        self.bias = True\n",
        "        self.max_dgrad = 1e-4\n",
        "        self.max_dout = 0.1\n",
        "        self.loss = 'hinge'\n",
        "        self.lossbeta = 20\n",
        "        self.train_time = 18000\n",
        "        self.chunk = 100\n",
        "        self.init_kernel = 0\n",
        "        self.delta_kernel = 0\n",
        "        self.final_kernel = 0\n",
        "        self.store_kernel = 0\n",
        "        self.save_outputs = 0\n",
        "        self.regular = 1\n",
        "        self.directory = 'C10k3Lsp_adam'\n",
        "        self.pickle = 'C10k3Lsp_adam.pickle'\n",
        "\n",
        "# 実験の実行と結果を保存する関数\n",
        "import pickle\n",
        "\n",
        "def run_and_save_experiment(hyper):\n",
        "    # ディレクトリの作成\n",
        "    if not os.path.exists(hyper.directory):\n",
        "        os.makedirs(hyper.directory)\n",
        "\n",
        "    # pickle ファイルのパスをディレクトリに基づいて変更\n",
        "    pickle_path = os.path.join(hyper.directory, hyper.pickle)\n",
        "\n",
        "    # test_size または chunk が None の場合は train_size の値で初期化\n",
        "    if hyper.test_size is None:\n",
        "        hyper.test_size = hyper.train_size\n",
        "\n",
        "    if hyper.chunk is None:\n",
        "        hyper.chunk = hyper.train_size\n",
        "\n",
        "    try:\n",
        "        # 引数を pickle ファイルに保存\n",
        "        with open(pickle_path, 'wb') as f:\n",
        "            pickle.dump(hyper, f)\n",
        "\n",
        "        with open(pickle_path, 'ab') as f:\n",
        "            for res in execute(hyper):\n",
        "                # 結果を pickle ファイルに追加で保存\n",
        "                pickle.dump(res, f)\n",
        "    except Exception as e:\n",
        "        if os.path.exists(pickle_path):\n",
        "            os.remove(pickle_path)\n",
        "        print(f\"An error occurred during saving: {e}\")\n",
        "        raise e\n",
        "\n",
        "######## 実験の実行と結果の保存\n",
        "hyper = HyperParams()\n",
        "run_and_save_experiment(hyper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "422-Gj9ij4pI",
        "outputId": "bf356eab-e7ad-471f-c5da-c5d0ec9a2ead"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available. Running on CPU.\n",
            "[i=0 wall=0] [train aL=6.05e+03 err=0.47 nd=800/800] [test aL=6.05e+03 err=0.52]\n",
            "[i=1 wall=0] [train aL=6.05e+03 err=0.47 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=2 wall=0] [train aL=6.05e+03 err=0.48 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=3 wall=0] [train aL=6.05e+03 err=0.46 nd=800/800] [test aL=6.05e+03 err=0.49]\n",
            "[i=4 wall=0] [train aL=6.05e+03 err=0.46 nd=800/800] [test aL=6.05e+03 err=0.49]\n",
            "[i=5 wall=0] [train aL=6.05e+03 err=0.46 nd=800/800] [test aL=6.05e+03 err=0.50]\n",
            "[i=6 wall=0] [train aL=6.05e+03 err=0.46 nd=800/800] [test aL=6.05e+03 err=0.50]\n",
            "[i=7 wall=0] [train aL=6.05e+03 err=0.47 nd=800/800] [test aL=6.05e+03 err=0.50]\n",
            "[i=8 wall=0] [train aL=6.05e+03 err=0.47 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=9 wall=0] [train aL=6.05e+03 err=0.47 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=10 wall=0] [train aL=6.05e+03 err=0.48 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=11 wall=0] [train aL=6.05e+03 err=0.48 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=13 wall=0] [train aL=6.05e+03 err=0.48 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=15 wall=0] [train aL=6.05e+03 err=0.48 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=17 wall=0] [train aL=6.05e+03 err=0.49 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=19 wall=0] [train aL=6.05e+03 err=0.48 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=21 wall=0] [train aL=6.05e+03 err=0.48 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=24 wall=1] [train aL=6.05e+03 err=0.48 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=27 wall=1] [train aL=6.05e+03 err=0.48 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=30 wall=1] [train aL=6.05e+03 err=0.49 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=33 wall=1] [train aL=6.05e+03 err=0.49 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=37 wall=1] [train aL=6.05e+03 err=0.49 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=41 wall=1] [train aL=6.05e+03 err=0.48 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=46 wall=1] [train aL=6.05e+03 err=0.48 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=51 wall=1] [train aL=6.05e+03 err=0.48 nd=800/800] [test aL=6.05e+03 err=0.50]\n",
            "[i=57 wall=1] [train aL=6.05e+03 err=0.48 nd=800/800] [test aL=6.05e+03 err=0.50]\n",
            "[i=63 wall=1] [train aL=6.05e+03 err=0.48 nd=800/800] [test aL=6.05e+03 err=0.50]\n",
            "[i=70 wall=1] [train aL=6.05e+03 err=0.48 nd=800/800] [test aL=6.05e+03 err=0.50]\n",
            "[i=77 wall=1] [train aL=6.05e+03 err=0.48 nd=800/800] [test aL=6.05e+03 err=0.50]\n",
            "[i=85 wall=1] [train aL=6.05e+03 err=0.48 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=94 wall=1] [train aL=6.05e+03 err=0.48 nd=800/800] [test aL=6.05e+03 err=0.50]\n",
            "[i=104 wall=1] [train aL=6.05e+03 err=0.48 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=115 wall=1] [train aL=6.05e+03 err=0.49 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=127 wall=1] [train aL=6.05e+03 err=0.49 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=140 wall=1] [train aL=6.05e+03 err=0.49 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=154 wall=1] [train aL=6.05e+03 err=0.48 nd=800/800] [test aL=6.05e+03 err=0.51]\n",
            "[i=170 wall=1] [train aL=6.05e+03 err=0.47 nd=800/800] [test aL=6.05e+03 err=0.50]\n",
            "[i=187 wall=1] [train aL=6.05e+03 err=0.41 nd=800/800] [test aL=6.05e+03 err=0.49]\n",
            "[i=206 wall=2] [train aL=6.05e+03 err=0.39 nd=800/800] [test aL=6.05e+03 err=0.49]\n",
            "[i=227 wall=2] [train aL=6.05e+03 err=0.36 nd=800/800] [test aL=6.05e+03 err=0.49]\n",
            "[i=250 wall=2] [train aL=6.05e+03 err=0.36 nd=800/800] [test aL=6.05e+03 err=0.48]\n",
            "[i=275 wall=2] [train aL=6.05e+03 err=0.38 nd=800/800] [test aL=6.05e+03 err=0.48]\n",
            "[i=303 wall=2] [train aL=6.05e+03 err=0.41 nd=800/800] [test aL=6.05e+03 err=0.49]\n",
            "[i=333 wall=2] [train aL=6.05e+03 err=0.41 nd=800/800] [test aL=6.05e+03 err=0.49]\n",
            "[i=366 wall=2] [train aL=6.05e+03 err=0.38 nd=800/800] [test aL=6.05e+03 err=0.49]\n",
            "[i=402 wall=2] [train aL=6.05e+03 err=0.37 nd=800/800] [test aL=6.05e+03 err=0.49]\n",
            "[i=442 wall=3] [train aL=6.05e+03 err=0.39 nd=800/800] [test aL=6.05e+03 err=0.49]\n",
            "[i=486 wall=3] [train aL=6.05e+03 err=0.39 nd=800/800] [test aL=6.05e+03 err=0.50]\n",
            "[i=534 wall=3] [train aL=6.05e+03 err=0.41 nd=800/800] [test aL=6.05e+03 err=0.49]\n",
            "[i=586 wall=3] [train aL=6.05e+03 err=0.41 nd=800/800] [test aL=6.05e+03 err=0.49]\n",
            "[i=643 wall=4] [train aL=6.05e+03 err=0.41 nd=800/800] [test aL=6.05e+03 err=0.49]\n",
            "[i=706 wall=4] [train aL=6.04e+03 err=0.41 nd=800/800] [test aL=6.05e+03 err=0.50]\n",
            "[i=775 wall=4] [train aL=6.04e+03 err=0.42 nd=800/800] [test aL=6.05e+03 err=0.50]\n",
            "[i=850 wall=5] [train aL=6.03e+03 err=0.41 nd=800/800] [test aL=6.05e+03 err=0.50]\n",
            "[i=932 wall=5] [train aL=6.02e+03 err=0.39 nd=800/800] [test aL=6.05e+03 err=0.49]\n",
            "[i=1021 wall=5] [train aL=6.00e+03 err=0.38 nd=800/800] [test aL=6.05e+03 err=0.49]\n",
            "[i=1119 wall=6] [train aL=5.97e+03 err=0.38 nd=800/800] [test aL=6.05e+03 err=0.48]\n",
            "[i=1225 wall=6] [train aL=5.93e+03 err=0.39 nd=800/800] [test aL=6.05e+03 err=0.49]\n",
            "[i=1341 wall=6] [train aL=5.88e+03 err=0.39 nd=800/800] [test aL=6.06e+03 err=0.49]\n",
            "[i=1467 wall=7] [train aL=5.82e+03 err=0.39 nd=800/800] [test aL=6.08e+03 err=0.48]\n",
            "[i=1604 wall=7] [train aL=5.76e+03 err=0.38 nd=800/800] [test aL=6.09e+03 err=0.49]\n",
            "[i=1753 wall=8] [train aL=5.68e+03 err=0.37 nd=800/800] [test aL=6.13e+03 err=0.48]\n",
            "[i=1914 wall=8] [train aL=5.61e+03 err=0.36 nd=800/800] [test aL=6.17e+03 err=0.49]\n",
            "[i=2089 wall=9] [train aL=5.55e+03 err=0.36 nd=800/800] [test aL=6.24e+03 err=0.49]\n",
            "[i=2278 wall=9] [train aL=5.49e+03 err=0.35 nd=800/800] [test aL=6.28e+03 err=0.49]\n",
            "[i=2482 wall=10] [train aL=5.44e+03 err=0.35 nd=800/800] [test aL=6.36e+03 err=0.48]\n",
            "[i=2702 wall=11] [train aL=5.38e+03 err=0.34 nd=800/800] [test aL=6.38e+03 err=0.48]\n",
            "[i=2939 wall=11] [train aL=5.33e+03 err=0.33 nd=800/800] [test aL=6.41e+03 err=0.48]\n",
            "[i=3194 wall=12] [train aL=5.27e+03 err=0.32 nd=800/800] [test aL=6.46e+03 err=0.47]\n",
            "[i=3468 wall=13] [train aL=5.20e+03 err=0.31 nd=800/800] [test aL=6.47e+03 err=0.48]\n",
            "[i=3762 wall=14] [train aL=5.12e+03 err=0.30 nd=800/800] [test aL=6.55e+03 err=0.48]\n",
            "[i=4076 wall=15] [train aL=5.03e+03 err=0.30 nd=800/800] [test aL=6.57e+03 err=0.48]\n",
            "[i=4411 wall=16] [train aL=4.91e+03 err=0.29 nd=800/800] [test aL=6.63e+03 err=0.48]\n",
            "[i=4768 wall=17] [train aL=4.78e+03 err=0.29 nd=797/800] [test aL=6.69e+03 err=0.48]\n",
            "[i=5148 wall=19] [train aL=4.61e+03 err=0.28 nd=797/800] [test aL=6.75e+03 err=0.48]\n",
            "[i=5551 wall=20] [train aL=4.40e+03 err=0.25 nd=794/800] [test aL=6.83e+03 err=0.49]\n",
            "[i=5977 wall=21] [train aL=4.14e+03 err=0.22 nd=792/800] [test aL=6.89e+03 err=0.47]\n",
            "[i=6427 wall=22] [train aL=3.83e+03 err=0.20 nd=782/800] [test aL=7.00e+03 err=0.47]\n",
            "[i=6902 wall=24] [train aL=3.49e+03 err=0.17 nd=773/800] [test aL=7.18e+03 err=0.48]\n",
            "[i=7401 wall=25] [train aL=3.15e+03 err=0.16 nd=746/800] [test aL=7.35e+03 err=0.47]\n",
            "[i=7924 wall=26] [train aL=2.85e+03 err=0.14 nd=709/800] [test aL=7.55e+03 err=0.47]\n",
            "[i=8472 wall=28] [train aL=2.56e+03 err=0.13 nd=672/800] [test aL=7.81e+03 err=0.48]\n",
            "[i=9044 wall=30] [train aL=2.30e+03 err=0.11 nd=643/800] [test aL=7.99e+03 err=0.46]\n",
            "[i=9640 wall=32] [train aL=2.04e+03 err=0.08 nd=606/800] [test aL=8.13e+03 err=0.47]\n",
            "[i=10259 wall=34] [train aL=1.81e+03 err=0.07 nd=581/800] [test aL=8.29e+03 err=0.46]\n",
            "[i=10901 wall=36] [train aL=1.56e+03 err=0.05 nd=546/800] [test aL=8.32e+03 err=0.46]\n",
            "[i=11565 wall=38] [train aL=1.33e+03 err=0.04 nd=521/800] [test aL=8.35e+03 err=0.46]\n",
            "[i=12251 wall=40] [train aL=1.11e+03 err=0.03 nd=497/800] [test aL=8.33e+03 err=0.45]\n",
            "[i=12958 wall=42] [train aL=9.22e+02 err=0.02 nd=484/800] [test aL=8.38e+03 err=0.44]\n",
            "[i=13685 wall=44] [train aL=7.52e+02 err=0.01 nd=477/800] [test aL=8.26e+03 err=0.43]\n",
            "[i=14431 wall=47] [train aL=6.00e+02 err=0.00 nd=451/800] [test aL=8.15e+03 err=0.42]\n",
            "[i=15195 wall=49] [train aL=4.88e+02 err=0.00 nd=430/800] [test aL=8.02e+03 err=0.41]\n",
            "[i=15977 wall=51] [train aL=3.69e+02 err=0.00 nd=404/800] [test aL=7.89e+03 err=0.41]\n",
            "[i=16775 wall=53] [train aL=2.80e+02 err=0.00 nd=387/800] [test aL=7.66e+03 err=0.40]\n",
            "[i=17589 wall=56] [train aL=2.15e+02 err=0.00 nd=341/800] [test aL=7.49e+03 err=0.39]\n",
            "[i=18417 wall=59] [train aL=1.61e+02 err=0.00 nd=321/800] [test aL=7.19e+03 err=0.38]\n",
            "[i=19259 wall=61] [train aL=1.19e+02 err=0.00 nd=265/800] [test aL=6.95e+03 err=0.37]\n",
            "[i=20114 wall=64] [train aL=8.47e+01 err=0.00 nd=224/800] [test aL=6.67e+03 err=0.35]\n",
            "[i=20981 wall=66] [train aL=5.99e+01 err=0.00 nd=176/800] [test aL=6.39e+03 err=0.35]\n",
            "[i=21859 wall=69] [train aL=4.14e+01 err=0.00 nd=103/800] [test aL=6.15e+03 err=0.34]\n",
            "[i=22747 wall=72] [train aL=2.77e+01 err=0.00 nd=60/800] [test aL=5.93e+03 err=0.33]\n",
            "[i=23645 wall=74] [train aL=1.85e+01 err=0.00 nd=30/800] [test aL=5.71e+03 err=0.33]\n",
            "[i=24552 wall=77] [train aL=1.15e+01 err=0.00 nd=15/800] [test aL=5.54e+03 err=0.32]\n",
            "[i=25467 wall=80] [train aL=6.89e+00 err=0.00 nd=3/800] [test aL=5.41e+03 err=0.31]\n",
            "[i=26389 wall=83] [train aL=3.87e+00 err=0.00 nd=1/800] [test aL=5.31e+03 err=0.31]\n",
            "[i=27318 wall=86] [train aL=2.06e+00 err=0.00 nd=1/800] [test aL=5.22e+03 err=0.30]\n",
            "[i=28253 wall=89] [train aL=1.04e+00 err=0.00 nd=1/800] [test aL=5.16e+03 err=0.30]\n",
            "[i=29194 wall=91] [train aL=4.70e-01 err=0.00 nd=0/800] [test aL=5.12e+03 err=0.29]\n",
            "[i=30141 wall=95] [train aL=2.74e-01 err=0.00 nd=0/800] [test aL=5.09e+03 err=0.29]\n",
            "[i=31092 wall=98] [train aL=8.91e-02 err=0.00 nd=0/800] [test aL=5.07e+03 err=0.29]\n",
            "[i=32048 wall=101] [train aL=2.85e-02 err=0.00 nd=0/800] [test aL=5.06e+03 err=0.29]\n",
            "[i=33008 wall=105] [train aL=3.64e-02 err=0.00 nd=0/800] [test aL=5.06e+03 err=0.29]\n",
            "[i=33972 wall=108] [train aL=3.99e-03 err=0.00 nd=0/800] [test aL=5.06e+03 err=0.29]\n",
            "[i=34939 wall=111] [train aL=1.93e-02 err=0.00 nd=0/800] [test aL=5.06e+03 err=0.29]\n",
            "[i=35909 wall=114] [train aL=2.80e-02 err=0.00 nd=0/800] [test aL=5.05e+03 err=0.29]\n",
            "[i=36882 wall=117] [train aL=2.97e-02 err=0.00 nd=0/800] [test aL=5.05e+03 err=0.29]\n",
            "[i=37857 wall=120] [train aL=1.76e-02 err=0.00 nd=0/800] [test aL=5.05e+03 err=0.29]\n",
            "[i=38835 wall=124] [train aL=1.22e-01 err=0.00 nd=0/800] [test aL=5.04e+03 err=0.29]\n",
            "[i=39815 wall=127] [train aL=5.10e-02 err=0.00 nd=0/800] [test aL=5.04e+03 err=0.29]\n",
            "[i=40797 wall=130] [train aL=2.87e-02 err=0.00 nd=0/800] [test aL=5.03e+03 err=0.29]\n",
            "[i=41781 wall=132] [train aL=6.35e-02 err=0.00 nd=0/800] [test aL=5.03e+03 err=0.29]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-219-4efad237e79e>\u001b[0m in \u001b[0;36m<cell line: 336>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;31m######## 実験の実行と結果の保存\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0mhyper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHyperParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m \u001b[0mrun_and_save_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-219-4efad237e79e>\u001b[0m in \u001b[0;36mrun_and_save_experiment\u001b[0;34m(hyper)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ab'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m                 \u001b[0;31m# 結果を pickle ファイルに追加で保存\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-219-4efad237e79e>\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(hyper)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-219-4efad237e79e>\u001b[0m in \u001b[0;36mrun_exp\u001b[0;34m(hyper, f0, xtr, ytr, xte, yte)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid loss function: {hyper.loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0m_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_regular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0mrun\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'regular'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-219-4efad237e79e>\u001b[0m in \u001b[0;36mrun_regular\u001b[0;34m(hyper, f0, loss, xtr, ytr, xte, yte)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxtr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0motr0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-219-4efad237e79e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m各分割に対してモデルを適用した結果を結合したテンソル\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \"\"\"\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhinge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-219-4efad237e79e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m各分割に対してモデルを適用した結果を結合したテンソル\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \"\"\"\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhinge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-f87f139e74fa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# 現在の入力の次元数を取得\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
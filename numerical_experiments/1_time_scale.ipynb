{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPNhkxuesT9kZCLDGmoAJO4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kodai-utsunomiya/memorization-and-generalization/blob/main/numerical_experiments/1_time_scale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $(k, d)$-Sparse Parity Task\n",
        "\n",
        "$d$-個の数字の内の $k$ 個の数字のパリティを計算する（$k \\le d$）という問題\n",
        "\n",
        "# データセット\n",
        "\n",
        "---\n",
        "\n",
        "- $\\mathcal{D}_{k, d} = \\{ (\\boldsymbol{x}_i , y_i) \\}_{i=1}^n$\n",
        "    - $n$ 個の学習データ\n",
        "    - $\\boldsymbol{x}_i \\in \\{ 0,1 \\}^d$：バイナリーベクトル．$\\boldsymbol{x}_i \\sim \\text{Unif} \\left( \\{0,1\\}^d \\right)$\n",
        "    - $y_i = \\left(\\sum_{i}^k x^{(i)} \\right) \\text{mod} \\hspace{2mm} 2$ ：最初の $k$  個の数字（clean digits）のパリティ\n",
        "\n",
        "<br>\n",
        "\n",
        "  - $\\boldsymbol{x}_i$ の残りの $d-k$ 個の数字（noisy digits）は $y_i$ とは無関係\n",
        "\n",
        "<br>\n",
        "\n",
        "- 例：\n",
        "    - $(3, 30)$-sparse parity dataset\n",
        "        - $\\boldsymbol{x}_1$：<font color=\"blue\">000</font>$110010110001010111001001011$，  $y_1 = 0$\n",
        "        - $\\boldsymbol{x}_2$：<font color=\"blue\">010</font>$110010110001010111001001011$，  $y_2 = 1$\n",
        "            \n",
        "            $\\hspace{2mm} \\vdots$\n",
        "            \n",
        "\n",
        "<br>\n",
        "\n",
        "- 学習データをまとめて，$\\mathcal{X} = \\left[ \\boldsymbol{x}_1, \\ldots, \\boldsymbol{x}_n \\right] \\in \\mathbb{R}^{n \\times d}$，$\\mathcal{Y} = \\left[ y_1 \\ldots, y_n \\right] \\in \\mathbb{R}^n$ と行列表記"
      ],
      "metadata": {
        "id": "vnvu-UoTZ-Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BinaryDataset(Dataset):\n",
        "    def __init__(self, n, k, train_size, test_size, seed):\n",
        "        \"\"\"\n",
        "        データセットの初期化\n",
        "\n",
        "        Parameters:\n",
        "        - n: バイナリ文字列の長さ\n",
        "        - k: 出力ラベルの計算に使用する最初の k 個のビット\n",
        "        - train_size: 訓練データのサイズ\n",
        "        - test_size: テストデータのサイズ\n",
        "        - seed: ランダムシード\n",
        "        \"\"\"\n",
        "        np.random.seed(seed)\n",
        "        self.n = n\n",
        "        self.k = k\n",
        "        self.train_size = train_size\n",
        "        self.test_size = test_size\n",
        "        self.total_size = train_size + test_size\n",
        "\n",
        "        # ユニークなバイナリ文字列を生成\n",
        "        self.unique_binary_strings = self._generate_unique_binary_strings()\n",
        "\n",
        "        # 入力データと出力ラベルを準備\n",
        "        self.inputs, self.outputs = self._prepare_data()\n",
        "\n",
        "        # データのインデックスをシャッフル\n",
        "        self.indices = np.random.permutation(len(self.inputs))\n",
        "\n",
        "        # 訓練データとテストデータのインデックスを分割\n",
        "        self.train_indices = self.indices[:self.train_size]\n",
        "        self.test_indices = self.indices[self.train_size:]\n",
        "\n",
        "    def _generate_unique_binary_strings(self):\n",
        "        \"\"\"\n",
        "        ユニークなバイナリ文字列を生成するヘルパーメソッド\n",
        "\n",
        "        Returns:\n",
        "        - list: ユニークなバイナリ文字列のリスト．各バイナリ文字列は長さ n のタプル\n",
        "        \"\"\"\n",
        "        # ユニークなバイナリ文字列を保存するための空のセットを作成\n",
        "        unique_binary_strings = set()\n",
        "\n",
        "        # 必要な数のユニークなバイナリ文字列が得られるまで繰り返す\n",
        "        while len(unique_binary_strings) < self.total_size:\n",
        "            # 長さ n のバイナリ文字列を生成\n",
        "            # np.random.randint(2, size=self.n) は 0 または 1 の整数を含む長さ n の配列を生成\n",
        "            binary_string = tuple(np.random.randint(2, size=self.n))\n",
        "            unique_binary_strings.add(binary_string)\n",
        "        return list(unique_binary_strings)\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        \"\"\"\n",
        "        入力データと出力ラベルを準備するヘルパーメソッド\n",
        "\n",
        "        Returns:\n",
        "        - tuple: (入力データ, 出力ラベル)\n",
        "          - 入力データ: バイナリ文字列を NumPy 配列として保持し，最後にバイアス列を追加\n",
        "          - 出力ラベル: 最初の k ビットの合計の 2 で割った余りとして計算\n",
        "        \"\"\"\n",
        "        # バイナリ文字列をNumPy配列に変換\n",
        "        inputs = np.array(self.unique_binary_strings, dtype=np.float32)\n",
        "\n",
        "        ########## この有無でどのくらい影響がある ???\n",
        "        # # 各サンプルをノルムで割って球状に正規化\n",
        "        # norms = np.linalg.norm(inputs, axis=1, keepdims=True)\n",
        "        # inputs = inputs / norms\n",
        "\n",
        "        # 出力ラベルを計算 (最初の k ビットの合計を 2 で割った余り)\n",
        "        outputs = np.sum(inputs[:, :self.k], axis=-1) % 2\n",
        "\n",
        "        ## 入力データにバイアス用の列を追加\n",
        "        # ones_column = np.ones((inputs.shape[0], 1), dtype=np.float32)\n",
        "        #inputs = np.concatenate((inputs, ones_column), axis=1)\n",
        "        return inputs, outputs\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        データセットのサイズを返す\n",
        "\n",
        "        Returns:\n",
        "        - int: データセットの総サンプル数（訓練データとテストデータの合計）を返す\n",
        "        \"\"\"\n",
        "        return self.total_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        インデックスに対応するデータを返す\n",
        "\n",
        "        Parameters:\n",
        "        - idx: 取得したいデータのインデックス\n",
        "\n",
        "        Returns:\n",
        "        - tuple: (入力データ, 出力ラベル)\n",
        "          - 入力データ: PyTorchテンソルとして返す\n",
        "          - 出力ラベル: PyTorchテンソルとして返す\n",
        "        \"\"\"\n",
        "        input_data = torch.tensor(self.inputs[self.indices[idx]], dtype=torch.float32)\n",
        "        output_data = torch.tensor(self.outputs[self.indices[idx]], dtype=torch.float32)\n",
        "        return input_data, output_data\n",
        "\n",
        "    def get_train_data(self):\n",
        "        \"\"\"\n",
        "        訓練データのサブセットを返す\n",
        "\n",
        "        Returns:\n",
        "        - Subset: 訓練データのサブセット．このサブセットには訓練データのインデックスが含まれている\n",
        "        \"\"\"\n",
        "        return torch.utils.data.Subset(self, self.train_indices)\n",
        "\n",
        "    def get_test_data(self):\n",
        "        \"\"\"\n",
        "        テストデータのサブセットを返す\n",
        "\n",
        "        Returns:\n",
        "        - Subset: テストデータのサブセット．このサブセットにはテストデータのインデックスが含まれている\n",
        "        \"\"\"\n",
        "        return torch.utils.data.Subset(self, self.test_indices)"
      ],
      "metadata": {
        "id": "kAd8vz52eW94"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# モデル\n",
        "\n",
        "---\n",
        "\n",
        "$ F(\\boldsymbol{w}, \\boldsymbol{x}) \\equiv \\alpha \\left\\lbrack f(\\boldsymbol{w}, \\boldsymbol{x}) - f(\\boldsymbol{w}_0, \\boldsymbol{x}) \\right\\rbrack $ という形をしたモデルの学習を考える（これを予測器として使用し，$\\boldsymbol{w}$を学習）"
      ],
      "metadata": {
        "id": "DaTTQLVtbhHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FCクラス．<font color=\"green\"> $f(\\boldsymbol{x})$ の定義 </font>\n",
        "\n",
        "### ネットワークの構造\n",
        "\n",
        "1. **入力層**: 次元数 $d$ の入力を受け取る．\n",
        "2. **隠れ層**: 層数 $L$ の隠れ層があり，各隠れ層のユニット数は $h$．\n",
        "3. **出力層**: 最終層は出力がスカラー値である 1 次元のベクトルを生成．\n",
        "\n",
        "<br>\n",
        "\n",
        "### 層ごとの計算\n",
        "\n",
        "1. **初期化**:\n",
        "   - 隠れ層 $i$ の重み行列 $W_i$ は，次のように初期化：\n",
        "     \n",
        "     $\n",
        "     W_i \\sim \\mathcal{N}(0, 1)\n",
        "     $\n",
        "\n",
        "     ここで，$W_i$ のサイズは $ h \\times \\text{hh}_{i}$ ．\n",
        "     \n",
        "     $\\text{hh}_{i} $ は前の層の出力ユニット数．\n",
        "\n",
        "   - メモリ効率を考慮し，重み行列を分割：\n",
        "     \n",
        "     \\begin{aligned} W_i =  \\begin{bmatrix}\n",
        "        W_i^{(0)} \\\\\n",
        "        W_i^{(1)} \\\\\n",
        "        \\vdots \\\\\n",
        "        W_i^{(n-1)}\n",
        "        \\end{bmatrix}  \\end{aligned}\n",
        "     \n",
        "     各部分行列 $W_i^{(j)}$ はサイズ $m \\times \\text{hh}_{i}$．ここで，$m$ は分割サイズ．\n",
        "\n",
        "<br>\n",
        "\n",
        "2. **順伝播計算**:\n",
        "   - 入力テンソル $x$ は，初期の隠れ層で次のように変換：\n",
        "     \n",
        "     $\n",
        "     x^{(0)} = x W_0^T / \\sqrt{d}\n",
        "     $\n",
        "\n",
        "     ここで，$W_0$ は最初の隠れ層の重み行列．バイアス項がある場合，次のように加算：\n",
        "     \n",
        "     $\n",
        "     x^{(0)} = x^{(0)} + b_0\n",
        "     $\n",
        "\n",
        "     その後，活性化関数 $ \\sigma $ を適用：\n",
        "     \n",
        "     $\n",
        "     x^{(1)} = \\sigma(x^{(0)})\n",
        "     $\n",
        "\n",
        "   - 次の隠れ層も同様に計算．一般的に，隠れ層 $i$ の計算は次のようになる：\n",
        "     \n",
        "     $\n",
        "     x^{(i)} = x^{(i-1)} W_i^T / \\sqrt{h}\n",
        "     $\n",
        "\n",
        "     ここで，$W_i$ は現在の層の重み行列．バイアス項がある場合，次のように加算：\n",
        "\n",
        "     $\n",
        "     x^{(i)} = x^{(i)} + b_i\n",
        "     $\n",
        "\n",
        "     そして，活性化関数を適用：\n",
        "     \n",
        "     $\n",
        "     x^{(i+1)} = \\sigma(x^{(i)})\n",
        "     $\n",
        "\n",
        "   - 最終層では，次のように計算：\n",
        "     \n",
        "     $\n",
        "     x^{(L)} = x^{(L-1)} W_L^T / h + b_L\n",
        "     $\n",
        "     \n",
        "     ここで，$W_L$ は最終層の重み行列．出力テンソル $x$ を 1 次元に変換して返す：\n",
        "\n",
        "     $\n",
        "     x^{(L)} = x^{(L)} \\text{view}(-1)\n",
        "     $"
      ],
      "metadata": {
        "id": "zTfCKOJKWt-E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jNvFCjVAUR0R"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\"\"\"\n",
        "全結合ネットワーク（Fully Connected Network, FC）のクラスを定義．\n",
        "任意の層数 L を持ち，各層のユニット数は h で指定．\n",
        "活性化関数 act は任意に指定可能で，バイアス項の有無も指定可能．\n",
        "\"\"\"\n",
        "\n",
        "class FC(nn.Module):\n",
        "    def __init__(self, d, h, L, act, bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # ネットワークの初期化\n",
        "        hh = d  # 入力の次元数\n",
        "        for i in range(L):\n",
        "            # 隠れ層の重み行列を正規分布で初期化\n",
        "            W = torch.randn(h, hh)\n",
        "\n",
        "            # メモリ効率を考慮し，重み行列を部分行列に分割して ParameterList に格納\n",
        "            # next two line are here to avoid memory issue when computing the kerne\n",
        "            n = max(1, 128 * 256 // hh)  # 分割サイズを計算\n",
        "            W = nn.ParameterList([nn.Parameter(W[j: j+n]) for j in range(0, len(W), n)])\n",
        "\n",
        "            # 分割した重み行列をレイヤーとして登録\n",
        "            setattr(self, \"W{}\".format(i), W)\n",
        "\n",
        "            # バイアス項が指定されている場合は，それをゼロで初期化して登録\n",
        "            if bias:\n",
        "                self.register_parameter(\"B{}\".format(i), nn.Parameter(torch.zeros(h)))\n",
        "\n",
        "            # 次のレイヤーの入力次元は現在の隠れ層のユニット数になる\n",
        "            hh = h\n",
        "\n",
        "        # 最終層の重み行列を初期化（出力がスカラー値なので次元は (1, h)）\n",
        "        self.register_parameter(\"W{}\".format(L), nn.Parameter(torch.randn(1, hh)))\n",
        "\n",
        "        # バイアス項が指定されている場合は，最終層のバイアスをゼロで初期化\n",
        "        if bias:\n",
        "            self.register_parameter(\"B{}\".format(L), nn.Parameter(torch.zeros(1)))\n",
        "\n",
        "        # クラス変数としてレイヤー数，活性化関数，バイアスの有無を保持\n",
        "        self.L = L\n",
        "        self.act = act\n",
        "        self.bias = bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 順伝播計算\n",
        "        for i in range(self.L + 1):\n",
        "            # i 番目の層の重み行列を取得\n",
        "            W = getattr(self, \"W{}\".format(i))\n",
        "\n",
        "            # ParameterList 形式の重み行列をフルの行列に結合\n",
        "            if isinstance(W, nn.ParameterList):\n",
        "                W = torch.cat(list(W))\n",
        "\n",
        "            # バイアス項が指定されている場合は，バイアスを取得\n",
        "            if self.bias:\n",
        "                B = self.bias * getattr(self, \"B{}\".format(i))\n",
        "            else:\n",
        "                B = 0\n",
        "\n",
        "            # 現在の入力の次元数を取得\n",
        "            h = x.size(1)\n",
        "\n",
        "            if i < self.L:\n",
        "                # 隠れ層での線形変換とスケーリング，そして活性化関数の適用\n",
        "                x = x @ (W.t() / h ** 0.5)  # 重み行列との積（次元スケーリング）\n",
        "                x = self.act(x + B)  # バイアス項を加えた後，活性化関数を適用\n",
        "            else:\n",
        "                # 最終層での線形変換（出力はスカラー値）\n",
        "                x = x @ (W.t() / h) + B  # スカラー出力\n",
        "\n",
        "        # 出力を 1 次元のテンソルに変換して返す\n",
        "        return x.view(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ダイナミクス"
      ],
      "metadata": {
        "id": "1RilangZmSpN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kgfnwRl4udyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Dynamics that compares the angle of the gradient between steps and keep it small\n",
        "\n",
        "- マージンに達したときに停止\n",
        "\n",
        "2つの実装：\n",
        "1. `train_regular` - 任意のモデルに対応\n",
        "2. `train_kernel` - 線形モデル専用\n",
        "\"\"\"\n",
        "\n",
        "import copy\n",
        "import itertools\n",
        "import math\n",
        "from time import perf_counter\n",
        "import torch\n",
        "\n",
        "\n",
        "def gradient(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False):\n",
        "    '''\n",
        "    `outputs` に対する `inputs` の勾配を計算する関数\n",
        "    使用例:\n",
        "    ```\n",
        "    gradient(x.sum(), x)          # x の合計に対する勾配\n",
        "    gradient((x * y).sum(), [x, y])  # x と y の要素ごとの積の合計に対する勾配\n",
        "    ```\n",
        "\n",
        "    :param outputs: 勾配を計算する対象の出力テンソル\n",
        "    :param inputs: 勾配を計算したい入力テンソルのリストまたは単一テンソル\n",
        "    :param grad_outputs: 出力テンソルの勾配を指定するためのオプション（通常は None で良い）\n",
        "    :param retain_graph: 計算グラフを保持するかどうかを指定するフラグ（デフォルトは None）\n",
        "    :param create_graph: 勾配の計算グラフを作成するかどうかを指定するフラグ（デフォルトは False）\n",
        "    :return: 入力テンソルに対する勾配をフラットなテンソルとして返す\n",
        "    '''\n",
        "\n",
        "    # `inputs` がテンソルの場合はリストに変換\n",
        "    if torch.is_tensor(inputs):\n",
        "        inputs = [inputs]\n",
        "    else:\n",
        "        inputs = list(inputs)\n",
        "\n",
        "    # `torch.autograd.grad` 関数を使用して勾配を計算\n",
        "    grads = torch.autograd.grad(outputs, inputs, grad_outputs,\n",
        "                                allow_unused=True, # 計算に使用されないテンソルには勾配が計算されない\n",
        "                                retain_graph=retain_graph, # 計算グラフを保持するかどうか\n",
        "                                create_graph=create_graph) # 勾配の計算グラフを作成するかどうか\n",
        "\n",
        "    # 勾配が None の場合は，同じサイズのゼロテンソルを代わりに使用\n",
        "    grads = [x if x is not None else torch.zeros_like(y) for x, y in zip(grads, inputs)]\n",
        "\n",
        "    # 勾配テンソルをフラットな形状に変換して連結\n",
        "    return torch.cat([x.contiguous().view(-1) for x in grads])\n",
        "\n",
        "\n",
        "def loglinspace(rate, step, end=None):\n",
        "    \"\"\"\n",
        "    対数線形間隔での数値を生成するジェネレーター関数\n",
        "    対数的に変化する間隔で数値を生成\n",
        "\n",
        "    `rate` と `step` のパラメータを使って，新しい値を計算\n",
        "    `end` が指定されていない場合は無限に数値を生成\n",
        "\n",
        "    Arguments:\n",
        "        rate (float): 対数的な変化の速度を制御するパラメータ\n",
        "        step (float): 各ステップでの間隔の大きさ\n",
        "        end (float, optional): 生成を停止する条件となる最大値．指定されない場合は無限に生成\n",
        "\n",
        "    Yields:\n",
        "        float: 現在の時間 `t` の値を生成\n",
        "    \"\"\"\n",
        "    t = 0\n",
        "    while end is None or t <= end:\n",
        "        yield t  # 現在の時間 `t` の値を生成\n",
        "        # 次の `t` を計算．ここで，`math.exp(-t * rate / step)` は指数関数的な減衰を表す\n",
        "        t = int(t + 1 + step * (1 - math.exp(-t * rate / step)))\n",
        "\n",
        "\n",
        "class ContinuousMomentum(torch.optim.Optimizer):\n",
        "    \"\"\"連続的なモーメンタムを実装\n",
        "\n",
        "    - d/dt velocity = -1/tau (velocity + grad)\n",
        "    - または\n",
        "    - d/dt velocity = -mu/t (velocity + grad)\n",
        "\n",
        "    - d/dt parameters = velocity\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, dt, tau):\n",
        "        \"\"\"\n",
        "        初期化メソッド\n",
        "\n",
        "        Arguments:\n",
        "            params (iterable): 最適化するパラメータのリスト\n",
        "            dt (float): 時間ステップのサイズ\n",
        "            tau (float): モーメンタムのタイムコンスタント\n",
        "        \"\"\"\n",
        "        defaults = dict(dt=dt, tau=tau)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"単一の最適化ステップを実行\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): モデルを再評価し，損失を返すクロージャ．多くの最適化器にはオプショナル．\n",
        "\n",
        "        Returns:\n",
        "            loss (Tensor or None): 損失の値．クロージャが指定された場合はその損失値を返す．\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            tau = group['tau']\n",
        "            dt = group['dt']\n",
        "\n",
        "            for p in group['params']:\n",
        "                # 勾配がないパラメータはスキップ\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                param_state = self.state[p]\n",
        "                # パラメータの状態が初めてのときは，時間 t を 0 に設定\n",
        "                if 't' not in param_state:\n",
        "                    t = param_state['t'] = 0\n",
        "                else:\n",
        "                    t = param_state['t']\n",
        "\n",
        "                # モーメンタムの状態（速度）を初期化\n",
        "                if tau != 0:\n",
        "                    if 'velocity' not in param_state:\n",
        "                        v = param_state['velocity'] = torch.zeros_like(p.data)\n",
        "                    else:\n",
        "                        v = param_state['velocity']\n",
        "\n",
        "                # モーメンタムの計算\n",
        "                if tau > 0:\n",
        "                    # tau > 0 の場合の連続モーメンタムの計算\n",
        "                    x = math.exp(-dt / tau)  # 時間の経過とともに減衰する係数\n",
        "                    v.mul_(x).add_(-(1 - x), p.grad.data) # 速度に勾配を加える\n",
        "                elif tau < 0:\n",
        "                    # tau < 0 の場合の連続モーメンタムの計算\n",
        "                    mu = -tau\n",
        "                    x = (t / (t + dt)) ** mu  # 時間の経過に伴う係数\n",
        "                    v.mul_(x).add_(-(1 - x), p.grad.data)  # 速度に勾配を加える\n",
        "                else:\n",
        "                    # tau = 0 の場合のシンプルな勾配降下\n",
        "                    v = -p.grad.data\n",
        "\n",
        "                # パラメータの更新\n",
        "                p.data.add_(dt, v)    # パラメータに速度を加える\n",
        "                param_state['t'] += dt    # 時間を進める\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "def make_step(f, optimizer, dt, grad):\n",
        "    \"\"\"\n",
        "    指定された勾配 `grad` を使用して，最適化ステップを実行\n",
        "\n",
        "    Arguments:\n",
        "        f (torch.nn.Module): トレーニングするモデル\n",
        "        optimizer (torch.optim.Optimizer): 使用する最適化器\n",
        "        dt (float): 時間刻み\n",
        "        grad (torch.Tensor): 勾配テンソル\n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    # モデルの全パラメータに対してループ\n",
        "    for p in f.parameters():\n",
        "        # パラメータの総要素数を取得\n",
        "        n = p.numel()\n",
        "        # 勾配テンソルを対応するパラメータに合わせてリシェイプし，割り当て\n",
        "        p.grad = grad[i: i + n].view_as(p)\n",
        "        i += n  # インデックスを次のパラメータに進める\n",
        "\n",
        "    # 各パラメータグループに対して時間刻み `dt` を設定\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['dt'] = dt\n",
        "\n",
        "    # 最適化ステップを実行\n",
        "    optimizer.step()\n",
        "\n",
        "    # 勾配をリセット（計算グラフから切り離し）\n",
        "    for p in f.parameters():\n",
        "        p.grad = None\n",
        "\n",
        "\n",
        "def train_regular(f0, x, y, tau, max_walltime, alpha, loss, subf0, max_dgrad=math.inf, max_dout=math.inf):\n",
        "    \"\"\"\n",
        "    一般的なモデルのトレーニング関数\n",
        "\n",
        "    Arguments:\n",
        "        f0 (torch.nn.Module): 初期モデル\n",
        "        x (torch.Tensor): 入力データ\n",
        "        y (torch.Tensor): 出力ラベル\n",
        "        tau (float): モーメンタムパラメータ\n",
        "        max_walltime (float): 最大経過時間\n",
        "        alpha (float): 出力変化に対する閾値\n",
        "        loss (callable): ロス関数\n",
        "        subf0 (bool): 初期モデルの出力を使用するかどうか\n",
        "        max_dgrad (float): 勾配の変化の最大許容値\n",
        "        max_dout (float): 出力の変化の最大許容値\n",
        "    \"\"\"\n",
        "\n",
        "    # 初期モデルのコピーを作成\n",
        "    f = copy.deepcopy(f0)\n",
        "\n",
        "    # モデルの出力を計算（必要に応じて初期モデルの出力を使用）\n",
        "    with torch.no_grad():\n",
        "        out0 = f0(x) if subf0 else 0\n",
        "\n",
        "    # 時間刻みとモーメンタムのパラメータを初期化\n",
        "    dt = 1\n",
        "    step_change_dt = 0\n",
        "\n",
        "    # ContinuousMomentum オプティマイザを初期化\n",
        "    optimizer = ContinuousMomentum(f.parameters(), dt=dt, tau=tau)\n",
        "\n",
        "    # ログ-線形間隔のチェックポイント生成器を作成\n",
        "    checkpoint_generator = loglinspace(0.01, 100)\n",
        "    checkpoint = next(checkpoint_generator)\n",
        "\n",
        "    # 経過時間を計測するためのタイマーを開始\n",
        "    wall = perf_counter()\n",
        "    t = 0\n",
        "    converged = False\n",
        "\n",
        "    # 最初のモデルの出力と勾配を計算\n",
        "    out = f(x)\n",
        "    grad = gradient(loss((out - out0) * y).mean(), f.parameters())\n",
        "\n",
        "    # トレーニングループ\n",
        "    for step in itertools.count():\n",
        "        # 現在のモデルとオプティマイザの状態を保存\n",
        "        state = copy.deepcopy((f.state_dict(), optimizer.state_dict(), t))\n",
        "\n",
        "        while True:\n",
        "            # 最適化ステップを実行\n",
        "            make_step(f, optimizer, dt, grad)\n",
        "            t += dt\n",
        "            current_dt = dt\n",
        "\n",
        "            # モデルの新しい出力と勾配を計算\n",
        "            new_out = f(x)\n",
        "            new_grad = gradient(loss((new_out - out0) * y).mean(), f.parameters())\n",
        "\n",
        "            # 出力の変化量を計算\n",
        "            dout = (out - new_out).mul(alpha).abs().max().item()\n",
        "\n",
        "            # 勾配の変化量を計算\n",
        "            if grad.norm() == 0 or new_grad.norm() == 0:\n",
        "                dgrad = 0\n",
        "            else:\n",
        "                dgrad = (grad - new_grad).norm().pow(2).div(grad.norm() * new_grad.norm()).item()\n",
        "\n",
        "            # 勾配の変化量と出力の変化量が閾値以下であれば，時間刻みを増加\n",
        "            if dgrad < max_dgrad and dout < max_dout:\n",
        "                if dgrad < 0.5 * max_dgrad and dout < 0.5 * max_dout:\n",
        "                    dt *= 1.1\n",
        "                break\n",
        "            # そうでない場合は，時間刻みを減少\n",
        "            dt /= 10\n",
        "\n",
        "            # 現在の状態を出力\n",
        "            print(\"[{} +{}] [dt={:.1e} dgrad={:.1e} dout={:.1e}]\".format(step, step - step_change_dt, dt, dgrad, dout), flush=True)\n",
        "\n",
        "            # モデルとオプティマイザの状態をリストア\n",
        "            step_change_dt = step\n",
        "            f.load_state_dict(state[0])\n",
        "            optimizer.load_state_dict(state[1])\n",
        "            t = state[2]\n",
        "\n",
        "        # 新しい出力と勾配を保存\n",
        "        out = new_out\n",
        "        grad = new_grad\n",
        "\n",
        "        save = False\n",
        "\n",
        "        # チェックポイントに達した場合，または収束した場合に状態を保存\n",
        "        if step == checkpoint:\n",
        "            checkpoint = next(checkpoint_generator)\n",
        "            assert checkpoint > step\n",
        "            save = True\n",
        "\n",
        "        # 出力が閾値を超え，収束していない場合に収束を宣言\n",
        "        if (alpha * (out - out0) * y >= 1).all() and not converged:\n",
        "            converged = True\n",
        "            save = True\n",
        "\n",
        "        if save:\n",
        "            state = {\n",
        "                'step': step,\n",
        "                'wall': perf_counter() - wall,\n",
        "                't': t,\n",
        "                'dt': current_dt,\n",
        "                'dgrad': dgrad,\n",
        "                'dout': dout,\n",
        "                'norm': sum(p.norm().pow(2) for p in f.parameters()).sqrt().item(),\n",
        "                'dnorm': sum((p0 - p).norm().pow(2) for p0, p in zip(f0.parameters(), f.parameters())).sqrt().item(),\n",
        "                'grad_norm': grad.norm().item(),\n",
        "            }\n",
        "\n",
        "            yield f, state, converged\n",
        "\n",
        "        # 収束した場合はトレーニングを終了\n",
        "        if converged:\n",
        "            break\n",
        "\n",
        "        # 経過時間が最大経過時間を超えた場合にトレーニングを終了\n",
        "        if perf_counter() > wall + max_walltime:\n",
        "            break\n",
        "\n",
        "        # 出力に NaN が含まれている場合，トレーニングを終了\n",
        "        if torch.isnan(out).any():\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "def train_kernel(ktrtr, ytr, tau, max_walltime, alpha, loss_prim, max_dgrad=math.inf, max_dout=math.inf):\n",
        "    \"\"\"\n",
        "    線形モデル専用のトレーニング関数\n",
        "\n",
        "    Arguments:\n",
        "        ktrtr (torch.Tensor): カーネル行列\n",
        "        ytr (torch.Tensor): 出力ラベル\n",
        "        tau (float): モーメンタムパラメータ\n",
        "        max_walltime (float): 最大経過時間\n",
        "        alpha (float): 出力変化に対する閾値\n",
        "        loss_prim (callable): プライムロス関数\n",
        "        max_dgrad (float): 勾配の変化の最大許容値\n",
        "        max_dout (float): 出力の変化の最大許容値\n",
        "    \"\"\"\n",
        "    # 初期出力と速度ベクトルをゼロで初期化\n",
        "    otr = ktrtr.new_zeros(len(ytr))\n",
        "    velo = otr.clone()\n",
        "\n",
        "    # 時間刻みとステップ変更のタイミングを初期化\n",
        "    dt = 1\n",
        "    step_change_dt = 0\n",
        "\n",
        "    # ログ-線形間隔のチェックポイント生成器を作成\n",
        "    checkpoint_generator = loglinspace(0.01, 100)\n",
        "    checkpoint = next(checkpoint_generator)\n",
        "\n",
        "    # 経過時間を計測するためのタイマーを開始\n",
        "    wall = perf_counter()\n",
        "    t = 0\n",
        "    converged = False\n",
        "\n",
        "    # 初期のプライムロスを計算\n",
        "    lprim = loss_prim(otr * ytr) * ytr\n",
        "    grad = ktrtr @ lprim / len(ytr)\n",
        "\n",
        "    # トレーニングループ\n",
        "    for step in itertools.count():\n",
        "\n",
        "        # 現在の出力，速度，時間を保存\n",
        "        state = copy.deepcopy((otr, velo, t))\n",
        "\n",
        "        while True:\n",
        "            # モーメンタムパラメータに基づいて速度を更新\n",
        "            if tau > 0:\n",
        "                x = math.exp(-dt / tau)\n",
        "                velo.mul_(x).add_(-(1 - x), grad)\n",
        "            elif tau < 0:\n",
        "                mu = -tau\n",
        "                x = (t / (t + dt)) ** mu\n",
        "                velo.mul_(x).add_(-(1 - x), grad)\n",
        "            else:\n",
        "                velo.copy_(-grad)\n",
        "\n",
        "            # 出力を更新\n",
        "            otr.add_(dt, velo)\n",
        "\n",
        "            t += dt\n",
        "            current_dt = dt\n",
        "\n",
        "            # 新しいプライムロスを計算し，勾配を更新\n",
        "            lprim = loss_prim(otr * ytr) * ytr\n",
        "            new_grad = ktrtr @ lprim / len(ytr)\n",
        "\n",
        "            # 出力の変化量を計算\n",
        "            dout = velo.mul(dt * alpha).abs().max().item()\n",
        "\n",
        "            # 勾配の変化量を計算\n",
        "            if grad.norm() == 0 or new_grad.norm() == 0:\n",
        "                dgrad = 0\n",
        "            else:\n",
        "                dgrad = (grad - new_grad).norm().pow(2).div(grad.norm() * new_grad.norm()).item()\n",
        "\n",
        "            # 勾配と出力の変化量が許容範囲内であれば，時間刻みを増加\n",
        "            if dgrad < max_dgrad and dout < max_dout:\n",
        "                if dgrad < 0.1 * max_dgrad and dout < 0.1 * max_dout:\n",
        "                    dt *= 1.1\n",
        "                break\n",
        "\n",
        "            # そうでない場合は，時間刻みを減少\n",
        "            dt /= 10\n",
        "\n",
        "            # 現在の状態を出力\n",
        "            print(\"[{} +{}] [dt={:.1e} dgrad={:.1e} dout={:.1e}]\".format(step, step - step_change_dt, dt, dgrad, dout), flush=True)\n",
        "            step_change_dt = step\n",
        "\n",
        "            # 保存した状態をリストア\n",
        "            otr.copy_(state[0])\n",
        "            velo.copy_(state[1])\n",
        "            t = state[2]\n",
        "\n",
        "        # 勾配を新しい値で更新\n",
        "        grad = new_grad\n",
        "\n",
        "        save = False\n",
        "\n",
        "        # チェックポイントに達した場合，または収束した場合に状態を保存\n",
        "        if step == checkpoint:\n",
        "            checkpoint = next(checkpoint_generator)\n",
        "            assert checkpoint > step\n",
        "            save = True\n",
        "\n",
        "        # 出力が閾値を超え，収束していない場合に収束を宣言\n",
        "        if (alpha * otr * ytr >= 1).all() and not converged:\n",
        "            converged = True\n",
        "            save = True\n",
        "\n",
        "        if save:\n",
        "            state = {\n",
        "                'step': step,\n",
        "                'wall': perf_counter() - wall,\n",
        "                't': t,\n",
        "                'dt': current_dt,\n",
        "                'dgrad': dgrad,\n",
        "                'dout': dout,\n",
        "                'grad_norm': grad.norm().item(),\n",
        "            }\n",
        "\n",
        "            yield otr, velo, grad, state, converged\n",
        "\n",
        "        # 収束した場合はトレーニングを終了\n",
        "        if converged:\n",
        "            break\n",
        "\n",
        "        # 経過時間が最大経過時間を超えた場合にトレーニングを終了\n",
        "        if perf_counter() > wall + max_walltime:\n",
        "            break\n",
        "\n",
        "        # 出力に NaN が含まれている場合，トレーニングを終了\n",
        "        if torch.isnan(otr).any():\n",
        "            break"
      ],
      "metadata": {
        "id": "gJZk7JSQdxx7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`compute_kernels` 関数は，与えられたモデル $ f $ の入力データ $ x_{\\text{tr}} $ と $ x_{\\text{te}} $ に基づいて，カーネル行列（Gram 行列）を計算．\n",
        "\n",
        "- $ K_{\\text{trtr}} $：トレーニングデータ間のカーネル行列\n",
        "- $ K_{\\text{tetr}} $：テストデータとトレーニングデータ間のカーネル行列\n",
        "- $ K_{\\text{tete}} $：テストデータ間のカーネル行列\n",
        "\n",
        "これらの行列は，モデルのパラメータに関する勾配を使って計算．\n",
        "\n",
        "<br>\n",
        "\n",
        "### モデルの勾配計算\n",
        "\n",
        "モデル $ f $ が入力 $ x $ に対して出力を生成し，その勾配を計算．勾配の計算は以下のように行う：\n",
        "\n",
        "1. 入力 $ x $ に対するモデルの出力を $ f(x) $ とする．\n",
        "2. この出力に対するパラメータ $ \\theta $ の勾配を求める：$ \\nabla_\\theta f(x) $\n",
        "\n",
        "  ここで，$ \\nabla_\\theta f(x) $ は $ x $ に対する勾配であり，モデルのパラメータ $ \\theta $ に関する勾配ベクトル．\n",
        "\n",
        "<br>\n",
        "\n",
        "### Gram 行列の計算\n",
        "\n",
        "1. $ K_{\\text{trtr}} $\n",
        "\n",
        "  トレーニングデータ $ x_{\\text{tr}} $ に対するカーネル行列 $ K_{\\text{trtr}} $ は，各トレーニングデータポイント $ x_i $ と $ x_j $ の勾配ベクトルに基づいて計算：\n",
        "\n",
        "  $ K_{\\text{trtr}} = J_{\\text{tr}} J_{\\text{tr}}^T $\n",
        "\n",
        "  ここで，$ J_{\\text{tr}} $ はトレーニングデータ $ x_{\\text{tr}} $ に対する勾配ベクトルを列に持つ行列．$ J_{\\text{tr}} $ の $ i $-th 行は，入力 $ x_i $ に対する勾配ベクトル．\n",
        "\n",
        "<br>\n",
        "\n",
        "2. $ K_{\\text{tetr}} $\n",
        "\n",
        "  テストデータ $ x_{\\text{te}} $ とトレーニングデータ $ x_{\\text{tr}} $ とのカーネル行列 $ K_{\\text{tetr}} $ は次のように計算：\n",
        "\n",
        "  $ K_{\\text{tetr}} = J_{\\text{te}} J_{\\text{tr}}^T $\n",
        "\n",
        "  ここで，$ J_{\\text{te}} $ はテストデータ $ x_{\\text{te}} $ に対する勾配ベクトルを列に持つ行列．\n",
        "\n",
        "<br>\n",
        "\n",
        "3. $ K_{\\text{tete}} $\n",
        "\n",
        "  テストデータ $ x_{\\text{te}} $ に対するカーネル行列 $ K_{\\text{tete}} $ は、テストデータポイント $ x_i $ と $ x_j $ の勾配ベクトルに基づいて計算：\n",
        "\n",
        "  $ K_{\\text{tete}} = J_{\\text{te}} J_{\\text{te}}^T $\n",
        "\n",
        "  ここで，$ J_{\\text{te}} $ の $ i $-th 行は，入力 $ x_i $ に対する勾配ベクトル．"
      ],
      "metadata": {
        "id": "iL4Pua0dm4DN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pylint: disable=no-member, C, not-callable\n",
        "\"\"\"\n",
        "Computes the Gram matrix of a given model\n",
        "\"\"\"\n",
        "\n",
        "def compute_kernels(f, xtr, xte):\n",
        "    # from hessian import gradient\n",
        "\n",
        "    # 新しいゼロ行列を作成\n",
        "    # ktrtr: トレーニングデータ間のカーネル行列\n",
        "    # ktetr: テストデータとトレーニングデータ間のカーネル行列\n",
        "    # ktete: テストデータ間のカーネル行列\n",
        "    ktrtr = xtr.new_zeros(len(xtr), len(xtr))\n",
        "    ktetr = xtr.new_zeros(len(xte), len(xtr))\n",
        "    ktete = xtr.new_zeros(len(xte), len(xte))\n",
        "\n",
        "    params = []\n",
        "    current = []\n",
        "\n",
        "    # モデルのパラメータをサイズで降順にソートし，メモリ制限に基づいて分割\n",
        "    for p in sorted(f.parameters(), key=lambda p: p.numel(), reverse=True):\n",
        "        current.append(p)\n",
        "        # メモリ制限に基づき，パラメータを分割\n",
        "        if sum(p.numel() for p in current) > 2e9 // (8 * (len(xtr) + len(xte))):\n",
        "            if len(current) > 1:\n",
        "                params.append(current[:-1])\n",
        "                current = current[-1:]\n",
        "            else:\n",
        "                params.append(current)\n",
        "                current = []\n",
        "    if len(current) > 0:\n",
        "        params.append(current)\n",
        "\n",
        "    # 各パラメータグループについてカーネル行列を計算\n",
        "    for i, p in enumerate(params):\n",
        "        print(\"[{}/{}] [len={} numel={}]\".format(i, len(params), len(p), sum(x.numel() for x in p)), flush=True)\n",
        "\n",
        "        # 勾配行列を初期化\n",
        "        jtr = xtr.new_empty(len(xtr), sum(u.numel() for u in p))  # (P, N~) # (トレーニングデータ数, パラメータ数の合計)\n",
        "        jte = xte.new_empty(len(xte), sum(u.numel() for u in p))  # (P, N~) # (テストデータ数, パラメータ数の合計)\n",
        "\n",
        "        # トレーニングデータに対する勾配行列を計算\n",
        "        for j, x in enumerate(xtr):\n",
        "            jtr[j] = gradient(f(x[None]), p)  # (N~) # (パラメータ数の合計)\n",
        "\n",
        "        # テストデータに対する勾配行列を計算\n",
        "        for j, x in enumerate(xte):\n",
        "            jte[j] = gradient(f(x[None]), p)  # (N~) # (パラメータ数の合計)\n",
        "\n",
        "        # カーネル行列を更新\n",
        "        ktrtr.add_(jtr @ jtr.t())  # トレーニングデータ間のカーネル行列\n",
        "        ktetr.add_(jte @ jtr.t())  # テストデータとトレーニングデータ間のカーネル行列\n",
        "        ktete.add_(jte @ jte.t())  # テストデータ間のカーネル行列\n",
        "        del jtr, jte  # 不要になった勾配行列を削除\n",
        "\n",
        "    return ktrtr, ktetr, ktete"
      ],
      "metadata": {
        "id": "KofuVIsHm5Qf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
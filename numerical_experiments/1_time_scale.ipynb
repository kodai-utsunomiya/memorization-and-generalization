{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvm+fov0l4UL2T7FkEI8Ub",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kodai-utsunomiya/memorization-and-generalization/blob/main/numerical_experiments/1_time_scale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNvFCjVAUR0R"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\"\"\"\n",
        "全結合ネットワーク（Fully Connected Network, FC）のクラスを定義．\n",
        "任意の層数 L を持ち，各層のユニット数は h で指定．\n",
        "活性化関数 act は任意に指定可能で，バイアス項の有無も指定可能．\n",
        "\"\"\"\n",
        "\n",
        "class FC(nn.Module):\n",
        "    def __init__(self, d, h, L, act, bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # ネットワークの初期化\n",
        "        hh = d  # 入力の次元数\n",
        "        for i in range(L):\n",
        "            # 隠れ層の重み行列を正規分布で初期化\n",
        "            W = torch.randn(h, hh)\n",
        "\n",
        "            # メモリ効率を考慮し，重み行列を部分行列に分割して ParameterList に格納\n",
        "            # next two line are here to avoid memory issue when computing the kerne\n",
        "            n = max(1, 128 * 256 // hh)  # 分割サイズを計算\n",
        "            W = nn.ParameterList([nn.Parameter(W[j: j+n]) for j in range(0, len(W), n)])\n",
        "\n",
        "            # 分割した重み行列をレイヤーとして登録\n",
        "            setattr(self, \"W{}\".format(i), W)\n",
        "\n",
        "            # バイアス項が指定されている場合は，それをゼロで初期化して登録\n",
        "            if bias:\n",
        "                self.register_parameter(\"B{}\".format(i), nn.Parameter(torch.zeros(h)))\n",
        "\n",
        "            # 次のレイヤーの入力次元は現在の隠れ層のユニット数になる\n",
        "            hh = h\n",
        "\n",
        "        # 最終層の重み行列を初期化（出力がスカラー値なので次元は (1, h)）\n",
        "        self.register_parameter(\"W{}\".format(L), nn.Parameter(torch.randn(1, hh)))\n",
        "\n",
        "        # バイアス項が指定されている場合は，最終層のバイアスをゼロで初期化\n",
        "        if bias:\n",
        "            self.register_parameter(\"B{}\".format(L), nn.Parameter(torch.zeros(1)))\n",
        "\n",
        "        # クラス変数としてレイヤー数，活性化関数，バイアスの有無を保持\n",
        "        self.L = L\n",
        "        self.act = act\n",
        "        self.bias = bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 順伝播計算\n",
        "        for i in range(self.L + 1):\n",
        "            # i 番目の層の重み行列を取得\n",
        "            W = getattr(self, \"W{}\".format(i))\n",
        "\n",
        "            # ParameterList 形式の重み行列をフルの行列に結合\n",
        "            if isinstance(W, nn.ParameterList):\n",
        "                W = torch.cat(list(W))\n",
        "\n",
        "            # バイアス項が指定されている場合は，バイアスを取得\n",
        "            if self.bias:\n",
        "                B = self.bias * getattr(self, \"B{}\".format(i))\n",
        "            else:\n",
        "                B = 0\n",
        "\n",
        "            # 現在の入力の次元数を取得\n",
        "            h = x.size(1)\n",
        "\n",
        "            if i < self.L:\n",
        "                # 隠れ層での線形変換とスケーリング，そして活性化関数の適用\n",
        "                x = x @ (W.t() / h ** 0.5)  # 重み行列との積（次元スケーリング）\n",
        "                x = self.act(x + B)  # バイアス項を加えた後，活性化関数を適用\n",
        "            else:\n",
        "                # 最終層での線形変換（出力はスカラー値）\n",
        "                x = x @ (W.t() / h) + B  # スカラー出力\n",
        "\n",
        "        # 出力を 1 次元のテンソルに変換して返す\n",
        "        return x.view(-1)"
      ]
    }
  ]
}
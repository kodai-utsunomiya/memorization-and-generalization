{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNrZKz7c3fZpuqV12rVGVLs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kodai-utsunomiya/memorization-and-generalization/blob/main/numerical_experiments/1_time_scale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $(k, d)$-Sparse Parity Task\n",
        "\n",
        "$d$-個の数字の内の $k$ 個の数字のパリティを計算する（$k \\le d$）という問題\n",
        "\n",
        "# データセット\n",
        "\n",
        "---\n",
        "\n",
        "- $\\mathcal{D}_{k, d} = \\{ (\\boldsymbol{x}_i , y_i) \\}_{i=1}^n$\n",
        "    - $n$ 個の学習データ\n",
        "    - $\\boldsymbol{x}_i \\in \\{ 0,1 \\}^d$：バイナリーベクトル．$\\boldsymbol{x}_i \\sim \\text{Unif} \\left( \\{0,1\\}^d \\right)$\n",
        "    - $y_i = \\left(\\sum_{i}^k x^{(i)} \\right) \\text{mod} \\hspace{2mm} 2$ ：最初の $k$  個の数字（clean digits）のパリティ\n",
        "\n",
        "<br>\n",
        "\n",
        "  - $\\boldsymbol{x}_i$ の残りの $d-k$ 個の数字（noisy digits）は $y_i$ とは無関係\n",
        "\n",
        "<br>\n",
        "\n",
        "- 例：\n",
        "    - $(3, 30)$-sparse parity dataset\n",
        "        - $\\boldsymbol{x}_1$：<font color=\"blue\">000</font>$110010110001010111001001011$，  $y_1 = 0$\n",
        "        - $\\boldsymbol{x}_2$：<font color=\"blue\">010</font>$110010110001010111001001011$，  $y_2 = 1$\n",
        "            \n",
        "            $\\hspace{2mm} \\vdots$\n",
        "            \n",
        "\n",
        "<br>\n",
        "\n",
        "- 学習データをまとめて，$\\mathcal{X} = \\left[ \\boldsymbol{x}_1, \\ldots, \\boldsymbol{x}_n \\right] \\in \\mathbb{R}^{n \\times d}$，$\\mathcal{Y} = \\left[ y_1 \\ldots, y_n \\right] \\in \\mathbb{R}^n$ と行列表記"
      ],
      "metadata": {
        "id": "vnvu-UoTZ-Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, Subset\n",
        "\n",
        "def _generate_unique_binary_strings(n, total_size, data_seed):\n",
        "    \"\"\"\n",
        "    ユニークなバイナリ文字列を生成する関数\n",
        "\n",
        "    Parameters:\n",
        "    - n: バイナリ文字列の長さ\n",
        "    - total_size: 必要なユニークなバイナリ文字列の数\n",
        "    - data_seed: ランダムシード\n",
        "\n",
        "    Returns:\n",
        "    - list: ユニークなバイナリ文字列のリスト．各バイナリ文字列は長さ n のタプル\n",
        "    \"\"\"\n",
        "    np.random.seed(data_seed)\n",
        "    unique_binary_strings = set()\n",
        "\n",
        "    # 必要な数のユニークなバイナリ文字列が得られるまで繰り返す\n",
        "    while len(unique_binary_strings) < total_size:\n",
        "        binary_string = tuple(np.random.randint(2, size=n))\n",
        "        unique_binary_strings.add(binary_string)\n",
        "    return list(unique_binary_strings)\n",
        "\n",
        "def _prepare_data(binary_strings, k):\n",
        "    \"\"\"\n",
        "    入力データと出力ラベルを準備する関数\n",
        "\n",
        "    Parameters:\n",
        "    - binary_strings: バイナリ文字列のリスト．各バイナリ文字列は長さ n のタプル\n",
        "    - k: 出力ラベルの計算に使用する最初の k 個のビット\n",
        "\n",
        "    Returns:\n",
        "    - tuple: (入力データ, 出力ラベル)\n",
        "      - 入力データ: バイナリ文字列を NumPy 配列として保持し，最後にバイアス列を追加\n",
        "      - 出力ラベル: 最初の k ビットの合計の 2 で割った余りとして計算\n",
        "    \"\"\"\n",
        "    # バイナリ文字列をNumPy配列に変換\n",
        "    inputs = np.array(binary_strings, dtype=np.float32)\n",
        "\n",
        "    # 出力ラベルを計算 (最初の k ビットの合計を 2 で割った余り)\n",
        "    outputs = np.sum(inputs[:, :k], axis=-1) % 2\n",
        "\n",
        "    # 出力ラベルを 0 -> -1, 1 -> 1 に変換（ヒンジ損失用）\n",
        "    outputs = 2 * outputs - 1\n",
        "\n",
        "    return inputs, outputs\n",
        "\n",
        "def _normalize_data(inputs):\n",
        "    \"\"\"\n",
        "    データの中心化と標準化を行うヘルパー関数\n",
        "\n",
        "    Parameters:\n",
        "    - inputs: 正規化対象の入力データ (NumPy配列)\n",
        "\n",
        "    Returns:\n",
        "    - 正規化された入力データ\n",
        "    \"\"\"\n",
        "    # NumPy配列をPyTorchテンソルに変換\n",
        "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
        "\n",
        "    # 1. データの中心化（平均を0に）\n",
        "    inputs = inputs - inputs.mean(dim=0)\n",
        "\n",
        "    # 2. 標準化（各サンプルのノルムで割る）．データが単位ノルムを持つようにスケーリングされる\n",
        "    norm = inputs.flatten(1).norm(dim=1).view(-1, *(1,) * (inputs.dim() - 1))\n",
        "    inputs = (inputs[0].numel() ** 0.5) * inputs / norm\n",
        "\n",
        "    return inputs.numpy()  # NumPy配列に戻す\n",
        "\n",
        "def get_binary_dataset(n, k, train_size, test_size, data_seed, normalize=False, device=None):\n",
        "    \"\"\"\n",
        "    二値分類用にデータセットを取得する関数\n",
        "\n",
        "    Parameters:\n",
        "    - n: バイナリ文字列の長さ\n",
        "    - k: 出力ラベルの計算に使用する最初の k 個のビット\n",
        "    - train_size: 訓練データのサイズ\n",
        "    - test_size: テストデータのサイズ\n",
        "    - data_seed: ランダムシード\n",
        "    - normalize: データを正規化するかどうかを示すフラグ\n",
        "    - device: データを移動するデバイス\n",
        "\n",
        "    Returns:\n",
        "    - tuple: ((訓練データのx, 訓練データのy), (テストデータのx, テストデータのy))\n",
        "      - 訓練データのx: 訓練データの入力データ\n",
        "      - 訓練データのy: 訓練データの出力ラベル\n",
        "      - テストデータのx: テストデータの入力データ\n",
        "      - テストデータのy: テストデータの出力ラベル\n",
        "    \"\"\"\n",
        "    total_size = train_size + test_size\n",
        "\n",
        "    # ユニークなバイナリ文字列を生成\n",
        "    binary_strings = _generate_unique_binary_strings(n, total_size, data_seed)\n",
        "\n",
        "    # 入力データと出力ラベルを準備\n",
        "    inputs, outputs = _prepare_data(binary_strings, k)\n",
        "\n",
        "    # 正規化を行う場合（中心化と標準化）\n",
        "    if normalize:\n",
        "        inputs = _normalize_data(inputs)\n",
        "\n",
        "    # データのインデックスをシャッフル\n",
        "    indices = np.random.permutation(total_size)\n",
        "\n",
        "    # 訓練データとテストデータのインデックスを分割\n",
        "    train_indices = indices[:train_size]\n",
        "    test_indices = indices[train_size:]\n",
        "\n",
        "    # 訓練データとテストデータをNumPy配列からPyTorchテンソルに変換\n",
        "    train_inputs = torch.tensor(inputs[train_indices], dtype=torch.float32)\n",
        "    train_outputs = torch.tensor(outputs[train_indices], dtype=torch.float32)\n",
        "    test_inputs = torch.tensor(inputs[test_indices], dtype=torch.float32)\n",
        "    test_outputs = torch.tensor(outputs[test_indices], dtype=torch.float32)\n",
        "\n",
        "    # データを指定されたデバイスに移動\n",
        "    if device:\n",
        "        train_inputs, train_outputs = train_inputs.to(device), train_outputs.to(device)\n",
        "        test_inputs, test_outputs = test_inputs.to(device), test_outputs.to(device)\n",
        "\n",
        "    return (train_inputs, train_outputs), (test_inputs, test_outputs)"
      ],
      "metadata": {
        "id": "LYe1p9BP1mgt"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = get_binary_dataset(n=30, k=3, train_size=900, test_size=100, data_seed=42, normalize=False)"
      ],
      "metadata": {
        "id": "PMHgp_d5bUAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"訓練データのパターン\\n\", a[0][0].shape)\n",
        "print(a[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6rdETkDbmPW",
        "outputId": "6b1dba6a-a256-4e92-cce9-4908f7fd08ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "訓練データのパターン\n",
            " torch.Size([900, 30])\n",
            "tensor([[1., 0., 1.,  ..., 0., 0., 1.],\n",
            "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
            "        [0., 1., 1.,  ..., 0., 1., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 1., 0., 1.],\n",
            "        [0., 1., 1.,  ..., 0., 0., 1.],\n",
            "        [1., 0., 0.,  ..., 1., 0., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"訓練データのラベル\\n\", a[0][1].shape)\n",
        "print(a[0][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ZOBMuW2IEG",
        "outputId": "504b26e4-4731-4332-8d35-0e1eaf46f14a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "訓練データのラベル\n",
            " torch.Size([900])\n",
            "tensor([-1.,  1., -1., -1., -1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
            "        -1., -1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
            "         1., -1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
            "        -1., -1., -1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,\n",
            "        -1.,  1., -1., -1., -1., -1., -1., -1., -1.,  1., -1.,  1., -1.,  1.,\n",
            "        -1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1.,\n",
            "         1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,\n",
            "         1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,  1.,  1., -1., -1., -1.,\n",
            "        -1., -1.,  1.,  1.,  1., -1., -1., -1., -1., -1.,  1.,  1., -1.,  1.,\n",
            "        -1.,  1.,  1., -1., -1.,  1., -1., -1., -1.,  1., -1.,  1., -1., -1.,\n",
            "         1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1.,\n",
            "         1.,  1., -1., -1., -1., -1.,  1., -1., -1.,  1.,  1., -1., -1., -1.,\n",
            "        -1., -1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,\n",
            "        -1., -1., -1.,  1., -1.,  1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,\n",
            "         1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,\n",
            "         1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,\n",
            "         1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1.,\n",
            "         1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,\n",
            "        -1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1.,\n",
            "        -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,\n",
            "        -1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,\n",
            "        -1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1., -1.,\n",
            "        -1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
            "         1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1.,\n",
            "         1., -1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
            "         1., -1., -1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
            "         1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
            "        -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,\n",
            "         1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
            "        -1.,  1.,  1., -1., -1., -1.,  1., -1., -1., -1., -1.,  1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1., -1.,  1., -1., -1.,  1., -1.,  1., -1., -1.,  1.,\n",
            "         1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,\n",
            "        -1.,  1.,  1., -1., -1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,\n",
            "         1., -1., -1., -1., -1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,\n",
            "        -1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,\n",
            "        -1.,  1., -1.,  1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,\n",
            "         1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "         1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1., -1., -1.,\n",
            "        -1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1.,  1.,\n",
            "         1., -1., -1., -1.,  1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,\n",
            "        -1., -1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.,  1.,\n",
            "        -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "        -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,  1., -1., -1.,\n",
            "         1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,  1., -1.,\n",
            "         1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,\n",
            "        -1.,  1., -1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
            "         1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1., -1., -1., -1.,  1.,\n",
            "        -1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,\n",
            "         1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,\n",
            "        -1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1.,\n",
            "         1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1.,\n",
            "        -1.,  1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,\n",
            "        -1.,  1., -1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,  1.,\n",
            "         1.,  1., -1.,  1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,\n",
            "        -1.,  1., -1., -1.,  1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,\n",
            "        -1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1., -1., -1.,  1., -1., -1.,\n",
            "        -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "        -1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,\n",
            "         1., -1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.,\n",
            "         1., -1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,\n",
            "        -1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,\n",
            "         1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "        -1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,\n",
            "        -1.,  1., -1.,  1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a[1][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc1uo1Bg2M5b",
        "outputId": "77fc0586-8a60-4c0e-af27-9e18b704db26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a[1][1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNeKhB2d2O0L",
        "outputId": "ee03d1d6-e3c6-4538-ffc0-b1d179930355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# モデル\n",
        "\n",
        "---\n",
        "\n",
        "$ F(\\boldsymbol{w}, \\boldsymbol{x}) \\equiv \\alpha \\left\\lbrack f(\\boldsymbol{w}, \\boldsymbol{x}) - f(\\boldsymbol{w}_0, \\boldsymbol{x}) \\right\\rbrack $ という形をしたモデルの学習を考える（これを予測器として使用し，$\\boldsymbol{w}$を学習）"
      ],
      "metadata": {
        "id": "DaTTQLVtbhHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FCクラス．<font color=\"green\"> $f(\\boldsymbol{x})$ の定義 </font>\n",
        "\n",
        "### ネットワークの構造\n",
        "\n",
        "1. **入力層**: 次元数 $d$ の入力を受け取る．\n",
        "2. **隠れ層**: 層数 $L$ の隠れ層があり，各隠れ層のユニット数は $h$．\n",
        "3. **出力層**: 最終層は出力がスカラー値である 1 次元のベクトルを生成．\n",
        "\n",
        "<br>\n",
        "\n",
        "### 層ごとの計算\n",
        "\n",
        "1. **初期化**:\n",
        "   - 隠れ層 $i$ の重み行列 $W_i$ は，次のように初期化：\n",
        "     \n",
        "     $\n",
        "     W_i \\sim \\mathcal{N}(0, 1)\n",
        "     $\n",
        "\n",
        "     ここで，$W_i$ のサイズは $ h \\times \\text{hh}_{i}$ ．\n",
        "     \n",
        "     $\\text{hh}_{i} $ は前の層の出力ユニット数．\n",
        "\n",
        "   - メモリ効率を考慮し，重み行列を分割：\n",
        "     \n",
        "     \\begin{aligned} W_i =  \\begin{bmatrix}\n",
        "        W_i^{(0)} \\\\\n",
        "        W_i^{(1)} \\\\\n",
        "        \\vdots \\\\\n",
        "        W_i^{(n-1)}\n",
        "        \\end{bmatrix}  \\end{aligned}\n",
        "     \n",
        "     各部分行列 $W_i^{(j)}$ はサイズ $m \\times \\text{hh}_{i}$．ここで，$m$ は分割サイズ．\n",
        "\n",
        "<br>\n",
        "\n",
        "2. **順伝播計算**:\n",
        "   - 入力テンソル $x$ は，初期の隠れ層で次のように変換：\n",
        "     \n",
        "     $\n",
        "     x^{(0)} = x W_0^T / \\sqrt{d}\n",
        "     $\n",
        "\n",
        "     ここで，$W_0$ は最初の隠れ層の重み行列．バイアス項がある場合，次のように加算：\n",
        "     \n",
        "     $\n",
        "     x^{(0)} = x^{(0)} + b_0\n",
        "     $\n",
        "\n",
        "     その後，活性化関数 $ \\sigma $ を適用：\n",
        "     \n",
        "     $\n",
        "     x^{(1)} = \\sigma(x^{(0)})\n",
        "     $\n",
        "\n",
        "   - 次の隠れ層も同様に計算．一般的に，隠れ層 $i$ の計算は次のようになる：\n",
        "     \n",
        "     $\n",
        "     x^{(i)} = x^{(i-1)} W_i^T / \\sqrt{h}\n",
        "     $\n",
        "\n",
        "     ここで，$W_i$ は現在の層の重み行列．バイアス項がある場合，次のように加算：\n",
        "\n",
        "     $\n",
        "     x^{(i)} = x^{(i)} + b_i\n",
        "     $\n",
        "\n",
        "     そして，活性化関数を適用：\n",
        "     \n",
        "     $\n",
        "     x^{(i+1)} = \\sigma(x^{(i)})\n",
        "     $\n",
        "\n",
        "   - 最終層では，次のように計算：\n",
        "     \n",
        "     $\n",
        "     x^{(L)} = x^{(L-1)} W_L^T / \\sqrt{h} + b_L\n",
        "     $\n",
        "     \n",
        "     ここで，$W_L$ は最終層の重み行列．出力テンソル $x$ を 1 次元に変換して返す：\n",
        "\n",
        "     $\n",
        "     x^{(L)} = x^{(L)} \\text{view}(-1)\n",
        "     $"
      ],
      "metadata": {
        "id": "zTfCKOJKWt-E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNvFCjVAUR0R"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\"\"\"\n",
        "全結合ネットワーク（Fully Connected Network, FC）のクラスを定義．\n",
        "任意の層数 L を持ち，各層のユニット数は h で指定．\n",
        "活性化関数 act は任意に指定可能で，バイアス項の有無も指定可能．\n",
        "\"\"\"\n",
        "\n",
        "class FC(nn.Module):\n",
        "    def __init__(self, d, h, L, act, bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # ネットワークの初期化\n",
        "        hh = d  # 入力の次元数\n",
        "        for i in range(L):\n",
        "            # 隠れ層の重み行列を正規分布で初期化\n",
        "            W = torch.randn(h, hh)\n",
        "\n",
        "            # メモリ効率を考慮し，重み行列を部分行列に分割して ParameterList に格納\n",
        "            # next two line are here to avoid memory issue when computing the kerne\n",
        "            n = max(1, 128 * 256 // hh)  # 分割サイズを計算\n",
        "            W = nn.ParameterList([nn.Parameter(W[j: j+n]) for j in range(0, len(W), n)])\n",
        "\n",
        "            # 分割した重み行列をレイヤーとして登録\n",
        "            setattr(self, \"W{}\".format(i), W)\n",
        "\n",
        "            # バイアス項が指定されている場合は，それをゼロで初期化して登録\n",
        "            if bias:\n",
        "                self.register_parameter(\"B{}\".format(i), nn.Parameter(torch.zeros(h)))\n",
        "\n",
        "            # 次のレイヤーの入力次元は現在の隠れ層のユニット数になる\n",
        "            hh = h\n",
        "\n",
        "        # 最終層の重み行列を初期化（出力がスカラー値なので次元は (1, h)）\n",
        "        self.register_parameter(\"W{}\".format(L), nn.Parameter(torch.randn(1, hh)))\n",
        "\n",
        "        # バイアス項が指定されている場合は，最終層のバイアスをゼロで初期化\n",
        "        if bias:\n",
        "            self.register_parameter(\"B{}\".format(L), nn.Parameter(torch.zeros(1)))\n",
        "\n",
        "        # クラス変数としてレイヤー数，活性化関数，バイアスの有無を保持\n",
        "        self.L = L\n",
        "        self.act = act\n",
        "        self.bias = bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 順伝播計算\n",
        "        for i in range(self.L + 1):\n",
        "            # i 番目の層の重み行列を取得\n",
        "            W = getattr(self, \"W{}\".format(i))\n",
        "\n",
        "            # ParameterList 形式の重み行列をフルの行列に結合\n",
        "            if isinstance(W, nn.ParameterList):\n",
        "                W = torch.cat(list(W))\n",
        "\n",
        "            # バイアス項が指定されている場合は，バイアスを取得\n",
        "            if self.bias:\n",
        "                B = self.bias * getattr(self, \"B{}\".format(i))\n",
        "            else:\n",
        "                B = 0\n",
        "\n",
        "            # 現在の入力の次元数を取得\n",
        "            h = x.size(1)\n",
        "\n",
        "            if i < self.L:\n",
        "                # 隠れ層での線形変換とスケーリング，そして活性化関数の適用\n",
        "                x = x @ (W.t() / h ** 0.5)  # 重み行列との積（次元スケーリング）\n",
        "                x = self.act(x + B)  # バイアス項を加えた後，活性化関数を適用\n",
        "            else:\n",
        "                # 最終層での線形変換（出力はスカラー値）\n",
        "                x = x @ (W.t() / h ** 0.5) + B  # スカラー出力\n",
        "\n",
        "        # 出力を 1 次元のテンソルに変換して返す\n",
        "        return x.view(-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = FC(d=10, h=100, L=2, act=nn.Sigmoid(), bias=False)"
      ],
      "metadata": {
        "id": "TcRKOJ_1b7FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b.forward(x=torch.randn(5, 10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnfQJymKcLsk",
        "outputId": "bc748235-f8c5-4a5d-96c2-e59006a6b621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0983, 0.0122, 0.1126, 0.0241, 0.0249], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b.W0[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vUNhZPvih83",
        "outputId": "cc541fe6-1195-4083-cb70-daf6c73e9a4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ダイナミクス"
      ],
      "metadata": {
        "id": "1RilangZmSpN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kgfnwRl4udyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Dynamics that compares the angle of the gradient between steps and keep it small\n",
        "\n",
        "- マージンに達したときに停止\n",
        "\n",
        "2つの実装：\n",
        "1. `train_regular` - 任意のモデルに対応\n",
        "2. `train_kernel` - 線形モデル専用\n",
        "\"\"\"\n",
        "\n",
        "import copy\n",
        "import itertools\n",
        "import math\n",
        "from time import perf_counter\n",
        "import torch\n",
        "\n",
        "\n",
        "def gradient(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False):\n",
        "    '''\n",
        "    `outputs` に対する `inputs` の勾配を計算する関数\n",
        "    使用例:\n",
        "    ```\n",
        "    gradient(x.sum(), x)          # x の合計に対する勾配\n",
        "    gradient((x * y).sum(), [x, y])  # x と y の要素ごとの積の合計に対する勾配\n",
        "    ```\n",
        "\n",
        "    :param outputs: 勾配を計算する対象の出力テンソル\n",
        "    :param inputs: 勾配を計算したい入力テンソルのリストまたは単一テンソル\n",
        "    :param grad_outputs: 出力テンソルの勾配を指定するためのオプション（通常は None で良い）\n",
        "    :param retain_graph: 計算グラフを保持するかどうかを指定するフラグ（デフォルトは None）\n",
        "    :param create_graph: 勾配の計算グラフを作成するかどうかを指定するフラグ（デフォルトは False）\n",
        "    :return: 入力テンソルに対する勾配をフラットなテンソルとして返す\n",
        "    '''\n",
        "\n",
        "    # `inputs` がテンソルの場合はリストに変換\n",
        "    if torch.is_tensor(inputs):\n",
        "        inputs = [inputs]\n",
        "    else:\n",
        "        inputs = list(inputs)\n",
        "\n",
        "    # `torch.autograd.grad` 関数を使用して勾配を計算\n",
        "    grads = torch.autograd.grad(outputs, inputs, grad_outputs,\n",
        "                                allow_unused=True, # 計算に使用されないテンソルには勾配が計算されない\n",
        "                                retain_graph=retain_graph, # 計算グラフを保持するかどうか\n",
        "                                create_graph=create_graph) # 勾配の計算グラフを作成するかどうか\n",
        "\n",
        "    # 勾配が None の場合は，同じサイズのゼロテンソルを代わりに使用\n",
        "    grads = [x if x is not None else torch.zeros_like(y) for x, y in zip(grads, inputs)]\n",
        "\n",
        "    # 勾配テンソルをフラットな形状に変換して連結\n",
        "    return torch.cat([x.contiguous().view(-1) for x in grads])\n",
        "\n",
        "\n",
        "def loglinspace(rate, step, end=None):\n",
        "    \"\"\"\n",
        "    対数線形間隔での数値を生成するジェネレーター関数\n",
        "    対数的に変化する間隔で数値を生成\n",
        "\n",
        "    `rate` と `step` のパラメータを使って，新しい値を計算\n",
        "    `end` が指定されていない場合は無限に数値を生成\n",
        "\n",
        "    Arguments:\n",
        "        rate (float): 対数的な変化の速度を制御するパラメータ\n",
        "        step (float): 各ステップでの間隔の大きさ\n",
        "        end (float, optional): 生成を停止する条件となる最大値．指定されない場合は無限に生成\n",
        "\n",
        "    Yields:\n",
        "        float: 現在の時間 `t` の値を生成\n",
        "    \"\"\"\n",
        "    t = 0\n",
        "    while end is None or t <= end:\n",
        "        yield t  # 現在の時間 `t` の値を生成\n",
        "        # 次の `t` を計算．ここで，`math.exp(-t * rate / step)` は指数関数的な減衰を表す\n",
        "        t = int(t + 1 + step * (1 - math.exp(-t * rate / step)))\n",
        "\n",
        "\n",
        "class ContinuousMomentum(torch.optim.Optimizer):\n",
        "    \"\"\"連続的なモーメンタムを実装\n",
        "\n",
        "    - d/dt velocity = -1/tau (velocity + grad)\n",
        "    - または\n",
        "    - d/dt velocity = -mu/t (velocity + grad)\n",
        "\n",
        "    - d/dt parameters = velocity\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, dt, tau):\n",
        "        \"\"\"\n",
        "        初期化メソッド\n",
        "\n",
        "        Arguments:\n",
        "            params (iterable): 最適化するパラメータのリスト\n",
        "            dt (float): 時間ステップのサイズ\n",
        "            tau (float): モーメンタムのタイムコンスタント\n",
        "        \"\"\"\n",
        "        defaults = dict(dt=dt, tau=tau)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"単一の最適化ステップを実行\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): モデルを再評価し，損失を返すクロージャ．多くの最適化器にはオプショナル．\n",
        "\n",
        "        Returns:\n",
        "            loss (Tensor or None): 損失の値．クロージャが指定された場合はその損失値を返す．\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            tau = group['tau']\n",
        "            dt = group['dt']\n",
        "\n",
        "            for p in group['params']:\n",
        "                # 勾配がないパラメータはスキップ\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                param_state = self.state[p]\n",
        "                # パラメータの状態が初めてのときは，時間 t を 0 に設定\n",
        "                if 't' not in param_state:\n",
        "                    t = param_state['t'] = 0\n",
        "                else:\n",
        "                    t = param_state['t']\n",
        "\n",
        "                # モーメンタムの状態（速度）を初期化\n",
        "                if tau != 0:\n",
        "                    if 'velocity' not in param_state:\n",
        "                        v = param_state['velocity'] = torch.zeros_like(p.data)\n",
        "                    else:\n",
        "                        v = param_state['velocity']\n",
        "\n",
        "                # モーメンタムの計算\n",
        "                if tau > 0:\n",
        "                    # tau > 0 の場合の連続モーメンタムの計算\n",
        "                    x = math.exp(-dt / tau)  # 時間の経過とともに減衰する係数\n",
        "                    v.mul_(x).add_(-(1 - x), p.grad.data) # 速度に勾配を加える\n",
        "                elif tau < 0:\n",
        "                    # tau < 0 の場合の連続モーメンタムの計算\n",
        "                    mu = -tau\n",
        "                    x = (t / (t + dt)) ** mu  # 時間の経過に伴う係数\n",
        "                    v.mul_(x).add_(-(1 - x), p.grad.data)  # 速度に勾配を加える\n",
        "                else:\n",
        "                    # tau = 0 の場合のシンプルな勾配降下\n",
        "                    v = -p.grad.data\n",
        "\n",
        "                # パラメータの更新\n",
        "                p.data.add_(dt, v)    # パラメータに速度を加える\n",
        "                param_state['t'] += dt    # 時間を進める\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "def make_step(f, optimizer, dt, grad):\n",
        "    \"\"\"\n",
        "    指定された勾配 `grad` を使用して，最適化ステップを実行\n",
        "\n",
        "    Arguments:\n",
        "        f (torch.nn.Module): トレーニングするモデル\n",
        "        optimizer (torch.optim.Optimizer): 使用する最適化器\n",
        "        dt (float): 時間刻み\n",
        "        grad (torch.Tensor): 勾配テンソル\n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    # モデルの全パラメータに対してループ\n",
        "    for p in f.parameters():\n",
        "        # パラメータの総要素数を取得\n",
        "        n = p.numel()\n",
        "        # 勾配テンソルを対応するパラメータに合わせてリシェイプし，割り当て\n",
        "        p.grad = grad[i: i + n].view_as(p)\n",
        "        i += n  # インデックスを次のパラメータに進める\n",
        "\n",
        "    # 各パラメータグループに対して時間刻み `dt` を設定\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['dt'] = dt\n",
        "\n",
        "    # 最適化ステップを実行\n",
        "    optimizer.step()\n",
        "\n",
        "    # 勾配をリセット（計算グラフから切り離し）\n",
        "    for p in f.parameters():\n",
        "        p.grad = None\n",
        "\n",
        "\n",
        "def train_regular(f0, x, y, tau, max_walltime, alpha, loss, subf0, max_dgrad=math.inf, max_dout=math.inf):\n",
        "    \"\"\"\n",
        "    一般的なモデルのトレーニング関数\n",
        "\n",
        "    Arguments:\n",
        "        f0 (torch.nn.Module): 初期モデル\n",
        "        x (torch.Tensor): 入力データ\n",
        "        y (torch.Tensor): 出力ラベル\n",
        "        tau (float): モーメンタムパラメータ\n",
        "        max_walltime (float): 最大経過時間\n",
        "        alpha (float): 出力変化に対する閾値\n",
        "        loss (callable): ロス関数\n",
        "        subf0 (bool): 初期モデルの出力を使用するかどうか\n",
        "        max_dgrad (float): 勾配の変化の最大許容値\n",
        "        max_dout (float): 出力の変化の最大許容値\n",
        "    \"\"\"\n",
        "\n",
        "    # 初期モデルのコピーを作成\n",
        "    f = copy.deepcopy(f0)\n",
        "\n",
        "    # モデルの出力を計算（必要に応じて初期モデルの出力を使用）\n",
        "    with torch.no_grad():\n",
        "        out0 = f0(x) if subf0 else 0\n",
        "\n",
        "    # 時間刻みとモーメンタムのパラメータを初期化\n",
        "    dt = 1\n",
        "    step_change_dt = 0\n",
        "\n",
        "    # ContinuousMomentum オプティマイザを初期化\n",
        "    optimizer = ContinuousMomentum(f.parameters(), dt=dt, tau=tau)\n",
        "\n",
        "    # ログ-線形間隔のチェックポイント生成器を作成\n",
        "    checkpoint_generator = loglinspace(0.01, 100)\n",
        "    checkpoint = next(checkpoint_generator)\n",
        "\n",
        "    # 経過時間を計測するためのタイマーを開始\n",
        "    wall = perf_counter()\n",
        "    t = 0\n",
        "    converged = False\n",
        "\n",
        "    # 最初のモデルの出力と勾配を計算\n",
        "    out = f(x)\n",
        "    grad = gradient(loss((out - out0) * y).mean(), f.parameters())\n",
        "\n",
        "    # トレーニングループ\n",
        "    for step in itertools.count():\n",
        "        # 現在のモデルとオプティマイザの状態を保存\n",
        "        state = copy.deepcopy((f.state_dict(), optimizer.state_dict(), t))\n",
        "\n",
        "        while True:\n",
        "            # 最適化ステップを実行\n",
        "            make_step(f, optimizer, dt, grad)\n",
        "            t += dt\n",
        "            current_dt = dt\n",
        "\n",
        "            # モデルの新しい出力と勾配を計算\n",
        "            new_out = f(x)\n",
        "            new_grad = gradient(loss((new_out - out0) * y).mean(), f.parameters())\n",
        "\n",
        "            # 出力の変化量を計算\n",
        "            dout = (out - new_out).mul(alpha).abs().max().item()\n",
        "\n",
        "            # 勾配の変化量を計算\n",
        "            if grad.norm() == 0 or new_grad.norm() == 0:\n",
        "                dgrad = 0\n",
        "            else:\n",
        "                dgrad = (grad - new_grad).norm().pow(2).div(grad.norm() * new_grad.norm()).item()\n",
        "\n",
        "            # 勾配の変化量と出力の変化量が閾値以下であれば，時間刻みを増加\n",
        "            if dgrad < max_dgrad and dout < max_dout:\n",
        "                if dgrad < 0.5 * max_dgrad and dout < 0.5 * max_dout:\n",
        "                    dt *= 1.1\n",
        "                break\n",
        "            # そうでない場合は，時間刻みを減少\n",
        "            dt /= 10\n",
        "\n",
        "            # 現在の状態を出力\n",
        "            print(\"[{} +{}] [dt={:.1e} dgrad={:.1e} dout={:.1e}]\".format(step, step - step_change_dt, dt, dgrad, dout), flush=True)\n",
        "\n",
        "            # モデルとオプティマイザの状態をリストア\n",
        "            step_change_dt = step\n",
        "            f.load_state_dict(state[0])\n",
        "            optimizer.load_state_dict(state[1])\n",
        "            t = state[2]\n",
        "\n",
        "        # 新しい出力と勾配を保存\n",
        "        out = new_out\n",
        "        grad = new_grad\n",
        "\n",
        "        save = False\n",
        "\n",
        "        # チェックポイントに達した場合，または収束した場合に状態を保存\n",
        "        if step == checkpoint:\n",
        "            checkpoint = next(checkpoint_generator)\n",
        "            assert checkpoint > step\n",
        "            save = True\n",
        "\n",
        "        # 出力が閾値を超え，収束していない場合に収束を宣言\n",
        "        if (alpha * (out - out0) * y >= 1).all() and not converged:\n",
        "            converged = True\n",
        "            save = True\n",
        "\n",
        "        if save:\n",
        "            state = {\n",
        "                'step': step,\n",
        "                'wall': perf_counter() - wall,\n",
        "                't': t,\n",
        "                'dt': current_dt,\n",
        "                'dgrad': dgrad,\n",
        "                'dout': dout,\n",
        "                'norm': sum(p.norm().pow(2) for p in f.parameters()).sqrt().item(),\n",
        "                'dnorm': sum((p0 - p).norm().pow(2) for p0, p in zip(f0.parameters(), f.parameters())).sqrt().item(),\n",
        "                'grad_norm': grad.norm().item(),\n",
        "            }\n",
        "\n",
        "            yield f, state, converged\n",
        "\n",
        "        # 収束した場合はトレーニングを終了\n",
        "        if converged:\n",
        "            break\n",
        "\n",
        "        # 経過時間が最大経過時間を超えた場合にトレーニングを終了\n",
        "        if perf_counter() > wall + max_walltime:\n",
        "            break\n",
        "\n",
        "        # 出力に NaN が含まれている場合，トレーニングを終了\n",
        "        if torch.isnan(out).any():\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "def train_kernel(ktrtr, ytr, tau, max_walltime, alpha, loss_prim, max_dgrad=math.inf, max_dout=math.inf):\n",
        "    \"\"\"\n",
        "    線形モデル専用のトレーニング関数\n",
        "\n",
        "    Arguments:\n",
        "        ktrtr (torch.Tensor): カーネル行列\n",
        "        ytr (torch.Tensor): 出力ラベル\n",
        "        tau (float): モーメンタムパラメータ\n",
        "        max_walltime (float): 最大経過時間\n",
        "        alpha (float): 出力変化に対する閾値\n",
        "        loss_prim (callable): プライムロス関数\n",
        "        max_dgrad (float): 勾配の変化の最大許容値\n",
        "        max_dout (float): 出力の変化の最大許容値\n",
        "    \"\"\"\n",
        "    # 初期出力と速度ベクトルをゼロで初期化\n",
        "    otr = ktrtr.new_zeros(len(ytr))\n",
        "    velo = otr.clone()\n",
        "\n",
        "    # 時間刻みとステップ変更のタイミングを初期化\n",
        "    dt = 1\n",
        "    step_change_dt = 0\n",
        "\n",
        "    # ログ-線形間隔のチェックポイント生成器を作成\n",
        "    checkpoint_generator = loglinspace(0.01, 100)\n",
        "    checkpoint = next(checkpoint_generator)\n",
        "\n",
        "    # 経過時間を計測するためのタイマーを開始\n",
        "    wall = perf_counter()\n",
        "    t = 0\n",
        "    converged = False\n",
        "\n",
        "    # 初期のプライムロスを計算\n",
        "    lprim = loss_prim(otr * ytr) * ytr\n",
        "    grad = ktrtr @ lprim / len(ytr)\n",
        "\n",
        "    # トレーニングループ\n",
        "    for step in itertools.count():\n",
        "\n",
        "        # 現在の出力，速度，時間を保存\n",
        "        state = copy.deepcopy((otr, velo, t))\n",
        "\n",
        "        while True:\n",
        "            # モーメンタムパラメータに基づいて速度を更新\n",
        "            if tau > 0:\n",
        "                x = math.exp(-dt / tau)\n",
        "                velo.mul_(x).add_(-(1 - x), grad)\n",
        "            elif tau < 0:\n",
        "                mu = -tau\n",
        "                x = (t / (t + dt)) ** mu\n",
        "                velo.mul_(x).add_(-(1 - x), grad)\n",
        "            else:\n",
        "                velo.copy_(-grad)\n",
        "\n",
        "            # 出力を更新\n",
        "            otr.add_(dt, velo)\n",
        "\n",
        "            t += dt\n",
        "            current_dt = dt\n",
        "\n",
        "            # 新しいプライムロスを計算し，勾配を更新\n",
        "            lprim = loss_prim(otr * ytr) * ytr\n",
        "            new_grad = ktrtr @ lprim / len(ytr)\n",
        "\n",
        "            # 出力の変化量を計算\n",
        "            dout = velo.mul(dt * alpha).abs().max().item()\n",
        "\n",
        "            # 勾配の変化量を計算\n",
        "            if grad.norm() == 0 or new_grad.norm() == 0:\n",
        "                dgrad = 0\n",
        "            else:\n",
        "                dgrad = (grad - new_grad).norm().pow(2).div(grad.norm() * new_grad.norm()).item()\n",
        "\n",
        "            # 勾配と出力の変化量が許容範囲内であれば，時間刻みを増加\n",
        "            if dgrad < max_dgrad and dout < max_dout:\n",
        "                if dgrad < 0.1 * max_dgrad and dout < 0.1 * max_dout:\n",
        "                    dt *= 1.1\n",
        "                break\n",
        "\n",
        "            # そうでない場合は，時間刻みを減少\n",
        "            dt /= 10\n",
        "\n",
        "            # 現在の状態を出力\n",
        "            print(\"[{} +{}] [dt={:.1e} dgrad={:.1e} dout={:.1e}]\".format(step, step - step_change_dt, dt, dgrad, dout), flush=True)\n",
        "            step_change_dt = step\n",
        "\n",
        "            # 保存した状態をリストア\n",
        "            otr.copy_(state[0])\n",
        "            velo.copy_(state[1])\n",
        "            t = state[2]\n",
        "\n",
        "        # 勾配を新しい値で更新\n",
        "        grad = new_grad\n",
        "\n",
        "        save = False\n",
        "\n",
        "        # チェックポイントに達した場合，または収束した場合に状態を保存\n",
        "        if step == checkpoint:\n",
        "            checkpoint = next(checkpoint_generator)\n",
        "            assert checkpoint > step\n",
        "            save = True\n",
        "\n",
        "        # 出力が閾値を超え，収束していない場合に収束を宣言\n",
        "        if (alpha * otr * ytr >= 1).all() and not converged:\n",
        "            converged = True\n",
        "            save = True\n",
        "\n",
        "        if save:\n",
        "            state = {\n",
        "                'step': step,\n",
        "                'wall': perf_counter() - wall,\n",
        "                't': t,\n",
        "                'dt': current_dt,\n",
        "                'dgrad': dgrad,\n",
        "                'dout': dout,\n",
        "                'grad_norm': grad.norm().item(),\n",
        "            }\n",
        "\n",
        "            yield otr, velo, grad, state, converged\n",
        "\n",
        "        # 収束した場合はトレーニングを終了\n",
        "        if converged:\n",
        "            break\n",
        "\n",
        "        # 経過時間が最大経過時間を超えた場合にトレーニングを終了\n",
        "        if perf_counter() > wall + max_walltime:\n",
        "            break\n",
        "\n",
        "        # 出力に NaN が含まれている場合，トレーニングを終了\n",
        "        if torch.isnan(otr).any():\n",
        "            break"
      ],
      "metadata": {
        "id": "gJZk7JSQdxx7"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# カーネル"
      ],
      "metadata": {
        "id": "YqlHUas5yfew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`compute_kernels` 関数は，与えられたモデル $ f $ の入力データ $ x_{\\text{tr}} $ と $ x_{\\text{te}} $ に基づいて，カーネル行列（Gram 行列）を計算．\n",
        "\n",
        "- $ K_{\\text{trtr}} $：トレーニングデータ間のカーネル行列\n",
        "- $ K_{\\text{tetr}} $：テストデータとトレーニングデータ間のカーネル行列\n",
        "- $ K_{\\text{tete}} $：テストデータ間のカーネル行列\n",
        "\n",
        "これらの行列は，モデルのパラメータに関する勾配を使って計算．\n",
        "\n",
        "<br>\n",
        "\n",
        "### モデルの勾配計算\n",
        "\n",
        "モデル $ f $ が入力 $ x $ に対して出力を生成し，その勾配を計算．勾配の計算は以下のように行う：\n",
        "\n",
        "1. 入力 $ x $ に対するモデルの出力を $ f(x) $ とする．\n",
        "2. この出力に対するパラメータ $ \\theta $ の勾配を求める：$ \\nabla_\\theta f(x) $\n",
        "\n",
        "  ここで，$ \\nabla_\\theta f(x) $ は $ x $ に対する勾配であり，モデルのパラメータ $ \\theta $ に関する勾配ベクトル．\n",
        "\n",
        "<br>\n",
        "\n",
        "### Gram 行列の計算\n",
        "\n",
        "1. $ K_{\\text{trtr}} $\n",
        "\n",
        "  トレーニングデータ $ x_{\\text{tr}} $ に対するカーネル行列 $ K_{\\text{trtr}} $ は，各トレーニングデータポイント $ x_i $ と $ x_j $ の勾配ベクトルに基づいて計算：\n",
        "\n",
        "  $ K_{\\text{trtr}} = J_{\\text{tr}} J_{\\text{tr}}^T $\n",
        "\n",
        "  ここで，$ J_{\\text{tr}} $ はトレーニングデータ $ x_{\\text{tr}} $ に対する勾配ベクトルを列に持つ行列．$ J_{\\text{tr}} $ の $ i $-th 行は，入力 $ x_i $ に対する勾配ベクトル．\n",
        "\n",
        "<br>\n",
        "\n",
        "2. $ K_{\\text{tetr}} $\n",
        "\n",
        "  テストデータ $ x_{\\text{te}} $ とトレーニングデータ $ x_{\\text{tr}} $ とのカーネル行列 $ K_{\\text{tetr}} $ は次のように計算：\n",
        "\n",
        "  $ K_{\\text{tetr}} = J_{\\text{te}} J_{\\text{tr}}^T $\n",
        "\n",
        "  ここで，$ J_{\\text{te}} $ はテストデータ $ x_{\\text{te}} $ に対する勾配ベクトルを列に持つ行列．\n",
        "\n",
        "<br>\n",
        "\n",
        "3. $ K_{\\text{tete}} $\n",
        "\n",
        "  テストデータ $ x_{\\text{te}} $ に対するカーネル行列 $ K_{\\text{tete}} $ は、テストデータポイント $ x_i $ と $ x_j $ の勾配ベクトルに基づいて計算：\n",
        "\n",
        "  $ K_{\\text{tete}} = J_{\\text{te}} J_{\\text{te}}^T $\n",
        "\n",
        "  ここで，$ J_{\\text{te}} $ の $ i $-th 行は，入力 $ x_i $ に対する勾配ベクトル．"
      ],
      "metadata": {
        "id": "iL4Pua0dm4DN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Computes the Gram matrix of a given model\n",
        "\"\"\"\n",
        "\n",
        "def compute_kernels(f, xtr, xte):\n",
        "    # from hessian import gradient\n",
        "\n",
        "    # 新しいゼロ行列を作成\n",
        "    # ktrtr: トレーニングデータ間のカーネル行列\n",
        "    # ktetr: テストデータとトレーニングデータ間のカーネル行列\n",
        "    # ktete: テストデータ間のカーネル行列\n",
        "    ktrtr = xtr.new_zeros(len(xtr), len(xtr))\n",
        "    ktetr = xtr.new_zeros(len(xte), len(xtr))\n",
        "    ktete = xtr.new_zeros(len(xte), len(xte))\n",
        "\n",
        "    params = []\n",
        "    current = []\n",
        "\n",
        "    # モデルのパラメータをサイズで降順にソートし，メモリ制限に基づいて分割\n",
        "    for p in sorted(f.parameters(), key=lambda p: p.numel(), reverse=True):\n",
        "        current.append(p)\n",
        "        # メモリ制限に基づき，パラメータを分割\n",
        "        if sum(p.numel() for p in current) > 2e9 // (8 * (len(xtr) + len(xte))):\n",
        "            if len(current) > 1:\n",
        "                params.append(current[:-1])\n",
        "                current = current[-1:]\n",
        "            else:\n",
        "                params.append(current)\n",
        "                current = []\n",
        "    if len(current) > 0:\n",
        "        params.append(current)\n",
        "\n",
        "    # 各パラメータグループについてカーネル行列を計算\n",
        "    for i, p in enumerate(params):\n",
        "        print(\"[{}/{}] [len={} numel={}]\".format(i, len(params), len(p), sum(x.numel() for x in p)), flush=True)\n",
        "\n",
        "        # 勾配行列を初期化\n",
        "        jtr = xtr.new_empty(len(xtr), sum(u.numel() for u in p))  # (P, N~) # (トレーニングデータ数, パラメータ数の合計)\n",
        "        jte = xte.new_empty(len(xte), sum(u.numel() for u in p))  # (P, N~) # (テストデータ数, パラメータ数の合計)\n",
        "\n",
        "        # トレーニングデータに対する勾配行列を計算\n",
        "        for j, x in enumerate(xtr):\n",
        "            jtr[j] = gradient(f(x[None]), p)  # (N~) # (パラメータ数の合計)\n",
        "\n",
        "        # テストデータに対する勾配行列を計算\n",
        "        for j, x in enumerate(xte):\n",
        "            jte[j] = gradient(f(x[None]), p)  # (N~) # (パラメータ数の合計)\n",
        "\n",
        "        # カーネル行列を更新\n",
        "        ktrtr.add_(jtr @ jtr.t())  # トレーニングデータ間のカーネル行列\n",
        "        ktetr.add_(jte @ jtr.t())  # テストデータとトレーニングデータ間のカーネル行列\n",
        "        ktete.add_(jte @ jte.t())  # テストデータ間のカーネル行列\n",
        "        del jtr, jte  # 不要になった勾配行列を削除\n",
        "\n",
        "    return ktrtr, ktetr, ktete"
      ],
      "metadata": {
        "id": "KofuVIsHm5Qf"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 損失関連"
      ],
      "metadata": {
        "id": "HquikxBO7SjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "def loss_func(hyper, fy):\n",
        "    \"\"\"\n",
        "    損失関数を計算．指定された損失関数のタイプに応じて異なる計算を行う．\n",
        "\n",
        "    Parameters:\n",
        "    - hyper: 設定を含むオブジェクト\n",
        "    - fy: モデルの予測値と実際のラベルを用いた計算結果\n",
        "\n",
        "    Returns:\n",
        "    - 損失値: 指定された損失関数に基づいて計算された損失\n",
        "    \"\"\"\n",
        "    if hyper.loss == 'softhinge':\n",
        "        sp = partial(torch.nn.functional.softplus, beta=hyper.lossbeta)\n",
        "        # 損失を計算し，alpha で正規化\n",
        "        return sp(1 - hyper.alpha * fy) / hyper.alpha\n",
        "    if hyper.loss == 'qhinge':\n",
        "        return 0.5 * (1 - hyper.alpha * fy).relu().pow(2) / hyper.alpha\n",
        "\n",
        "\n",
        "def loss_func_prime(hyper, fy):\n",
        "    \"\"\"\n",
        "    損失関数の勾配（微分）を計算．指定された損失関数のタイプに応じて異なる計算を行う．\n",
        "\n",
        "    Parameters:\n",
        "    - hyper: 設定を含むオブジェクト\n",
        "    - fy: モデルの予測値と実際のラベルを用いた計算結果\n",
        "\n",
        "    Returns:\n",
        "    - 損失の勾配: 損失関数の微分\n",
        "    \"\"\"\n",
        "    if hyper.loss == 'softhinge':\n",
        "        return -torch.sigmoid(hyper.lossbeta * (1 - hyper.alpha * fy)).mul(hyper.lossbeta)\n",
        "    if hyper.loss == 'qhinge':\n",
        "        return -(1 - hyper.alpha * fy).relu()"
      ],
      "metadata": {
        "id": "2WKWCbZI7RxG"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run_kernel と run_regular"
      ],
      "metadata": {
        "id": "lvM2H10K73Ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_kernel(hyper, ktrtr, ktetr, ktete, f, xtr, ytr, xte, yte):\n",
        "    \"\"\"\n",
        "    カーネル回帰の実行と評価を行う関数．トレーニングデータに対するカーネル回帰を実行し，トレーニングとテストデータに対する出力を計算\n",
        "\n",
        "    Parameters:\n",
        "    - hyper: 設定やパラメータが格納されたオブジェクト\n",
        "    - ktrtr: トレーニングデータに対するカーネル行列\n",
        "    - ktetr: トレーニングデータとテストデータ間のカーネル行列\n",
        "    - ktete: テストデータに対するカーネル行列\n",
        "    - f: モデル関数\n",
        "    - xtr: トレーニングデータの入力\n",
        "    - ytr: トレーニングデータのラベル\n",
        "    - xte: テストデータの入力\n",
        "    - yte: テストデータのラベル\n",
        "\n",
        "    Returns:\n",
        "    - out: トレーニングおよびテストの結果とカーネル行列の統計量を含む辞書\n",
        "    \"\"\"\n",
        "\n",
        "    # 引数の f0 が 1 であることを確認\n",
        "    assert hyper.f0 == 1\n",
        "\n",
        "    # 結果を保存するためのリストを初期化\n",
        "    dynamics = []\n",
        "\n",
        "    # 時間スケール tau を設定\n",
        "    tau = hyper.tau_over_h * hyper.h\n",
        "    # alpha に基づいて tau を調整\n",
        "    if hyper.tau_alpha_crit is not None:\n",
        "        tau *= min(1, hyper.tau_alpha_crit / hyper.alpha)\n",
        "\n",
        "    # カーネル回帰のトレーニングを実行し，各ステップの状態を取得\n",
        "    for otr, _velo, _grad, state, _converged in train_kernel(ktrtr, ytr, tau, hyper.train_time, hyper.alpha, partial(loss_func_prime, hyper), hyper.max_dgrad, hyper.max_dout):\n",
        "\n",
        "        # トレーニングの結果を状態に保存\n",
        "        state['train'] = {\n",
        "            'loss': loss_func(hyper, otr * ytr).mean().item(),\n",
        "            'aloss': hyper.alpha * loss_func(hyper, otr * ytr).mean().item(),\n",
        "            'err': (otr * ytr <= 0).double().mean().item(),\n",
        "            'nd': (hyper.alpha * otr * ytr < 1).long().sum().item(),\n",
        "            'dfnorm': otr.pow(2).mean().sqrt(),\n",
        "            'outputs': otr if hyper.save_outputs else None,\n",
        "            'labels': ytr if hyper.save_outputs else None,\n",
        "        }\n",
        "\n",
        "        print(\"[i={d[step]:d} t={d[t]:.2e} wall={d[wall]:.0f}] [dt={d[dt]:.1e} dgrad={d[dgrad]:.1e} dout={d[dout]:.1e}] [train aL={d[train][aloss]:.2e} err={d[train][err]:.2f} nd={d[train][nd]}]\".format(d=state), flush=True)\n",
        "        dynamics.append(state)\n",
        "\n",
        "    # トレーニングデータに対するカーネル回帰の係数を計算\n",
        "    c = torch.lstsq(otr.view(-1, 1), ktrtr).solution.flatten()\n",
        "\n",
        "    # テストデータの出力を計算\n",
        "    if len(xte) > len(xtr):\n",
        "        # from hessian import gradient\n",
        "        # テストデータがトレーニングデータより多い場合，勾配を用いて出力を計算\n",
        "        a = gradient(f(xtr) @ c, f.parameters())\n",
        "        ote = torch.stack([gradient(f(x[None]), f.parameters()) @ a for x in xte])\n",
        "    else:\n",
        "        # それ以外の場合，事前に計算されたカーネル行列を使用\n",
        "        ote = ktetr @ c\n",
        "\n",
        "    # 結果を辞書としてまとめて返す\n",
        "    out = {\n",
        "        'dynamics': dynamics,\n",
        "        'train': {\n",
        "            'outputs': otr,\n",
        "            'labels': ytr,\n",
        "        },\n",
        "        'test': {\n",
        "            'outputs': ote,\n",
        "            'labels': yte,\n",
        "        },\n",
        "        'kernel': {\n",
        "            'train': {\n",
        "                'value': ktrtr.cpu() if hyper.store_kernel == 1 else None,\n",
        "                'diag': ktrtr.diag(),\n",
        "                'mean': ktrtr.mean(),\n",
        "                'std': ktrtr.std(),\n",
        "                'norm': ktrtr.norm(),\n",
        "            },\n",
        "            'test': {\n",
        "                'value': ktete.cpu() if hyper.store_kernel == 1 else None,\n",
        "                'diag': ktete.diag(),\n",
        "                'mean': ktete.mean(),\n",
        "                'std': ktete.std(),\n",
        "                'norm': ktete.norm(),\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def run_regular(hyper, f0, xtr, ytr, xte, yte):\n",
        "    \"\"\"\n",
        "    通常のトレーニングプロセスを実行し，トレーニングとテストの結果を収集する関数\n",
        "\n",
        "    Parameters:\n",
        "    - hyper: 設定やパラメータが格納されたオブジェクト\n",
        "    - f0: 初期モデル関数\n",
        "    - xtr: トレーニングデータの入力\n",
        "    - ytr: トレーニングデータのラベル\n",
        "    - xte: テストデータの入力\n",
        "    - yte: テストデータのラベル\n",
        "\n",
        "    Yields:\n",
        "    - f: トレーニング後のモデル関数\n",
        "    - out: トレーニングとテストの結果を含む辞書\n",
        "    \"\"\"\n",
        "\n",
        "    # 勾配計算を無効にし，初期モデルによる出力を計算\n",
        "    with torch.no_grad():\n",
        "        otr0 = f0(xtr)\n",
        "        ote0 = f0(xte)\n",
        "\n",
        "    # 初期モデルが無い場合は，出力をゼロに設定\n",
        "    if hyper.f0 == 0:\n",
        "        otr0 = torch.zeros_like(otr0)\n",
        "        ote0 = torch.zeros_like(ote0)\n",
        "\n",
        "    # ランダムにサンプルインデックスを選択し，部分データを取得\n",
        "    j = torch.randperm(min(len(xte), len(xtr)))[:10 * hyper.chunk]\n",
        "    ytrj = ytr[j]\n",
        "    ytej = yte[j]\n",
        "\n",
        "    # トレーニング開始時間を記録\n",
        "    t = perf_counter()\n",
        "\n",
        "    # 時間スケール tau を設定\n",
        "    tau = hyper.tau_over_h * hyper.h\n",
        "    if hyper.tau_alpha_crit is not None:\n",
        "        tau *= min(1, hyper.tau_alpha_crit / hyper.alpha)\n",
        "\n",
        "    # トレーニングのダイナミクスを保存するリスト\n",
        "    dynamics = []\n",
        "\n",
        "    # トレーニングを実行し，各ステップの状態を取得\n",
        "    for f, state, done in train_regular(f0, xtr, ytr, tau, hyper.train_time, hyper.alpha, partial(loss_func, hyper), bool(hyper.f0), hyper.max_dgrad, hyper.max_dout):\n",
        "        with torch.no_grad():\n",
        "            # 勾配計算を無効にし，トレーニングとテストデータの出力を計算\n",
        "            otr = f(xtr[j]) - otr0[j]\n",
        "            ote = f(xte[j]) - ote0[j]\n",
        "\n",
        "        # モデルアーキテクチャが 'fc' の場合，重みのノルムを計算\n",
        "        if hyper.arch.split('_')[0] == 'fc':\n",
        "            def getw(f, i):\n",
        "                return torch.cat(list(getattr(f.f, \"W{}\".format(i))))\n",
        "            state['wnorm'] = [getw(f, i).norm().item() for i in range(f.f.L + 1)]\n",
        "            state['dwnorm'] = [(getw(f, i) - getw(f0, i)).norm().item() for i in range(f.f.L + 1)]\n",
        "\n",
        "        # トレーニングデータの損失，エラー，その他の情報を計算\n",
        "        state['train'] = {\n",
        "            'loss': loss_func(hyper, otr * ytrj).mean().item(),\n",
        "            'aloss': hyper.alpha * loss_func(hyper, otr * ytrj).mean().item(),\n",
        "            'err': (otr * ytr[j] <= 0).double().mean().item(),\n",
        "            'nd': (hyper.alpha * otr * ytr[j] < 1).long().sum().item(),\n",
        "            'dfnorm': otr.pow(2).mean().sqrt(),\n",
        "            'fnorm': (otr + otr0[j]).pow(2).mean().sqrt(),\n",
        "            'outputs': otr if hyper.save_outputs else None,\n",
        "            'labels': ytrj if hyper.save_outputs else None,\n",
        "        }\n",
        "\n",
        "        # テストデータの損失，エラー，その他の情報を計算\n",
        "        state['test'] = {\n",
        "            'loss': loss_func(hyper, ote * ytej).mean().item(),\n",
        "            'aloss': hyper.alpha * loss_func(hyper, ote * ytej).mean().item(),\n",
        "            'err': (ote * yte[j] <= 0).double().mean().item(),\n",
        "            'nd': (hyper.alpha * ote * yte[j] < 1).long().sum().item(),\n",
        "            'dfnorm': ote.pow(2).mean().sqrt(),\n",
        "            'fnorm': (ote + ote0[j]).pow(2).mean().sqrt(),\n",
        "            'outputs': ote if hyper.save_outputs else None,\n",
        "            'labels': ytej if hyper.save_outputs else None,\n",
        "        }\n",
        "\n",
        "        print(\"[i={d[step]:d} t={d[t]:.2e} wall={d[wall]:.0f}] [dt={d[dt]:.1e} dgrad={d[dgrad]:.1e} dout={d[dout]:.1e}] [train aL={d[train][aloss]:.2e} err={d[train][err]:.2f} nd={d[train][nd]}/{p}] [test aL={d[test][aloss]:.2e} err={d[test][err]:.2f}]\".format(d=state, p=len(j)), flush=True)\n",
        "\n",
        "        # ダイナミクスリストに現在のステートを追加\n",
        "        dynamics.append(state)\n",
        "\n",
        "        # トレーニングが完了した場合または実行時間が 6000秒 を超えた場合に結果を出力\n",
        "        if done or perf_counter() - t > 6000:\n",
        "            t = perf_counter()\n",
        "\n",
        "            # 勾配計算を無効にし，最終的なトレーニングとテストの出力を計算\n",
        "            with torch.no_grad():\n",
        "                otr = f(xtr) - otr0\n",
        "                ote = f(xte) - ote0\n",
        "\n",
        "            # 結果を辞書としてまとめて返す\n",
        "            out = {\n",
        "                'dynamics': dynamics,\n",
        "                'train': {\n",
        "                    'f0': otr0,\n",
        "                    'outputs': otr,\n",
        "                    'labels': ytr,\n",
        "                },\n",
        "                'test': {\n",
        "                    'f0': ote0,\n",
        "                    'outputs': ote,\n",
        "                    'labels': yte,\n",
        "                }\n",
        "            }\n",
        "            yield f, out"
      ],
      "metadata": {
        "id": "WNUmIC4h71lQ"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# メイン"
      ],
      "metadata": {
        "id": "X3le98ocmswR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "class SplitEval(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    モデルを分割して評価するためのクラス．入力データを指定されたサイズに分割し，それぞれの分割に対してモデルを評価．\n",
        "    最後に，評価結果を結合して返す．\n",
        "\n",
        "    Attributes:\n",
        "    - f: 評価対象のモデル\n",
        "    - size: 入力データを分割するサイズ\n",
        "    \"\"\"\n",
        "    def __init__(self, f, size):\n",
        "        \"\"\"\n",
        "        コンストラクタ．モデルと分割サイズを初期化\n",
        "\n",
        "        Parameters:\n",
        "        - f: 評価対象のモデル\n",
        "        - size: 入力データを分割するサイズ\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.f = f    # モデルを保存\n",
        "        self.size = size    # 分割サイズを保存\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        フォワードパス．入力データを指定されたサイズに分割し，各分割に対してモデルを適用し，結果を結合して返す．\n",
        "\n",
        "        Parameters:\n",
        "        - x: 入力データ（バッチ）\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: 各分割に対してモデルを適用した結果を結合したテンソル\n",
        "        \"\"\"\n",
        "        return torch.cat([self.f(x[i: i + self.size]) for i in range(0, len(x), self.size)])\n",
        "\n",
        "def run_exp(hyper, f0, xtr, ytr, xte, yte):\n",
        "    \"\"\"\n",
        "    実験を実行し，トレーニングとテストの結果を収集する関数\n",
        "\n",
        "    Parameters:\n",
        "    - hyper: 設定やパラメータが格納されたオブジェクト\n",
        "    - f0: 初期モデル関数\n",
        "    - xtr: トレーニングデータの入力\n",
        "    - ytr: トレーニングデータのラベル\n",
        "    - xte: テストデータの入力\n",
        "    - yte: テストデータのラベル\n",
        "\n",
        "    Yields:\n",
        "    - run: 実験の結果を含む辞書\n",
        "    \"\"\"\n",
        "    run = {\n",
        "        'hyper': hyper,\n",
        "        'N': sum(p.numel() for p in f0.parameters()),  # モデルのパラメータの総数\n",
        "    }\n",
        "\n",
        "    if hyper.delta_kernel == 1 or hyper.init_kernel == 1:\n",
        "        init_kernel = compute_kernels(f0, xtr, xte[:len(xtr)])\n",
        "\n",
        "    if hyper.init_kernel == 1:\n",
        "        run['init_kernel'] = run_kernel(hyper, *init_kernel, f0, xtr, ytr, xte, yte)\n",
        "\n",
        "    if hyper.delta_kernel == 1:\n",
        "        init_kernel = (init_kernel[0].cpu(), init_kernel[2].cpu())\n",
        "    elif hyper.init_kernel == 1:\n",
        "        del init_kernel\n",
        "\n",
        "    if hyper.regular == 1:\n",
        "        for f, out in run_regular(hyper, f0, xtr, ytr, xte, yte):\n",
        "            run['regular'] = out\n",
        "            yield run\n",
        "\n",
        "        if hyper.delta_kernel == 1 or hyper.final_kernel == 1:\n",
        "            final_kernel = compute_kernels(f, xtr, xte[:len(xtr)])\n",
        "\n",
        "        if hyper.final_kernel == 1:\n",
        "            run['final_kernel'] = run_kernel(hyper, *final_kernel, f, xtr, ytr, xte, yte)\n",
        "\n",
        "        if hyper.delta_kernel == 1:\n",
        "            final_kernel = (final_kernel[0].cpu(), final_kernel[2].cpu())\n",
        "            run['delta_kernel'] = {\n",
        "                'train': (init_kernel[0] - final_kernel[0]).norm().item(),\n",
        "                'test': (init_kernel[1] - final_kernel[1]).norm().item(),\n",
        "            }\n",
        "\n",
        "    yield run\n",
        "\n",
        "def execute(hyper):\n",
        "    \"\"\"\n",
        "    実験を実行するためのメイン関数．データセットの準備，モデルの設定，実験の実行\n",
        "\n",
        "    Parameters:\n",
        "    - hyper: 実験の設定を含むオブジェクト\n",
        "    \"\"\"\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        if hyper.dtype == 'float64':\n",
        "            torch.set_default_dtype(torch.float64)\n",
        "        elif hyper.dtype == 'float32':\n",
        "            torch.set_default_dtype(torch.float32)\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"CUDA is not available. Running on CPU.\")\n",
        "        if hyper.dtype == 'float64':\n",
        "            torch.set_default_dtype(torch.float64)\n",
        "        elif hyper.dtype == 'float32':\n",
        "            torch.set_default_dtype(torch.float32)\n",
        "\n",
        "    (xtr, ytr), (xte, yte) = get_binary_dataset(hyper.n, hyper.k, hyper.train_size, hyper.test_size, hyper.data_seed, hyper.normalize, device)\n",
        "    xtr = xtr.type(torch.get_default_dtype())\n",
        "    xte = xte.type(torch.get_default_dtype())\n",
        "    ytr = ytr.type(torch.get_default_dtype())\n",
        "    yte = yte.type(torch.get_default_dtype())\n",
        "\n",
        "    torch.manual_seed(hyper.init_seed + hash(hyper.alpha))\n",
        "\n",
        "    arch, act = hyper.arch.split('_')\n",
        "    if act == 'relu':\n",
        "        act = lambda x: 2 ** 0.5 * torch.relu(x)\n",
        "    elif act == 'tanh':\n",
        "        act = torch.tanh\n",
        "    elif act == 'softplus':\n",
        "        factor = torch.nn.functional.softplus(torch.randn(100000, dtype=torch.float64), hyper.spbeta).pow(2).mean().rsqrt().item()\n",
        "        act = lambda x: torch.nn.functional.softplus(x, beta=hyper.spbeta).mul(factor)\n",
        "    else:\n",
        "        raise ValueError('act not specified')\n",
        "\n",
        "    if arch == 'fc':\n",
        "        assert hyper.L is not None\n",
        "        xtr = xtr.flatten(1)\n",
        "        xte = xte.flatten(1)\n",
        "        f = FC(xtr.size(1), hyper.h, hyper.L, act, hyper.bias).to(device)\n",
        "    else:\n",
        "        raise ValueError('arch not specified')\n",
        "\n",
        "    f = SplitEval(f, hyper.chunk)\n",
        "\n",
        "    torch.manual_seed(hyper.batch_seed)\n",
        "\n",
        "    for run in run_exp(hyper, f, xtr, ytr, xte, yte):\n",
        "        yield run\n",
        "\n",
        "class HyperParams:\n",
        "    def __init__(self):\n",
        "        self.device = 'cuda'\n",
        "        self.dtype = 'float64'\n",
        "        self.init_seed = 0\n",
        "        self.batch_seed = 0\n",
        "        self.n = 30\n",
        "        self.k = 3\n",
        "        self.train_size = 700\n",
        "        self.test_size = 400\n",
        "        self.normalize = True\n",
        "        self.data_seed = 0\n",
        "        self.alpha = 1e-4   # [例：1e-4]\n",
        "        self.f0 = 1\n",
        "        self.tau_over_h = 1e-3\n",
        "        self.tau_alpha_crit = 1e3\n",
        "        self.L = 2\n",
        "        self.h = 100\n",
        "        self.arch = 'fc_softplus'\n",
        "        self.spbeta = 5\n",
        "        self.bias = True\n",
        "        self.max_dgrad = 1e-4\n",
        "        self.max_dout = 0.1\n",
        "        self.loss = 'softhinge'\n",
        "        self.lossbeta = 20\n",
        "        self.train_time = 18000\n",
        "        self.chunk = 100\n",
        "        self.init_kernel = 0\n",
        "        self.delta_kernel = 0\n",
        "        self.final_kernel = 0\n",
        "        self.store_kernel = 0\n",
        "        self.save_outputs = 0\n",
        "        self.regular = 1\n",
        "        self.directory = 'F10k3Lsp_h_init'\n",
        "        self.pickle = 'F10k3Lsp_h_init.pickle'\n",
        "\n",
        "# 実験の実行と結果を保存する関数\n",
        "import pickle\n",
        "\n",
        "def run_and_save_experiment(hyper):\n",
        "    # ディレクトリの作成\n",
        "    if not os.path.exists(hyper.directory):\n",
        "        os.makedirs(hyper.directory)\n",
        "\n",
        "    # pickle ファイルのパスをディレクトリに基づいて変更\n",
        "    pickle_path = os.path.join(hyper.directory, hyper.pickle)\n",
        "\n",
        "    # test_size または chunk が None の場合は train_size の値で初期化\n",
        "    if hyper.test_size is None:\n",
        "        hyper.test_size = hyper.train_size\n",
        "\n",
        "    if hyper.chunk is None:\n",
        "        hyper.chunk = hyper.train_size\n",
        "\n",
        "    try:\n",
        "        # 引数を pickle ファイルに保存\n",
        "        with open(pickle_path, 'wb') as f:\n",
        "            pickle.dump(hyper, f)\n",
        "\n",
        "        with open(pickle_path, 'ab') as f:\n",
        "            for res in execute(hyper):\n",
        "                # 結果を pickle ファイルに追加で保存\n",
        "                pickle.dump(res, f)\n",
        "    except Exception as e:\n",
        "        if os.path.exists(pickle_path):\n",
        "            os.remove(pickle_path)\n",
        "        print(f\"An error occurred during saving: {e}\")\n",
        "        raise e\n",
        "\n",
        "######## 実験の実行と結果の保存\n",
        "hyper = HyperParams()\n",
        "run_and_save_experiment(hyper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk-AWvkGJOFk",
        "outputId": "2cd8ca14-8b37-431c-ef1d-d9c7b86a0bf4"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available. Running on CPU.\n",
            "[0 +0] [dt=1.0e-01 dgrad=1.1e-04 dout=3.1e-06]\n",
            "[i=0 t=1.00e-01 wall=0] [dt=1.0e-01 dgrad=4.6e-07 dout=1.9e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=1 t=2.10e-01 wall=0] [dt=1.1e-01 dgrad=1.1e-06 dout=3.0e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=2 t=3.31e-01 wall=0] [dt=1.2e-01 dgrad=1.6e-06 dout=3.6e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=3 t=4.64e-01 wall=0] [dt=1.3e-01 dgrad=2.0e-06 dout=4.0e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=4 t=6.11e-01 wall=0] [dt=1.5e-01 dgrad=2.4e-06 dout=4.5e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=5 t=7.72e-01 wall=0] [dt=1.6e-01 dgrad=3.0e-06 dout=4.9e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=6 t=9.49e-01 wall=0] [dt=1.8e-01 dgrad=3.6e-06 dout=5.4e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=7 t=1.14e+00 wall=0] [dt=1.9e-01 dgrad=4.3e-06 dout=6.0e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=8 t=1.36e+00 wall=0] [dt=2.1e-01 dgrad=5.3e-06 dout=6.6e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=9 t=1.59e+00 wall=0] [dt=2.4e-01 dgrad=6.3e-06 dout=7.3e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=10 t=1.85e+00 wall=0] [dt=2.6e-01 dgrad=7.7e-06 dout=8.0e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=11 t=2.14e+00 wall=0] [dt=2.9e-01 dgrad=9.3e-06 dout=8.8e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=12 t=2.45e+00 wall=0] [dt=3.1e-01 dgrad=1.1e-05 dout=9.7e-07] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=13 t=2.80e+00 wall=0] [dt=3.5e-01 dgrad=1.4e-05 dout=1.1e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=14 t=3.18e+00 wall=0] [dt=3.8e-01 dgrad=1.6e-05 dout=1.2e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=15 t=3.59e+00 wall=0] [dt=4.2e-01 dgrad=2.0e-05 dout=1.3e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=16 t=4.05e+00 wall=1] [dt=4.6e-01 dgrad=2.4e-05 dout=1.4e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=17 t=4.56e+00 wall=1] [dt=5.1e-01 dgrad=2.9e-05 dout=1.6e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=18 t=5.12e+00 wall=1] [dt=5.6e-01 dgrad=3.5e-05 dout=1.7e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=19 t=5.73e+00 wall=1] [dt=6.1e-01 dgrad=4.2e-05 dout=1.9e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=20 t=6.40e+00 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.1e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=21 t=7.07e+00 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.1e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=22 t=7.75e+00 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.1e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=23 t=8.42e+00 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.2e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=24 t=9.09e+00 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.2e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=25 t=9.76e+00 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.2e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=26 t=1.04e+01 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.2e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=27 t=1.11e+01 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.2e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=28 t=1.18e+01 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.2e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=29 t=1.25e+01 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.3e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=30 t=1.31e+01 wall=1] [dt=6.7e-01 dgrad=5.1e-05 dout=2.3e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=31 t=1.38e+01 wall=1] [dt=6.7e-01 dgrad=5.2e-05 dout=2.3e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=32 t=1.45e+01 wall=1] [dt=6.7e-01 dgrad=5.2e-05 dout=2.3e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=33 t=1.51e+01 wall=1] [dt=6.7e-01 dgrad=5.2e-05 dout=2.3e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=34 t=1.58e+01 wall=1] [dt=6.7e-01 dgrad=5.2e-05 dout=2.4e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=35 t=1.65e+01 wall=1] [dt=6.7e-01 dgrad=5.2e-05 dout=2.4e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=36 t=1.72e+01 wall=1] [dt=6.7e-01 dgrad=5.2e-05 dout=2.4e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=37 t=1.78e+01 wall=1] [dt=6.7e-01 dgrad=5.3e-05 dout=2.4e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=38 t=1.85e+01 wall=1] [dt=6.7e-01 dgrad=5.3e-05 dout=2.4e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=39 t=1.92e+01 wall=1] [dt=6.7e-01 dgrad=5.3e-05 dout=2.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=40 t=1.99e+01 wall=1] [dt=6.7e-01 dgrad=5.3e-05 dout=2.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=41 t=2.05e+01 wall=1] [dt=6.7e-01 dgrad=5.3e-05 dout=2.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=42 t=2.12e+01 wall=1] [dt=6.7e-01 dgrad=5.3e-05 dout=2.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=43 t=2.19e+01 wall=1] [dt=6.7e-01 dgrad=5.4e-05 dout=2.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=44 t=2.25e+01 wall=1] [dt=6.7e-01 dgrad=5.4e-05 dout=2.6e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=45 t=2.32e+01 wall=1] [dt=6.7e-01 dgrad=5.4e-05 dout=2.6e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=46 t=2.39e+01 wall=1] [dt=6.7e-01 dgrad=5.4e-05 dout=2.6e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=47 t=2.46e+01 wall=1] [dt=6.7e-01 dgrad=5.4e-05 dout=2.6e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=48 t=2.52e+01 wall=1] [dt=6.7e-01 dgrad=5.4e-05 dout=2.6e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=49 t=2.59e+01 wall=1] [dt=6.7e-01 dgrad=5.4e-05 dout=2.7e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=50 t=2.66e+01 wall=1] [dt=6.7e-01 dgrad=5.4e-05 dout=2.7e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=51 t=2.73e+01 wall=1] [dt=6.7e-01 dgrad=5.4e-05 dout=2.7e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=52 t=2.79e+01 wall=1] [dt=6.7e-01 dgrad=5.5e-05 dout=2.7e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=53 t=2.86e+01 wall=1] [dt=6.7e-01 dgrad=5.5e-05 dout=2.8e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=54 t=2.93e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=2.8e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=55 t=2.99e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=2.8e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=56 t=3.06e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=2.8e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=57 t=3.13e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=2.8e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=58 t=3.20e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=2.9e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=59 t=3.26e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=2.9e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=60 t=3.33e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=2.9e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=61 t=3.40e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=2.9e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=62 t=3.47e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=3.0e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=63 t=3.53e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=3.0e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=64 t=3.60e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=3.0e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=65 t=3.67e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=3.1e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=66 t=3.73e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=3.1e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=67 t=3.80e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=3.1e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=68 t=3.87e+01 wall=2] [dt=6.7e-01 dgrad=5.5e-05 dout=3.1e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=69 t=3.94e+01 wall=2] [dt=6.7e-01 dgrad=5.6e-05 dout=3.2e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=70 t=4.00e+01 wall=2] [dt=6.7e-01 dgrad=5.6e-05 dout=3.2e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=71 t=4.07e+01 wall=2] [dt=6.7e-01 dgrad=5.6e-05 dout=3.2e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=72 t=4.14e+01 wall=2] [dt=6.7e-01 dgrad=5.6e-05 dout=3.3e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=73 t=4.21e+01 wall=2] [dt=6.7e-01 dgrad=5.6e-05 dout=3.3e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=74 t=4.27e+01 wall=2] [dt=6.7e-01 dgrad=5.6e-05 dout=3.3e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=75 t=4.34e+01 wall=2] [dt=6.7e-01 dgrad=5.6e-05 dout=3.4e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=76 t=4.41e+01 wall=2] [dt=6.7e-01 dgrad=5.6e-05 dout=3.4e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=77 t=4.47e+01 wall=2] [dt=6.7e-01 dgrad=5.6e-05 dout=3.4e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=78 t=4.54e+01 wall=2] [dt=6.7e-01 dgrad=5.6e-05 dout=3.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=79 t=4.61e+01 wall=2] [dt=6.7e-01 dgrad=5.7e-05 dout=3.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=80 t=4.68e+01 wall=2] [dt=6.7e-01 dgrad=5.7e-05 dout=3.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=81 t=4.74e+01 wall=2] [dt=6.7e-01 dgrad=5.7e-05 dout=3.6e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=82 t=4.81e+01 wall=2] [dt=6.7e-01 dgrad=5.7e-05 dout=3.6e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=83 t=4.88e+01 wall=2] [dt=6.7e-01 dgrad=5.7e-05 dout=3.6e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=84 t=4.95e+01 wall=2] [dt=6.7e-01 dgrad=5.7e-05 dout=3.7e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=85 t=5.01e+01 wall=2] [dt=6.7e-01 dgrad=5.7e-05 dout=3.7e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=86 t=5.08e+01 wall=2] [dt=6.7e-01 dgrad=5.8e-05 dout=3.8e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=87 t=5.15e+01 wall=2] [dt=6.7e-01 dgrad=5.8e-05 dout=3.8e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=88 t=5.21e+01 wall=2] [dt=6.7e-01 dgrad=5.8e-05 dout=3.8e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=89 t=5.28e+01 wall=2] [dt=6.7e-01 dgrad=5.8e-05 dout=3.9e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=90 t=5.35e+01 wall=3] [dt=6.7e-01 dgrad=5.8e-05 dout=3.9e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=91 t=5.42e+01 wall=3] [dt=6.7e-01 dgrad=5.9e-05 dout=4.0e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=92 t=5.48e+01 wall=3] [dt=6.7e-01 dgrad=5.9e-05 dout=4.0e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=93 t=5.55e+01 wall=3] [dt=6.7e-01 dgrad=5.9e-05 dout=4.1e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=94 t=5.62e+01 wall=3] [dt=6.7e-01 dgrad=5.9e-05 dout=4.1e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=95 t=5.69e+01 wall=3] [dt=6.7e-01 dgrad=6.0e-05 dout=4.2e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=96 t=5.75e+01 wall=3] [dt=6.7e-01 dgrad=6.0e-05 dout=4.2e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=97 t=5.82e+01 wall=3] [dt=6.7e-01 dgrad=6.0e-05 dout=4.2e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=98 t=5.89e+01 wall=3] [dt=6.7e-01 dgrad=6.0e-05 dout=4.3e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=99 t=5.95e+01 wall=3] [dt=6.7e-01 dgrad=6.1e-05 dout=4.3e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=100 t=6.02e+01 wall=3] [dt=6.7e-01 dgrad=6.1e-05 dout=4.4e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=101 t=6.09e+01 wall=3] [dt=6.7e-01 dgrad=6.1e-05 dout=4.4e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=103 t=6.22e+01 wall=3] [dt=6.7e-01 dgrad=6.2e-05 dout=4.6e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=105 t=6.36e+01 wall=3] [dt=6.7e-01 dgrad=6.3e-05 dout=4.7e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=107 t=6.49e+01 wall=3] [dt=6.7e-01 dgrad=6.3e-05 dout=4.8e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=109 t=6.63e+01 wall=3] [dt=6.7e-01 dgrad=6.4e-05 dout=4.9e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=111 t=6.76e+01 wall=3] [dt=6.7e-01 dgrad=6.4e-05 dout=5.0e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=113 t=6.90e+01 wall=3] [dt=6.7e-01 dgrad=6.5e-05 dout=5.1e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=115 t=7.03e+01 wall=3] [dt=6.7e-01 dgrad=6.6e-05 dout=5.3e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=117 t=7.17e+01 wall=3] [dt=6.7e-01 dgrad=6.6e-05 dout=5.4e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=119 t=7.30e+01 wall=3] [dt=6.7e-01 dgrad=6.7e-05 dout=5.5e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=121 t=7.43e+01 wall=3] [dt=6.7e-01 dgrad=6.7e-05 dout=5.7e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=123 t=7.57e+01 wall=3] [dt=6.7e-01 dgrad=6.8e-05 dout=5.8e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=125 t=7.70e+01 wall=3] [dt=6.7e-01 dgrad=6.9e-05 dout=6.0e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=127 t=7.84e+01 wall=3] [dt=6.7e-01 dgrad=6.9e-05 dout=6.1e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=129 t=7.97e+01 wall=3] [dt=6.7e-01 dgrad=7.0e-05 dout=6.3e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=131 t=8.11e+01 wall=3] [dt=6.7e-01 dgrad=7.0e-05 dout=6.4e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=133 t=8.24e+01 wall=3] [dt=6.7e-01 dgrad=7.1e-05 dout=6.6e-06] [train aL=1.00e+00 err=0.45 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=135 t=8.38e+01 wall=4] [dt=6.7e-01 dgrad=7.1e-05 dout=6.8e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=137 t=8.51e+01 wall=4] [dt=6.7e-01 dgrad=7.2e-05 dout=6.9e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=139 t=8.65e+01 wall=4] [dt=6.7e-01 dgrad=7.3e-05 dout=7.1e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=141 t=8.78e+01 wall=4] [dt=6.7e-01 dgrad=7.3e-05 dout=7.3e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=143 t=8.91e+01 wall=4] [dt=6.7e-01 dgrad=7.4e-05 dout=7.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=145 t=9.05e+01 wall=4] [dt=6.7e-01 dgrad=7.5e-05 dout=7.7e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=147 t=9.18e+01 wall=4] [dt=6.7e-01 dgrad=7.6e-05 dout=7.9e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=149 t=9.32e+01 wall=4] [dt=6.7e-01 dgrad=7.6e-05 dout=8.1e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=151 t=9.45e+01 wall=4] [dt=6.7e-01 dgrad=7.7e-05 dout=8.3e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=153 t=9.59e+01 wall=4] [dt=6.7e-01 dgrad=7.8e-05 dout=8.6e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=155 t=9.72e+01 wall=4] [dt=6.7e-01 dgrad=7.9e-05 dout=8.8e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=157 t=9.86e+01 wall=4] [dt=6.7e-01 dgrad=8.0e-05 dout=9.0e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=159 t=9.99e+01 wall=4] [dt=6.7e-01 dgrad=8.1e-05 dout=9.3e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=161 t=1.01e+02 wall=4] [dt=6.7e-01 dgrad=8.2e-05 dout=9.6e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=163 t=1.03e+02 wall=4] [dt=6.7e-01 dgrad=8.2e-05 dout=9.8e-06] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=165 t=1.04e+02 wall=4] [dt=6.7e-01 dgrad=8.3e-05 dout=1.0e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=167 t=1.05e+02 wall=4] [dt=6.7e-01 dgrad=8.4e-05 dout=1.0e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=169 t=1.07e+02 wall=4] [dt=6.7e-01 dgrad=8.6e-05 dout=1.1e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=171 t=1.08e+02 wall=4] [dt=6.7e-01 dgrad=8.7e-05 dout=1.1e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=173 t=1.09e+02 wall=4] [dt=6.7e-01 dgrad=8.8e-05 dout=1.1e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=175 t=1.11e+02 wall=4] [dt=6.7e-01 dgrad=8.9e-05 dout=1.2e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=177 t=1.12e+02 wall=4] [dt=6.7e-01 dgrad=9.0e-05 dout=1.2e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=179 t=1.13e+02 wall=4] [dt=6.7e-01 dgrad=9.1e-05 dout=1.3e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=181 t=1.15e+02 wall=5] [dt=6.7e-01 dgrad=9.3e-05 dout=1.3e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=183 t=1.16e+02 wall=5] [dt=6.7e-01 dgrad=9.4e-05 dout=1.3e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=185 t=1.17e+02 wall=5] [dt=6.7e-01 dgrad=9.6e-05 dout=1.4e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=187 t=1.19e+02 wall=5] [dt=6.7e-01 dgrad=9.7e-05 dout=1.4e-05] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=189 t=1.20e+02 wall=5] [dt=6.7e-01 dgrad=9.8e-05 dout=1.5e-05] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=191 t=1.21e+02 wall=5] [dt=6.7e-01 dgrad=1.0e-04 dout=1.5e-05] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[192 +192] [dt=6.7e-02 dgrad=1.0e-04 dout=1.6e-05]\n",
            "[i=193 t=1.22e+02 wall=5] [dt=7.4e-02 dgrad=1.2e-06 dout=1.7e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=195 t=1.22e+02 wall=5] [dt=9.0e-02 dgrad=1.8e-06 dout=2.1e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=197 t=1.22e+02 wall=5] [dt=1.1e-01 dgrad=2.6e-06 dout=2.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=199 t=1.22e+02 wall=5] [dt=1.3e-01 dgrad=3.9e-06 dout=3.1e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=201 t=1.23e+02 wall=5] [dt=1.6e-01 dgrad=5.7e-06 dout=3.8e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=203 t=1.23e+02 wall=5] [dt=1.9e-01 dgrad=8.3e-06 dout=4.6e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=206 t=1.24e+02 wall=5] [dt=2.6e-01 dgrad=1.5e-05 dout=6.2e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=209 t=1.25e+02 wall=5] [dt=3.4e-01 dgrad=2.7e-05 dout=8.5e-06] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=212 t=1.26e+02 wall=5] [dt=4.5e-01 dgrad=4.8e-05 dout=1.2e-05] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=215 t=1.27e+02 wall=5] [dt=5.0e-01 dgrad=5.9e-05 dout=1.3e-05] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=218 t=1.29e+02 wall=6] [dt=5.0e-01 dgrad=6.0e-05 dout=1.4e-05] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=221 t=1.30e+02 wall=6] [dt=5.0e-01 dgrad=6.1e-05 dout=1.4e-05] [train aL=1.00e+00 err=0.44 nd=400/400] [test aL=1.00e+00 err=0.43]\n",
            "[i=224 t=1.32e+02 wall=6] [dt=5.0e-01 dgrad=6.2e-05 dout=1.5e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=227 t=1.33e+02 wall=6] [dt=5.0e-01 dgrad=6.3e-05 dout=1.6e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=230 t=1.35e+02 wall=6] [dt=5.0e-01 dgrad=6.4e-05 dout=1.6e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=233 t=1.36e+02 wall=6] [dt=5.0e-01 dgrad=6.5e-05 dout=1.7e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=236 t=1.38e+02 wall=6] [dt=5.0e-01 dgrad=6.6e-05 dout=1.8e-05] [train aL=1.00e+00 err=0.43 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=239 t=1.39e+02 wall=6] [dt=5.0e-01 dgrad=6.7e-05 dout=1.9e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=242 t=1.41e+02 wall=6] [dt=5.0e-01 dgrad=6.9e-05 dout=2.0e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=245 t=1.42e+02 wall=6] [dt=5.0e-01 dgrad=7.0e-05 dout=2.0e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=248 t=1.44e+02 wall=6] [dt=5.0e-01 dgrad=7.2e-05 dout=2.1e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=251 t=1.45e+02 wall=6] [dt=5.0e-01 dgrad=7.3e-05 dout=2.2e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=254 t=1.47e+02 wall=6] [dt=5.0e-01 dgrad=7.5e-05 dout=2.4e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=257 t=1.48e+02 wall=7] [dt=5.0e-01 dgrad=7.6e-05 dout=2.5e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=260 t=1.50e+02 wall=7] [dt=5.0e-01 dgrad=7.8e-05 dout=2.6e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=263 t=1.51e+02 wall=7] [dt=5.0e-01 dgrad=8.0e-05 dout=2.8e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=266 t=1.53e+02 wall=7] [dt=5.0e-01 dgrad=8.1e-05 dout=2.9e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=269 t=1.54e+02 wall=7] [dt=5.0e-01 dgrad=8.3e-05 dout=3.1e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=272 t=1.56e+02 wall=7] [dt=5.0e-01 dgrad=8.5e-05 dout=3.2e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.44]\n",
            "[i=275 t=1.57e+02 wall=7] [dt=5.0e-01 dgrad=8.7e-05 dout=3.4e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=278 t=1.59e+02 wall=7] [dt=5.0e-01 dgrad=8.9e-05 dout=3.6e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=281 t=1.60e+02 wall=7] [dt=5.0e-01 dgrad=9.1e-05 dout=3.8e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=284 t=1.62e+02 wall=7] [dt=5.0e-01 dgrad=9.3e-05 dout=4.1e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=287 t=1.63e+02 wall=7] [dt=5.0e-01 dgrad=9.5e-05 dout=4.3e-05] [train aL=1.00e+00 err=0.42 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=290 t=1.65e+02 wall=7] [dt=5.0e-01 dgrad=9.7e-05 dout=4.6e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=293 t=1.66e+02 wall=8] [dt=5.0e-01 dgrad=1.0e-04 dout=4.8e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[294 +102] [dt=5.0e-02 dgrad=1.0e-04 dout=4.9e-05]\n",
            "[i=296 t=1.66e+02 wall=8] [dt=6.0e-02 dgrad=1.5e-06 dout=5.9e-06] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=299 t=1.66e+02 wall=8] [dt=8.0e-02 dgrad=2.6e-06 dout=8.0e-06] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=302 t=1.67e+02 wall=8] [dt=1.1e-01 dgrad=4.7e-06 dout=1.1e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=305 t=1.67e+02 wall=8] [dt=1.4e-01 dgrad=8.3e-06 dout=1.5e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=309 t=1.68e+02 wall=8] [dt=2.1e-01 dgrad=1.8e-05 dout=2.2e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=313 t=1.69e+02 wall=8] [dt=3.0e-01 dgrad=3.9e-05 dout=3.3e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=317 t=1.70e+02 wall=8] [dt=3.7e-01 dgrad=5.9e-05 dout=4.3e-05] [train aL=1.00e+00 err=0.40 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=321 t=1.72e+02 wall=8] [dt=3.7e-01 dgrad=6.0e-05 dout=4.5e-05] [train aL=1.00e+00 err=0.40 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=325 t=1.73e+02 wall=8] [dt=3.7e-01 dgrad=6.2e-05 dout=4.8e-05] [train aL=1.00e+00 err=0.40 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=329 t=1.75e+02 wall=8] [dt=3.7e-01 dgrad=6.3e-05 dout=5.1e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=333 t=1.76e+02 wall=8] [dt=3.7e-01 dgrad=6.5e-05 dout=5.5e-05] [train aL=1.00e+00 err=0.40 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=337 t=1.78e+02 wall=8] [dt=3.7e-01 dgrad=6.7e-05 dout=5.8e-05] [train aL=1.00e+00 err=0.40 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=341 t=1.79e+02 wall=8] [dt=3.7e-01 dgrad=6.9e-05 dout=6.2e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=345 t=1.81e+02 wall=9] [dt=3.7e-01 dgrad=7.1e-05 dout=6.6e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=349 t=1.82e+02 wall=9] [dt=3.7e-01 dgrad=7.3e-05 dout=7.1e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=353 t=1.84e+02 wall=9] [dt=3.7e-01 dgrad=7.5e-05 dout=7.6e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=357 t=1.85e+02 wall=9] [dt=3.7e-01 dgrad=7.7e-05 dout=8.2e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=361 t=1.87e+02 wall=9] [dt=3.7e-01 dgrad=8.0e-05 dout=8.8e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=365 t=1.88e+02 wall=9] [dt=3.7e-01 dgrad=8.2e-05 dout=9.4e-05] [train aL=1.00e+00 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=369 t=1.90e+02 wall=9] [dt=3.7e-01 dgrad=8.5e-05 dout=1.0e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=373 t=1.91e+02 wall=9] [dt=3.7e-01 dgrad=8.8e-05 dout=1.1e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=377 t=1.92e+02 wall=9] [dt=3.7e-01 dgrad=9.2e-05 dout=1.2e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.45]\n",
            "[i=381 t=1.94e+02 wall=9] [dt=3.7e-01 dgrad=9.5e-05 dout=1.3e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=385 t=1.95e+02 wall=9] [dt=3.7e-01 dgrad=9.9e-05 dout=1.4e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[386 +92] [dt=3.7e-02 dgrad=1.0e-04 dout=1.4e-04]\n",
            "[i=389 t=1.96e+02 wall=9] [dt=4.9e-02 dgrad=1.8e-06 dout=1.9e-05] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=393 t=1.96e+02 wall=9] [dt=7.2e-02 dgrad=3.8e-06 dout=2.8e-05] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=397 t=1.96e+02 wall=9] [dt=1.1e-01 dgrad=8.3e-06 dout=4.2e-05] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=401 t=1.97e+02 wall=10] [dt=1.5e-01 dgrad=1.8e-05 dout=6.3e-05] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=405 t=1.98e+02 wall=10] [dt=2.3e-01 dgrad=4.0e-05 dout=9.6e-05] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=409 t=1.99e+02 wall=10] [dt=2.7e-01 dgrad=6.0e-05 dout=1.2e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=414 t=2.00e+02 wall=10] [dt=2.7e-01 dgrad=6.2e-05 dout=1.3e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=419 t=2.01e+02 wall=10] [dt=2.7e-01 dgrad=6.5e-05 dout=1.4e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=424 t=2.03e+02 wall=10] [dt=2.7e-01 dgrad=6.7e-05 dout=1.6e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=1.00e+00 err=0.46]\n",
            "[i=429 t=2.04e+02 wall=10] [dt=2.7e-01 dgrad=7.0e-05 dout=1.7e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=434 t=2.05e+02 wall=10] [dt=2.7e-01 dgrad=7.3e-05 dout=1.9e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=439 t=2.07e+02 wall=10] [dt=2.7e-01 dgrad=7.6e-05 dout=2.0e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=444 t=2.08e+02 wall=10] [dt=2.7e-01 dgrad=7.9e-05 dout=2.2e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=449 t=2.09e+02 wall=10] [dt=2.7e-01 dgrad=8.2e-05 dout=2.4e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=454 t=2.11e+02 wall=10] [dt=2.7e-01 dgrad=8.6e-05 dout=2.7e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=459 t=2.12e+02 wall=11] [dt=2.7e-01 dgrad=9.0e-05 dout=2.9e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=464 t=2.14e+02 wall=11] [dt=2.7e-01 dgrad=9.4e-05 dout=3.3e-04] [train aL=9.99e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=469 t=2.15e+02 wall=11] [dt=2.7e-01 dgrad=9.9e-05 dout=3.6e-04] [train aL=9.98e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[471 +85] [dt=2.7e-02 dgrad=1.0e-04 dout=3.7e-04]\n",
            "[i=474 t=2.15e+02 wall=11] [dt=3.6e-02 dgrad=1.8e-06 dout=5.0e-05] [train aL=9.98e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=479 t=2.16e+02 wall=11] [dt=5.8e-02 dgrad=4.7e-06 dout=8.2e-05] [train aL=9.98e-01 err=0.41 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=484 t=2.16e+02 wall=11] [dt=9.4e-02 dgrad=1.2e-05 dout=1.4e-04] [train aL=9.98e-01 err=0.42 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=489 t=2.17e+02 wall=11] [dt=1.5e-01 dgrad=3.2e-05 dout=2.3e-04] [train aL=9.98e-01 err=0.42 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=494 t=2.18e+02 wall=11] [dt=2.0e-01 dgrad=5.9e-05 dout=3.3e-04] [train aL=9.98e-01 err=0.42 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=499 t=2.19e+02 wall=11] [dt=2.0e-01 dgrad=6.2e-05 dout=3.5e-04] [train aL=9.98e-01 err=0.42 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=504 t=2.20e+02 wall=11] [dt=2.0e-01 dgrad=6.4e-05 dout=3.8e-04] [train aL=9.98e-01 err=0.42 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=509 t=2.21e+02 wall=11] [dt=2.0e-01 dgrad=6.7e-05 dout=4.2e-04] [train aL=9.98e-01 err=0.42 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=514 t=2.22e+02 wall=11] [dt=2.0e-01 dgrad=7.0e-05 dout=4.5e-04] [train aL=9.98e-01 err=0.42 nd=400/400] [test aL=9.99e-01 err=0.46]\n",
            "[i=520 t=2.23e+02 wall=12] [dt=2.0e-01 dgrad=7.3e-05 dout=5.0e-04] [train aL=9.98e-01 err=0.42 nd=400/400] [test aL=9.98e-01 err=0.46]\n",
            "[i=526 t=2.24e+02 wall=12] [dt=2.0e-01 dgrad=7.7e-05 dout=5.6e-04] [train aL=9.97e-01 err=0.42 nd=400/400] [test aL=9.98e-01 err=0.46]\n",
            "[i=532 t=2.25e+02 wall=12] [dt=2.0e-01 dgrad=8.2e-05 dout=6.3e-04] [train aL=9.97e-01 err=0.42 nd=400/400] [test aL=9.98e-01 err=0.46]\n",
            "[i=538 t=2.26e+02 wall=12] [dt=2.0e-01 dgrad=8.6e-05 dout=7.1e-04] [train aL=9.97e-01 err=0.42 nd=400/400] [test aL=9.98e-01 err=0.46]\n",
            "[i=544 t=2.28e+02 wall=12] [dt=2.0e-01 dgrad=9.1e-05 dout=7.9e-04] [train aL=9.97e-01 err=0.42 nd=400/400] [test aL=9.98e-01 err=0.46]\n",
            "[i=550 t=2.29e+02 wall=12] [dt=2.0e-01 dgrad=9.6e-05 dout=9.0e-04] [train aL=9.96e-01 err=0.42 nd=400/400] [test aL=9.98e-01 err=0.46]\n",
            "[554 +83] [dt=2.0e-02 dgrad=1.0e-04 dout=9.7e-04]\n",
            "[i=556 t=2.30e+02 wall=12] [dt=2.4e-02 dgrad=1.5e-06 dout=1.2e-04] [train aL=9.96e-01 err=0.42 nd=400/400] [test aL=9.97e-01 err=0.46]\n",
            "[i=562 t=2.30e+02 wall=12] [dt=4.3e-02 dgrad=4.7e-06 dout=2.1e-04] [train aL=9.96e-01 err=0.42 nd=400/400] [test aL=9.97e-01 err=0.46]\n",
            "[i=568 t=2.30e+02 wall=12] [dt=7.7e-02 dgrad=1.5e-05 dout=3.9e-04] [train aL=9.96e-01 err=0.42 nd=400/400] [test aL=9.97e-01 err=0.46]\n",
            "[i=574 t=2.31e+02 wall=12] [dt=1.4e-01 dgrad=4.8e-05 dout=7.4e-04] [train aL=9.96e-01 err=0.42 nd=400/400] [test aL=9.97e-01 err=0.46]\n",
            "[i=580 t=2.32e+02 wall=12] [dt=1.5e-01 dgrad=6.1e-05 dout=8.9e-04] [train aL=9.96e-01 err=0.42 nd=400/400] [test aL=9.97e-01 err=0.46]\n",
            "[i=586 t=2.33e+02 wall=13] [dt=1.5e-01 dgrad=6.4e-05 dout=9.8e-04] [train aL=9.95e-01 err=0.42 nd=400/400] [test aL=9.97e-01 err=0.46]\n",
            "[i=592 t=2.33e+02 wall=13] [dt=1.5e-01 dgrad=6.7e-05 dout=1.1e-03] [train aL=9.95e-01 err=0.42 nd=400/400] [test aL=9.97e-01 err=0.46]\n",
            "[i=598 t=2.34e+02 wall=13] [dt=1.5e-01 dgrad=7.1e-05 dout=1.2e-03] [train aL=9.94e-01 err=0.42 nd=400/400] [test aL=9.96e-01 err=0.46]\n",
            "[i=604 t=2.35e+02 wall=13] [dt=1.5e-01 dgrad=7.4e-05 dout=1.3e-03] [train aL=9.94e-01 err=0.42 nd=400/400] [test aL=9.96e-01 err=0.46]\n",
            "[i=610 t=2.36e+02 wall=13] [dt=1.5e-01 dgrad=7.9e-05 dout=1.5e-03] [train aL=9.94e-01 err=0.43 nd=400/400] [test aL=9.96e-01 err=0.47]\n",
            "[i=616 t=2.37e+02 wall=13] [dt=1.5e-01 dgrad=8.3e-05 dout=1.7e-03] [train aL=9.93e-01 err=0.42 nd=400/400] [test aL=9.95e-01 err=0.47]\n",
            "[i=622 t=2.38e+02 wall=13] [dt=1.5e-01 dgrad=8.8e-05 dout=1.9e-03] [train aL=9.92e-01 err=0.42 nd=400/400] [test aL=9.95e-01 err=0.46]\n",
            "[i=629 t=2.39e+02 wall=13] [dt=1.5e-01 dgrad=9.4e-05 dout=2.1e-03] [train aL=9.92e-01 err=0.42 nd=400/400] [test aL=9.94e-01 err=0.46]\n",
            "[635 +81] [dt=1.5e-02 dgrad=1.0e-04 dout=2.4e-03]\n",
            "[i=636 t=2.40e+02 wall=13] [dt=1.6e-02 dgrad=1.2e-06 dout=2.6e-04] [train aL=9.91e-01 err=0.42 nd=400/400] [test aL=9.94e-01 err=0.46]\n",
            "[i=643 t=2.40e+02 wall=14] [dt=3.2e-02 dgrad=4.6e-06 dout=5.3e-04] [train aL=9.91e-01 err=0.42 nd=400/400] [test aL=9.94e-01 err=0.46]\n",
            "[i=650 t=2.40e+02 wall=14] [dt=6.2e-02 dgrad=1.8e-05 dout=1.1e-03] [train aL=9.91e-01 err=0.43 nd=400/400] [test aL=9.93e-01 err=0.46]\n",
            "[i=657 t=2.41e+02 wall=14] [dt=1.1e-01 dgrad=5.9e-05 dout=2.1e-03] [train aL=9.90e-01 err=0.43 nd=400/400] [test aL=9.93e-01 err=0.46]\n",
            "[i=664 t=2.42e+02 wall=14] [dt=1.1e-01 dgrad=6.2e-05 dout=2.3e-03] [train aL=9.89e-01 err=0.43 nd=400/400] [test aL=9.92e-01 err=0.46]\n",
            "[i=671 t=2.42e+02 wall=14] [dt=1.1e-01 dgrad=6.6e-05 dout=2.6e-03] [train aL=9.88e-01 err=0.43 nd=400/400] [test aL=9.92e-01 err=0.46]\n",
            "[i=678 t=2.43e+02 wall=14] [dt=1.1e-01 dgrad=7.0e-05 dout=2.9e-03] [train aL=9.87e-01 err=0.43 nd=400/400] [test aL=9.91e-01 err=0.46]\n",
            "[i=685 t=2.44e+02 wall=14] [dt=1.1e-01 dgrad=7.4e-05 dout=3.3e-03] [train aL=9.86e-01 err=0.44 nd=400/400] [test aL=9.90e-01 err=0.46]\n",
            "[i=692 t=2.45e+02 wall=14] [dt=1.1e-01 dgrad=7.8e-05 dout=3.8e-03] [train aL=9.85e-01 err=0.44 nd=400/400] [test aL=9.89e-01 err=0.46]\n",
            "[i=699 t=2.46e+02 wall=14] [dt=1.1e-01 dgrad=8.3e-05 dout=4.3e-03] [train aL=9.83e-01 err=0.44 nd=400/400] [test aL=9.88e-01 err=0.46]\n",
            "[i=706 t=2.46e+02 wall=15] [dt=1.1e-01 dgrad=8.9e-05 dout=4.9e-03] [train aL=9.82e-01 err=0.43 nd=400/400] [test aL=9.87e-01 err=0.46]\n",
            "[i=713 t=2.47e+02 wall=15] [dt=1.1e-01 dgrad=9.5e-05 dout=5.6e-03] [train aL=9.80e-01 err=0.43 nd=400/400] [test aL=9.85e-01 err=0.45]\n",
            "[718 +83] [dt=1.1e-02 dgrad=1.0e-04 dout=6.2e-03]\n",
            "[i=720 t=2.48e+02 wall=15] [dt=1.3e-02 dgrad=1.5e-06 dout=7.5e-04] [train aL=9.78e-01 err=0.43 nd=400/400] [test aL=9.84e-01 err=0.45]\n",
            "[i=727 t=2.48e+02 wall=15] [dt=2.6e-02 dgrad=5.6e-06 dout=1.5e-03] [train aL=9.78e-01 err=0.43 nd=400/400] [test aL=9.84e-01 err=0.45]\n",
            "[i=735 t=2.48e+02 wall=15] [dt=5.6e-02 dgrad=2.7e-05 dout=3.4e-03] [train aL=9.77e-01 err=0.43 nd=400/400] [test aL=9.83e-01 err=0.45]\n",
            "[i=743 t=2.49e+02 wall=15] [dt=8.2e-02 dgrad=6.0e-05 dout=5.6e-03] [train aL=9.75e-01 err=0.43 nd=400/400] [test aL=9.81e-01 err=0.45]\n",
            "[i=751 t=2.49e+02 wall=15] [dt=8.2e-02 dgrad=6.4e-05 dout=6.4e-03] [train aL=9.73e-01 err=0.43 nd=400/400] [test aL=9.79e-01 err=0.45]\n",
            "[i=759 t=2.50e+02 wall=15] [dt=8.2e-02 dgrad=6.9e-05 dout=7.3e-03] [train aL=9.70e-01 err=0.43 nd=400/400] [test aL=9.77e-01 err=0.45]\n",
            "[i=767 t=2.51e+02 wall=15] [dt=8.2e-02 dgrad=7.4e-05 dout=8.4e-03] [train aL=9.67e-01 err=0.43 nd=400/400] [test aL=9.75e-01 err=0.45]\n",
            "[i=775 t=2.51e+02 wall=16] [dt=8.2e-02 dgrad=7.9e-05 dout=9.7e-03] [train aL=9.63e-01 err=0.44 nd=400/400] [test aL=9.72e-01 err=0.45]\n",
            "[i=783 t=2.52e+02 wall=16] [dt=8.2e-02 dgrad=8.1e-05 dout=1.1e-02] [train aL=9.59e-01 err=0.44 nd=400/400] [test aL=9.68e-01 err=0.45]\n",
            "[i=791 t=2.53e+02 wall=16] [dt=8.2e-02 dgrad=6.7e-05 dout=1.3e-02] [train aL=9.54e-01 err=0.45 nd=400/400] [test aL=9.64e-01 err=0.45]\n",
            "[i=799 t=2.53e+02 wall=16] [dt=9.9e-02 dgrad=7.7e-05 dout=1.8e-02] [train aL=9.48e-01 err=0.45 nd=400/400] [test aL=9.60e-01 err=0.45]\n",
            "[800 +82] [dt=9.9e-03 dgrad=1.2e-04 dout=1.8e-02]\n",
            "[i=807 t=2.53e+02 wall=16] [dt=1.9e-02 dgrad=6.1e-06 dout=3.5e-03] [train aL=9.47e-01 err=0.45 nd=400/400] [test aL=9.59e-01 err=0.45]\n",
            "[i=815 t=2.54e+02 wall=16] [dt=3.8e-02 dgrad=6.5e-05 dout=6.7e-03] [train aL=9.45e-01 err=0.45 nd=400/400] [test aL=9.58e-01 err=0.45]\n",
            "[821 +21] [dt=3.8e-03 dgrad=1.1e-04 dout=6.4e-03]\n",
            "[i=823 t=2.54e+02 wall=16] [dt=4.5e-03 dgrad=1.6e-06 dout=7.8e-04] [train aL=9.44e-01 err=0.45 nd=398/400] [test aL=9.57e-01 err=0.45]\n",
            "[i=831 t=2.54e+02 wall=17] [dt=9.7e-03 dgrad=8.4e-06 dout=1.6e-03] [train aL=9.43e-01 err=0.45 nd=398/400] [test aL=9.56e-01 err=0.45]\n",
            "[i=839 t=2.54e+02 wall=17] [dt=2.1e-02 dgrad=5.8e-05 dout=3.4e-03] [train aL=9.42e-01 err=0.45 nd=396/400] [test aL=9.56e-01 err=0.45]\n",
            "[i=848 t=2.54e+02 wall=17] [dt=2.3e-02 dgrad=5.4e-05 dout=3.4e-03] [train aL=9.41e-01 err=0.44 nd=395/400] [test aL=9.55e-01 err=0.45]\n",
            "[i=857 t=2.54e+02 wall=17] [dt=2.3e-02 dgrad=7.1e-05 dout=3.2e-03] [train aL=9.40e-01 err=0.43 nd=394/400] [test aL=9.55e-01 err=0.45]\n",
            "[i=866 t=2.55e+02 wall=17] [dt=2.3e-02 dgrad=6.1e-05 dout=2.9e-03] [train aL=9.38e-01 err=0.43 nd=393/400] [test aL=9.54e-01 err=0.46]\n",
            "[i=875 t=2.55e+02 wall=17] [dt=2.5e-02 dgrad=7.2e-05 dout=2.9e-03] [train aL=9.37e-01 err=0.43 nd=393/400] [test aL=9.54e-01 err=0.45]\n",
            "[i=884 t=2.55e+02 wall=17] [dt=2.5e-02 dgrad=8.4e-05 dout=2.7e-03] [train aL=9.36e-01 err=0.44 nd=392/400] [test aL=9.54e-01 err=0.46]\n",
            "[i=893 t=2.55e+02 wall=18] [dt=2.5e-02 dgrad=7.1e-05 dout=2.5e-03] [train aL=9.35e-01 err=0.43 nd=391/400] [test aL=9.54e-01 err=0.46]\n",
            "[895 +74] [dt=2.5e-03 dgrad=1.1e-04 dout=2.4e-03]\n",
            "[i=902 t=2.55e+02 wall=18] [dt=4.9e-03 dgrad=4.6e-06 dout=4.7e-04] [train aL=9.34e-01 err=0.43 nd=391/400] [test aL=9.54e-01 err=0.46]\n",
            "[i=911 t=2.55e+02 wall=18] [dt=1.2e-02 dgrad=2.3e-05 dout=1.1e-03] [train aL=9.34e-01 err=0.43 nd=391/400] [test aL=9.54e-01 err=0.46]\n",
            "[i=920 t=2.56e+02 wall=18] [dt=2.1e-02 dgrad=5.8e-05 dout=1.8e-03] [train aL=9.33e-01 err=0.43 nd=391/400] [test aL=9.55e-01 err=0.46]\n",
            "[i=929 t=2.56e+02 wall=18] [dt=2.5e-02 dgrad=8.3e-05 dout=2.1e-03] [train aL=9.32e-01 err=0.43 nd=389/400] [test aL=9.55e-01 err=0.46]\n",
            "[930 +35] [dt=2.5e-03 dgrad=1.2e-04 dout=2.0e-03]\n",
            "[i=938 t=2.56e+02 wall=18] [dt=5.3e-03 dgrad=7.5e-06 dout=4.4e-04] [train aL=9.32e-01 err=0.43 nd=389/400] [test aL=9.55e-01 err=0.46]\n",
            "[i=947 t=2.56e+02 wall=19] [dt=1.3e-02 dgrad=2.1e-05 dout=1.0e-03] [train aL=9.32e-01 err=0.43 nd=389/400] [test aL=9.55e-01 err=0.47]\n",
            "[i=957 t=2.56e+02 wall=19] [dt=2.2e-02 dgrad=4.7e-05 dout=1.8e-03] [train aL=9.31e-01 err=0.42 nd=388/400] [test aL=9.55e-01 err=0.47]\n",
            "[959 +29] [dt=2.5e-03 dgrad=1.0e-04 dout=2.0e-03]\n",
            "[i=967 t=2.56e+02 wall=19] [dt=5.3e-03 dgrad=4.1e-06 dout=4.3e-04] [train aL=9.31e-01 err=0.42 nd=388/400] [test aL=9.55e-01 err=0.47]\n",
            "[i=977 t=2.56e+02 wall=19] [dt=1.1e-02 dgrad=6.0e-05 dout=9.3e-04] [train aL=9.30e-01 err=0.42 nd=388/400] [test aL=9.56e-01 err=0.47]\n",
            "[i=987 t=2.56e+02 wall=20] [dt=2.4e-02 dgrad=5.4e-05 dout=1.9e-03] [train aL=9.30e-01 err=0.41 nd=386/400] [test aL=9.56e-01 err=0.47]\n",
            "[993 +34] [dt=2.4e-03 dgrad=1.1e-04 dout=1.9e-03]\n",
            "[i=997 t=2.57e+02 wall=20] [dt=3.5e-03 dgrad=2.4e-06 dout=2.8e-04] [train aL=9.29e-01 err=0.41 nd=386/400] [test aL=9.56e-01 err=0.47]\n",
            "[i=1007 t=2.57e+02 wall=20] [dt=9.2e-03 dgrad=2.1e-05 dout=7.5e-04] [train aL=9.29e-01 err=0.42 nd=385/400] [test aL=9.56e-01 err=0.47]\n",
            "[i=1017 t=2.57e+02 wall=20] [dt=2.4e-02 dgrad=6.5e-05 dout=2.2e-03] [train aL=9.28e-01 err=0.41 nd=384/400] [test aL=9.57e-01 err=0.47]\n",
            "[1018 +25] [dt=2.4e-03 dgrad=1.6e-04 dout=2.2e-03]\n",
            "[i=1027 t=2.57e+02 wall=21] [dt=5.6e-03 dgrad=4.0e-06 dout=5.3e-04] [train aL=9.28e-01 err=0.42 nd=384/400] [test aL=9.57e-01 err=0.47]\n",
            "[i=1037 t=2.57e+02 wall=21] [dt=1.5e-02 dgrad=3.9e-05 dout=1.5e-03] [train aL=9.28e-01 err=0.42 nd=384/400] [test aL=9.57e-01 err=0.47]\n",
            "[1042 +24] [dt=2.1e-03 dgrad=1.5e-04 dout=2.2e-03]\n",
            "[i=1047 t=2.57e+02 wall=21] [dt=3.4e-03 dgrad=4.2e-06 dout=3.5e-04] [train aL=9.27e-01 err=0.42 nd=384/400] [test aL=9.57e-01 err=0.47]\n",
            "[i=1057 t=2.57e+02 wall=21] [dt=8.9e-03 dgrad=3.3e-05 dout=9.1e-04] [train aL=9.27e-01 err=0.42 nd=384/400] [test aL=9.57e-01 err=0.47]\n",
            "[1062 +20] [dt=1.1e-03 dgrad=1.1e-04 dout=1.1e-03]\n",
            "[i=1068 t=2.57e+02 wall=21] [dt=1.9e-03 dgrad=3.6e-06 dout=1.9e-04] [train aL=9.27e-01 err=0.42 nd=383/400] [test aL=9.57e-01 err=0.47]\n",
            "[i=1079 t=2.57e+02 wall=22] [dt=5.4e-03 dgrad=1.3e-05 dout=5.3e-04] [train aL=9.27e-01 err=0.42 nd=383/400] [test aL=9.57e-01 err=0.47]\n",
            "[i=1090 t=2.57e+02 wall=22] [dt=1.6e-02 dgrad=4.4e-05 dout=1.4e-03] [train aL=9.26e-01 err=0.42 nd=383/400] [test aL=9.58e-01 err=0.47]\n",
            "[i=1101 t=2.57e+02 wall=22] [dt=2.3e-02 dgrad=6.8e-05 dout=2.0e-03] [train aL=9.26e-01 err=0.42 nd=382/400] [test aL=9.58e-01 err=0.47]\n",
            "[1103 +41] [dt=2.3e-03 dgrad=1.3e-04 dout=2.0e-03]\n",
            "[i=1112 t=2.58e+02 wall=22] [dt=5.4e-03 dgrad=8.2e-06 dout=4.8e-04] [train aL=9.25e-01 err=0.42 nd=382/400] [test aL=9.58e-01 err=0.47]\n",
            "[i=1123 t=2.58e+02 wall=22] [dt=1.5e-02 dgrad=1.7e-05 dout=1.5e-03] [train aL=9.25e-01 err=0.42 nd=382/400] [test aL=9.58e-01 err=0.47]\n",
            "[1128 +25] [dt=2.2e-03 dgrad=1.0e-04 dout=2.2e-03]\n",
            "[i=1134 t=2.58e+02 wall=22] [dt=4.0e-03 dgrad=2.9e-06 dout=3.8e-04] [train aL=9.25e-01 err=0.43 nd=382/400] [test aL=9.59e-01 err=0.47]\n",
            "[i=1145 t=2.58e+02 wall=23] [dt=1.1e-02 dgrad=1.1e-05 dout=1.1e-03] [train aL=9.24e-01 err=0.43 nd=382/400] [test aL=9.59e-01 err=0.47]\n",
            "[i=1156 t=2.58e+02 wall=23] [dt=1.8e-02 dgrad=8.1e-05 dout=1.5e-03] [train aL=9.23e-01 err=0.43 nd=382/400] [test aL=9.59e-01 err=0.47]\n",
            "[1162 +34] [dt=2.0e-03 dgrad=1.1e-04 dout=1.6e-03]\n",
            "[i=1167 t=2.58e+02 wall=23] [dt=3.2e-03 dgrad=2.9e-06 dout=2.5e-04] [train aL=9.23e-01 err=0.43 nd=382/400] [test aL=9.59e-01 err=0.47]\n",
            "[i=1179 t=2.58e+02 wall=23] [dt=1.0e-02 dgrad=1.2e-05 dout=7.7e-04] [train aL=9.23e-01 err=0.43 nd=382/400] [test aL=9.59e-01 err=0.48]\n",
            "[i=1191 t=2.58e+02 wall=23] [dt=2.6e-02 dgrad=5.7e-05 dout=2.1e-03] [train aL=9.22e-01 err=0.43 nd=381/400] [test aL=9.60e-01 err=0.47]\n",
            "[i=1203 t=2.59e+02 wall=24] [dt=3.5e-02 dgrad=5.9e-05 dout=3.0e-03] [train aL=9.20e-01 err=0.42 nd=379/400] [test aL=9.60e-01 err=0.47]\n",
            "[i=1215 t=2.59e+02 wall=24] [dt=4.2e-02 dgrad=4.4e-05 dout=4.0e-03] [train aL=9.18e-01 err=0.43 nd=378/400] [test aL=9.61e-01 err=0.47]\n",
            "[1218 +56] [dt=4.7e-03 dgrad=1.4e-04 dout=4.5e-03]\n",
            "[i=1227 t=2.59e+02 wall=24] [dt=1.1e-02 dgrad=3.6e-06 dout=1.1e-03] [train aL=9.18e-01 err=0.43 nd=378/400] [test aL=9.61e-01 err=0.48]\n",
            "[i=1239 t=2.60e+02 wall=24] [dt=3.4e-02 dgrad=2.4e-05 dout=3.5e-03] [train aL=9.16e-01 err=0.44 nd=378/400] [test aL=9.61e-01 err=0.47]\n",
            "[1250 +32] [dt=5.6e-03 dgrad=1.3e-04 dout=6.4e-03]\n",
            "[i=1251 t=2.60e+02 wall=24] [dt=6.1e-03 dgrad=1.5e-06 dout=7.0e-04] [train aL=9.14e-01 err=0.44 nd=377/400] [test aL=9.62e-01 err=0.47]\n",
            "[i=1263 t=2.60e+02 wall=25] [dt=1.9e-02 dgrad=1.9e-05 dout=2.3e-03] [train aL=9.13e-01 err=0.44 nd=377/400] [test aL=9.62e-01 err=0.48]\n",
            "[i=1275 t=2.61e+02 wall=25] [dt=4.5e-02 dgrad=5.0e-05 dout=5.9e-03] [train aL=9.11e-01 err=0.43 nd=377/400] [test aL=9.63e-01 err=0.46]\n",
            "[1280 +30] [dt=5.5e-03 dgrad=1.1e-04 dout=7.6e-03]\n",
            "[i=1287 t=2.61e+02 wall=25] [dt=1.1e-02 dgrad=3.4e-06 dout=1.5e-03] [train aL=9.09e-01 err=0.43 nd=376/400] [test aL=9.63e-01 err=0.47]\n",
            "[i=1300 t=2.61e+02 wall=25] [dt=3.7e-02 dgrad=3.2e-05 dout=5.5e-03] [train aL=9.08e-01 err=0.43 nd=376/400] [test aL=9.64e-01 err=0.47]\n",
            "[1310 +30] [dt=4.9e-03 dgrad=1.1e-04 dout=8.2e-03]\n",
            "[i=1313 t=2.62e+02 wall=25] [dt=6.5e-03 dgrad=1.9e-06 dout=1.1e-03] [train aL=9.05e-01 err=0.43 nd=376/400] [test aL=9.64e-01 err=0.46]\n",
            "[i=1326 t=2.62e+02 wall=25] [dt=2.2e-02 dgrad=1.2e-05 dout=3.9e-03] [train aL=9.04e-01 err=0.43 nd=376/400] [test aL=9.65e-01 err=0.47]\n",
            "[i=1339 t=2.62e+02 wall=26] [dt=5.3e-02 dgrad=6.3e-05 dout=1.0e-02] [train aL=9.00e-01 err=0.42 nd=377/400] [test aL=9.66e-01 err=0.47]\n",
            "[1348 +38] [dt=7.1e-03 dgrad=1.0e-04 dout=1.5e-02]\n",
            "[i=1352 t=2.63e+02 wall=26] [dt=1.0e-02 dgrad=2.3e-06 dout=2.2e-03] [train aL=8.96e-01 err=0.42 nd=378/400] [test aL=9.67e-01 err=0.47]\n",
            "[i=1365 t=2.63e+02 wall=26] [dt=3.6e-02 dgrad=5.3e-05 dout=7.7e-03] [train aL=8.94e-01 err=0.42 nd=378/400] [test aL=9.68e-01 err=0.47]\n",
            "[1369 +21] [dt=3.6e-03 dgrad=1.1e-04 dout=7.5e-03]\n",
            "[i=1378 t=2.63e+02 wall=26] [dt=8.4e-03 dgrad=6.7e-06 dout=1.8e-03] [train aL=8.93e-01 err=0.43 nd=378/400] [test aL=9.68e-01 err=0.47]\n",
            "[i=1391 t=2.64e+02 wall=27] [dt=2.2e-02 dgrad=6.9e-05 dout=4.4e-03] [train aL=8.91e-01 err=0.43 nd=374/400] [test aL=9.69e-01 err=0.47]\n",
            "[1400 +31] [dt=2.2e-03 dgrad=1.0e-04 dout=4.1e-03]\n",
            "[i=1404 t=2.64e+02 wall=27] [dt=3.2e-03 dgrad=2.2e-06 dout=6.0e-04] [train aL=8.90e-01 err=0.43 nd=373/400] [test aL=9.69e-01 err=0.46]\n",
            "[i=1418 t=2.64e+02 wall=27] [dt=1.2e-02 dgrad=3.7e-05 dout=2.2e-03] [train aL=8.90e-01 err=0.43 nd=371/400] [test aL=9.70e-01 err=0.46]\n",
            "[i=1432 t=2.64e+02 wall=27] [dt=1.5e-02 dgrad=6.3e-05 dout=2.3e-03] [train aL=8.89e-01 err=0.43 nd=370/400] [test aL=9.70e-01 err=0.46]\n",
            "[i=1446 t=2.64e+02 wall=27] [dt=1.5e-02 dgrad=6.7e-05 dout=2.0e-03] [train aL=8.88e-01 err=0.43 nd=370/400] [test aL=9.71e-01 err=0.47]\n",
            "[i=1460 t=2.65e+02 wall=28] [dt=1.5e-02 dgrad=6.4e-05 dout=1.8e-03] [train aL=8.87e-01 err=0.43 nd=369/400] [test aL=9.72e-01 err=0.47]\n",
            "[i=1474 t=2.65e+02 wall=28] [dt=1.5e-02 dgrad=5.6e-05 dout=1.5e-03] [train aL=8.87e-01 err=0.43 nd=367/400] [test aL=9.72e-01 err=0.47]\n",
            "[i=1488 t=2.65e+02 wall=28] [dt=1.5e-02 dgrad=5.3e-05 dout=1.3e-03] [train aL=8.86e-01 err=0.43 nd=363/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1502 t=2.65e+02 wall=28] [dt=1.8e-02 dgrad=5.8e-05 dout=1.3e-03] [train aL=8.85e-01 err=0.43 nd=362/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1516 t=2.65e+02 wall=28] [dt=2.0e-02 dgrad=7.9e-05 dout=1.2e-03] [train aL=8.85e-01 err=0.42 nd=358/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1531 t=2.66e+02 wall=29] [dt=2.4e-02 dgrad=5.2e-05 dout=1.2e-03] [train aL=8.84e-01 err=0.42 nd=356/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1546 t=2.66e+02 wall=29] [dt=2.6e-02 dgrad=5.6e-05 dout=1.2e-03] [train aL=8.83e-01 err=0.42 nd=353/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1561 t=2.67e+02 wall=29] [dt=2.6e-02 dgrad=7.9e-05 dout=1.1e-03] [train aL=8.83e-01 err=0.42 nd=352/400] [test aL=9.73e-01 err=0.47]\n",
            "[1568 +168] [dt=2.6e-03 dgrad=1.1e-04 dout=1.1e-03]\n",
            "[i=1576 t=2.67e+02 wall=29] [dt=5.6e-03 dgrad=6.0e-06 dout=2.5e-04] [train aL=8.82e-01 err=0.42 nd=352/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1591 t=2.67e+02 wall=30] [dt=1.9e-02 dgrad=5.0e-05 dout=9.6e-04] [train aL=8.82e-01 err=0.42 nd=352/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1606 t=2.67e+02 wall=30] [dt=2.3e-02 dgrad=5.8e-05 dout=1.2e-03] [train aL=8.81e-01 err=0.42 nd=351/400] [test aL=9.73e-01 err=0.47]\n",
            "[1616 +48] [dt=2.3e-03 dgrad=1.0e-04 dout=1.2e-03]\n",
            "[i=1621 t=2.67e+02 wall=30] [dt=3.8e-03 dgrad=2.8e-06 dout=1.9e-04] [train aL=8.81e-01 err=0.42 nd=351/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1636 t=2.68e+02 wall=30] [dt=1.6e-02 dgrad=4.2e-05 dout=8.1e-04] [train aL=8.80e-01 err=0.42 nd=351/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1652 t=2.68e+02 wall=31] [dt=1.9e-02 dgrad=5.7e-05 dout=9.5e-04] [train aL=8.80e-01 err=0.42 nd=352/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1668 t=2.68e+02 wall=31] [dt=2.1e-02 dgrad=5.3e-05 dout=1.1e-03] [train aL=8.79e-01 err=0.42 nd=350/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1684 t=2.69e+02 wall=31] [dt=2.8e-02 dgrad=5.6e-05 dout=1.4e-03] [train aL=8.78e-01 err=0.42 nd=351/400] [test aL=9.73e-01 err=0.47]\n",
            "[1688 +72] [dt=2.8e-03 dgrad=1.0e-04 dout=1.4e-03]\n",
            "[i=1700 t=2.69e+02 wall=32] [dt=8.7e-03 dgrad=7.9e-06 dout=4.6e-04] [train aL=8.78e-01 err=0.42 nd=351/400] [test aL=9.73e-01 err=0.47]\n",
            "[1712 +24] [dt=2.3e-03 dgrad=1.0e-04 dout=1.2e-03]\n",
            "[i=1716 t=2.69e+02 wall=32] [dt=3.3e-03 dgrad=2.2e-06 dout=1.8e-04] [train aL=8.78e-01 err=0.42 nd=350/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1732 t=2.69e+02 wall=32] [dt=1.5e-02 dgrad=3.0e-05 dout=7.7e-04] [train aL=8.77e-01 err=0.42 nd=350/400] [test aL=9.73e-01 err=0.47]\n",
            "[1743 +31] [dt=2.4e-03 dgrad=1.1e-04 dout=1.2e-03]\n",
            "[i=1748 t=2.69e+02 wall=33] [dt=3.9e-03 dgrad=3.1e-06 dout=1.9e-04] [train aL=8.77e-01 err=0.42 nd=350/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1765 t=2.70e+02 wall=33] [dt=2.0e-02 dgrad=6.2e-05 dout=9.8e-04] [train aL=8.77e-01 err=0.42 nd=350/400] [test aL=9.73e-01 err=0.47]\n",
            "[1776 +33] [dt=2.7e-03 dgrad=1.0e-04 dout=1.2e-03]\n",
            "[i=1782 t=2.70e+02 wall=33] [dt=4.7e-03 dgrad=3.5e-06 dout=2.1e-04] [train aL=8.76e-01 err=0.43 nd=349/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1799 t=2.70e+02 wall=34] [dt=2.4e-02 dgrad=5.8e-05 dout=1.2e-03] [train aL=8.76e-01 err=0.43 nd=347/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1816 t=2.70e+02 wall=34] [dt=3.2e-02 dgrad=6.9e-05 dout=1.3e-03] [train aL=8.75e-01 err=0.43 nd=347/400] [test aL=9.73e-01 err=0.47]\n",
            "[i=1833 t=2.71e+02 wall=34] [dt=3.8e-02 dgrad=4.9e-05 dout=1.6e-03] [train aL=8.74e-01 err=0.42 nd=347/400] [test aL=9.74e-01 err=0.47]\n",
            "[1838 +62] [dt=4.2e-03 dgrad=1.1e-04 dout=1.7e-03]\n",
            "[i=1850 t=2.71e+02 wall=35] [dt=1.3e-02 dgrad=9.2e-06 dout=5.3e-04] [train aL=8.73e-01 err=0.42 nd=347/400] [test aL=9.74e-01 err=0.47]\n",
            "[i=1867 t=2.72e+02 wall=35] [dt=4.1e-02 dgrad=6.4e-05 dout=1.7e-03] [train aL=8.72e-01 err=0.42 nd=347/400] [test aL=9.74e-01 err=0.47]\n",
            "[1878 +40] [dt=4.6e-03 dgrad=1.2e-04 dout=1.9e-03]\n",
            "[i=1885 t=2.72e+02 wall=35] [dt=8.9e-03 dgrad=5.2e-06 dout=3.6e-04] [train aL=8.71e-01 err=0.41 nd=347/400] [test aL=9.75e-01 err=0.47]\n",
            "[i=1903 t=2.73e+02 wall=35] [dt=3.7e-02 dgrad=5.1e-05 dout=1.7e-03] [train aL=8.70e-01 err=0.41 nd=347/400] [test aL=9.75e-01 err=0.47]\n",
            "[i=1921 t=2.73e+02 wall=36] [dt=4.1e-02 dgrad=5.9e-05 dout=1.7e-03] [train aL=8.69e-01 err=0.41 nd=348/400] [test aL=9.76e-01 err=0.47]\n",
            "[i=1939 t=2.74e+02 wall=36] [dt=5.4e-02 dgrad=5.5e-05 dout=2.1e-03] [train aL=8.68e-01 err=0.41 nd=347/400] [test aL=9.76e-01 err=0.47]\n",
            "[i=1957 t=2.75e+02 wall=36] [dt=6.6e-02 dgrad=5.4e-05 dout=2.4e-03] [train aL=8.66e-01 err=0.41 nd=345/400] [test aL=9.77e-01 err=0.47]\n",
            "[i=1975 t=2.77e+02 wall=37] [dt=6.6e-02 dgrad=7.9e-05 dout=2.2e-03] [train aL=8.63e-01 err=0.41 nd=343/400] [test aL=9.78e-01 err=0.46]\n",
            "[i=1993 t=2.78e+02 wall=37] [dt=6.6e-02 dgrad=7.0e-05 dout=2.3e-03] [train aL=8.61e-01 err=0.41 nd=342/400] [test aL=9.79e-01 err=0.46]\n",
            "[1998 +120] [dt=6.6e-03 dgrad=1.1e-04 dout=2.4e-03]\n",
            "[i=2012 t=2.78e+02 wall=37] [dt=2.5e-02 dgrad=1.2e-05 dout=9.2e-04] [train aL=8.59e-01 err=0.42 nd=342/400] [test aL=9.79e-01 err=0.46]\n",
            "[i=2031 t=2.79e+02 wall=37] [dt=6.5e-02 dgrad=7.3e-05 dout=2.4e-03] [train aL=8.57e-01 err=0.42 nd=342/400] [test aL=9.80e-01 err=0.46]\n",
            "[2040 +42] [dt=6.5e-03 dgrad=1.0e-04 dout=2.6e-03]\n",
            "[i=2050 t=2.80e+02 wall=38] [dt=1.7e-02 dgrad=7.4e-06 dout=6.9e-04] [train aL=8.55e-01 err=0.41 nd=340/400] [test aL=9.80e-01 err=0.46]\n",
            "[i=2069 t=2.81e+02 wall=38] [dt=5.8e-02 dgrad=5.7e-05 dout=2.5e-03] [train aL=8.53e-01 err=0.41 nd=341/400] [test aL=9.81e-01 err=0.46]\n",
            "[i=2088 t=2.82e+02 wall=38] [dt=7.0e-02 dgrad=6.1e-05 dout=2.9e-03] [train aL=8.50e-01 err=0.41 nd=341/400] [test aL=9.81e-01 err=0.46]\n",
            "[i=2107 t=2.83e+02 wall=39] [dt=7.0e-02 dgrad=5.4e-05 dout=2.6e-03] [train aL=8.45e-01 err=0.41 nd=339/400] [test aL=9.81e-01 err=0.46]\n",
            "[i=2126 t=2.85e+02 wall=39] [dt=7.0e-02 dgrad=8.4e-05 dout=2.4e-03] [train aL=8.41e-01 err=0.40 nd=339/400] [test aL=9.80e-01 err=0.46]\n",
            "[2128 +88] [dt=7.0e-03 dgrad=1.0e-04 dout=2.4e-03]\n",
            "[i=2146 t=2.85e+02 wall=39] [dt=3.9e-02 dgrad=2.1e-05 dout=1.3e-03] [train aL=8.39e-01 err=0.40 nd=339/400] [test aL=9.80e-01 err=0.46]\n",
            "[2155 +27] [dt=6.9e-03 dgrad=1.1e-04 dout=2.3e-03]\n",
            "[i=2166 t=2.86e+02 wall=39] [dt=2.0e-02 dgrad=7.6e-06 dout=6.5e-04] [train aL=8.37e-01 err=0.40 nd=337/400] [test aL=9.80e-01 err=0.46]\n",
            "[i=2186 t=2.87e+02 wall=40] [dt=7.5e-02 dgrad=6.9e-05 dout=2.7e-03] [train aL=8.33e-01 err=0.40 nd=336/400] [test aL=9.79e-01 err=0.46]\n",
            "[i=2206 t=2.88e+02 wall=40] [dt=8.2e-02 dgrad=8.3e-05 dout=3.1e-03] [train aL=8.26e-01 err=0.40 nd=333/400] [test aL=9.77e-01 err=0.46]\n",
            "[i=2226 t=2.90e+02 wall=40] [dt=8.2e-02 dgrad=8.1e-05 dout=3.1e-03] [train aL=8.18e-01 err=0.39 nd=333/400] [test aL=9.73e-01 err=0.45]\n",
            "[2230 +75] [dt=8.2e-03 dgrad=1.1e-04 dout=3.3e-03]\n",
            "[i=2246 t=2.90e+02 wall=41] [dt=3.8e-02 dgrad=2.2e-05 dout=1.6e-03] [train aL=8.14e-01 err=0.39 nd=332/400] [test aL=9.72e-01 err=0.46]\n",
            "[2256 +26] [dt=5.5e-03 dgrad=1.3e-04 dout=2.5e-03]\n",
            "[i=2267 t=2.91e+02 wall=41] [dt=1.6e-02 dgrad=6.4e-06 dout=7.3e-04] [train aL=8.11e-01 err=0.39 nd=332/400] [test aL=9.70e-01 err=0.45]\n",
            "[2281 +25] [dt=5.5e-03 dgrad=1.0e-04 dout=2.8e-03]\n",
            "[i=2288 t=2.92e+02 wall=41] [dt=1.1e-02 dgrad=4.2e-06 dout=5.5e-04] [train aL=8.08e-01 err=0.39 nd=330/400] [test aL=9.69e-01 err=0.45]\n",
            "[i=2309 t=2.92e+02 wall=42] [dt=4.4e-02 dgrad=8.8e-05 dout=2.6e-03] [train aL=8.03e-01 err=0.39 nd=328/400] [test aL=9.67e-01 err=0.46]\n",
            "[2325 +44] [dt=5.9e-03 dgrad=1.1e-04 dout=3.4e-03]\n",
            "[i=2330 t=2.93e+02 wall=42] [dt=9.5e-03 dgrad=3.0e-06 dout=5.5e-04] [train aL=7.97e-01 err=0.38 nd=328/400] [test aL=9.63e-01 err=0.45]\n",
            "[i=2351 t=2.94e+02 wall=42] [dt=4.0e-02 dgrad=6.7e-05 dout=2.8e-03] [train aL=7.93e-01 err=0.38 nd=327/400] [test aL=9.61e-01 err=0.45]\n",
            "[2359 +34] [dt=4.8e-03 dgrad=1.0e-04 dout=3.9e-03]\n",
            "[i=2372 t=2.94e+02 wall=43] [dt=1.7e-02 dgrad=1.0e-05 dout=1.3e-03] [train aL=7.90e-01 err=0.38 nd=326/400] [test aL=9.59e-01 err=0.45]\n",
            "[2386 +27] [dt=5.2e-03 dgrad=1.1e-04 dout=4.6e-03]\n",
            "[i=2394 t=2.94e+02 wall=43] [dt=1.1e-02 dgrad=9.2e-06 dout=1.0e-03] [train aL=7.85e-01 err=0.38 nd=325/400] [test aL=9.56e-01 err=0.46]\n",
            "[i=2416 t=2.95e+02 wall=43] [dt=5.7e-02 dgrad=5.4e-05 dout=5.5e-03] [train aL=7.78e-01 err=0.38 nd=326/400] [test aL=9.52e-01 err=0.46]\n",
            "[2426 +40] [dt=6.2e-03 dgrad=1.7e-04 dout=6.1e-03]\n",
            "[i=2438 t=2.96e+02 wall=44] [dt=2.0e-02 dgrad=1.1e-05 dout=1.9e-03] [train aL=7.72e-01 err=0.38 nd=323/400] [test aL=9.47e-01 err=0.46]\n",
            "[2453 +27] [dt=4.6e-03 dgrad=1.4e-04 dout=4.8e-03]\n",
            "[i=2460 t=2.96e+02 wall=44] [dt=9.0e-03 dgrad=9.8e-06 dout=9.5e-04] [train aL=7.66e-01 err=0.37 nd=322/400] [test aL=9.44e-01 err=0.45]\n",
            "[i=2482 t=2.97e+02 wall=45] [dt=2.8e-02 dgrad=5.4e-05 dout=3.8e-03] [train aL=7.60e-01 err=0.36 nd=322/400] [test aL=9.40e-01 err=0.45]\n",
            "[2488 +35] [dt=3.4e-03 dgrad=1.2e-04 dout=4.7e-03]\n",
            "[i=2504 t=2.97e+02 wall=45] [dt=1.6e-02 dgrad=1.4e-05 dout=2.3e-03] [train aL=7.57e-01 err=0.35 nd=320/400] [test aL=9.38e-01 err=0.45]\n",
            "[2522 +34] [dt=3.4e-03 dgrad=1.1e-04 dout=4.9e-03]\n",
            "[i=2527 t=2.98e+02 wall=45] [dt=5.4e-03 dgrad=3.0e-06 dout=7.8e-04] [train aL=7.50e-01 err=0.34 nd=321/400] [test aL=9.33e-01 err=0.45]\n",
            "[i=2550 t=2.98e+02 wall=46] [dt=3.6e-02 dgrad=8.2e-05 dout=5.4e-03] [train aL=7.44e-01 err=0.34 nd=321/400] [test aL=9.28e-01 err=0.45]\n",
            "[2551 +29] [dt=3.6e-03 dgrad=1.6e-04 dout=5.4e-03]\n",
            "[i=2573 t=2.98e+02 wall=46] [dt=2.7e-02 dgrad=5.9e-05 dout=4.1e-03] [train aL=7.40e-01 err=0.34 nd=320/400] [test aL=9.25e-01 err=0.44]\n",
            "[2591 +40] [dt=3.3e-03 dgrad=2.1e-04 dout=5.2e-03]\n",
            "[i=2596 t=2.99e+02 wall=47] [dt=5.2e-03 dgrad=6.0e-06 dout=8.5e-04] [train aL=7.31e-01 err=0.33 nd=320/400] [test aL=9.19e-01 err=0.44]\n",
            "[i=2619 t=2.99e+02 wall=47] [dt=3.2e-02 dgrad=5.6e-05 dout=5.2e-03] [train aL=7.24e-01 err=0.33 nd=318/400] [test aL=9.13e-01 err=0.44]\n",
            "[2623 +32] [dt=3.5e-03 dgrad=1.1e-04 dout=5.7e-03]\n",
            "[i=2643 t=3.00e+02 wall=48] [dt=2.4e-02 dgrad=3.2e-05 dout=3.9e-03] [train aL=7.18e-01 err=0.32 nd=318/400] [test aL=9.09e-01 err=0.43]\n",
            "[2655 +32] [dt=3.2e-03 dgrad=1.1e-04 dout=5.5e-03]\n",
            "[i=2667 t=3.00e+02 wall=48] [dt=9.9e-03 dgrad=1.2e-05 dout=1.7e-03] [train aL=7.11e-01 err=0.31 nd=316/400] [test aL=9.03e-01 err=0.43]\n",
            "[2681 +26] [dt=3.4e-03 dgrad=1.0e-04 dout=6.5e-03]\n",
            "[i=2691 t=3.00e+02 wall=48] [dt=8.9e-03 dgrad=6.7e-06 dout=1.7e-03] [train aL=7.04e-01 err=0.30 nd=313/400] [test aL=8.97e-01 err=0.42]\n",
            "[2708 +27] [dt=3.1e-03 dgrad=1.2e-04 dout=6.3e-03]\n",
            "[i=2715 t=3.01e+02 wall=49] [dt=6.0e-03 dgrad=3.9e-06 dout=1.2e-03] [train aL=6.97e-01 err=0.29 nd=313/400] [test aL=8.91e-01 err=0.41]\n",
            "[2732 +24] [dt=2.7e-03 dgrad=1.2e-04 dout=5.8e-03]\n",
            "[i=2739 t=3.01e+02 wall=49] [dt=5.3e-03 dgrad=9.3e-06 dout=1.1e-03] [train aL=6.92e-01 err=0.28 nd=315/400] [test aL=8.86e-01 err=0.41]\n",
            "[2759 +27] [dt=2.7e-03 dgrad=1.6e-04 dout=6.0e-03]\n",
            "[i=2763 t=3.01e+02 wall=50] [dt=4.0e-03 dgrad=3.6e-06 dout=8.8e-04] [train aL=6.85e-01 err=0.29 nd=315/400] [test aL=8.80e-01 err=0.41]\n",
            "[i=2788 t=3.02e+02 wall=50] [dt=2.2e-02 dgrad=5.1e-05 dout=5.3e-03] [train aL=6.77e-01 err=0.29 nd=315/400] [test aL=8.73e-01 err=0.40]\n",
            "[2790 +31] [dt=2.2e-03 dgrad=1.7e-04 dout=5.3e-03]\n",
            "[i=2813 t=3.02e+02 wall=50] [dt=1.8e-02 dgrad=6.9e-05 dout=4.1e-03] [train aL=6.72e-01 err=0.29 nd=314/400] [test aL=8.68e-01 err=0.41]\n",
            "[2820 +30] [dt=2.4e-03 dgrad=1.0e-04 dout=5.4e-03]\n",
            "[i=2838 t=3.02e+02 wall=51] [dt=1.3e-02 dgrad=3.6e-05 dout=2.9e-03] [train aL=6.66e-01 err=0.28 nd=311/400] [test aL=8.63e-01 err=0.41]\n",
            "[2843 +23] [dt=1.9e-03 dgrad=1.1e-04 dout=4.4e-03]\n",
            "[i=2863 t=3.02e+02 wall=51] [dt=1.1e-02 dgrad=4.9e-05 dout=2.5e-03] [train aL=6.61e-01 err=0.28 nd=311/400] [test aL=8.58e-01 err=0.40]\n",
            "[2870 +27] [dt=1.7e-03 dgrad=1.6e-04 dout=4.1e-03]\n",
            "[i=2888 t=3.03e+02 wall=52] [dt=9.7e-03 dgrad=3.9e-05 dout=2.3e-03] [train aL=6.56e-01 err=0.28 nd=311/400] [test aL=8.54e-01 err=0.40]\n",
            "[2902 +32] [dt=2.3e-03 dgrad=1.7e-04 dout=5.5e-03]\n",
            "[i=2914 t=3.03e+02 wall=52] [dt=7.1e-03 dgrad=4.5e-06 dout=1.7e-03] [train aL=6.49e-01 err=0.28 nd=310/400] [test aL=8.47e-01 err=0.40]\n",
            "[2927 +25] [dt=1.9e-03 dgrad=2.0e-04 dout=4.5e-03]\n",
            "[i=2940 t=3.03e+02 wall=52] [dt=6.4e-03 dgrad=9.2e-06 dout=1.5e-03] [train aL=6.43e-01 err=0.28 nd=311/400] [test aL=8.41e-01 err=0.39]\n",
            "[2954 +27] [dt=1.8e-03 dgrad=1.0e-04 dout=4.6e-03]\n",
            "[i=2966 t=3.03e+02 wall=53] [dt=5.7e-03 dgrad=1.0e-05 dout=1.5e-03] [train aL=6.37e-01 err=0.28 nd=310/400] [test aL=8.35e-01 err=0.38]\n",
            "[2977 +23] [dt=1.6e-03 dgrad=1.0e-04 dout=4.1e-03]\n",
            "[i=2992 t=3.03e+02 wall=53] [dt=6.8e-03 dgrad=5.3e-06 dout=1.8e-03] [train aL=6.32e-01 err=0.27 nd=309/400] [test aL=8.30e-01 err=0.38]\n",
            "[3012 +35] [dt=1.5e-03 dgrad=1.4e-04 dout=4.0e-03]\n",
            "[i=3018 t=3.04e+02 wall=54] [dt=2.6e-03 dgrad=6.6e-06 dout=7.1e-04] [train aL=6.24e-01 err=0.27 nd=306/400] [test aL=8.22e-01 err=0.38]\n",
            "[3032 +20] [dt=9.8e-04 dgrad=1.2e-04 dout=3.0e-03]\n",
            "[i=3045 t=3.04e+02 wall=54] [dt=3.4e-03 dgrad=1.4e-05 dout=1.1e-03] [train aL=6.21e-01 err=0.27 nd=305/400] [test aL=8.19e-01 err=0.38]\n",
            "[3063 +31] [dt=1.4e-03 dgrad=1.3e-04 dout=4.8e-03]\n",
            "[i=3072 t=3.04e+02 wall=54] [dt=3.3e-03 dgrad=2.7e-05 dout=1.1e-03] [train aL=6.16e-01 err=0.27 nd=304/400] [test aL=8.13e-01 err=0.37]\n",
            "[3094 +31] [dt=1.4e-03 dgrad=1.3e-04 dout=4.8e-03]\n",
            "[i=3099 t=3.04e+02 wall=55] [dt=2.3e-03 dgrad=3.9e-06 dout=7.7e-04] [train aL=6.08e-01 err=0.27 nd=303/400] [test aL=8.06e-01 err=0.37]\n",
            "[3114 +20] [dt=8.6e-04 dgrad=1.4e-04 dout=3.2e-03]\n",
            "[i=3126 t=3.04e+02 wall=55] [dt=2.7e-03 dgrad=2.4e-05 dout=1.0e-03] [train aL=6.05e-01 err=0.27 nd=303/400] [test aL=8.02e-01 err=0.38]\n",
            "[3150 +36] [dt=1.1e-03 dgrad=1.1e-04 dout=4.5e-03]\n",
            "[i=3153 t=3.04e+02 wall=56] [dt=1.5e-03 dgrad=2.4e-06 dout=6.0e-04] [train aL=5.98e-01 err=0.26 nd=301/400] [test aL=7.96e-01 err=0.37]\n",
            "[3178 +28] [dt=1.6e-03 dgrad=1.6e-04 dout=6.9e-03]\n",
            "[i=3181 t=3.04e+02 wall=56] [dt=2.2e-03 dgrad=2.4e-06 dout=9.1e-04] [train aL=5.91e-01 err=0.26 nd=301/400] [test aL=7.89e-01 err=0.36]\n",
            "[3205 +27] [dt=1.6e-03 dgrad=1.0e-04 dout=6.6e-03]\n",
            "[i=3209 t=3.05e+02 wall=57] [dt=2.3e-03 dgrad=2.4e-06 dout=9.7e-04] [train aL=5.83e-01 err=0.25 nd=298/400] [test aL=7.80e-01 err=0.36]\n",
            "[3230 +25] [dt=1.4e-03 dgrad=1.0e-04 dout=5.9e-03]\n",
            "[i=3237 t=3.05e+02 wall=57] [dt=2.8e-03 dgrad=4.2e-06 dout=1.1e-03] [train aL=5.76e-01 err=0.25 nd=295/400] [test aL=7.72e-01 err=0.34]\n",
            "[3263 +33] [dt=1.5e-03 dgrad=1.5e-04 dout=6.1e-03]\n",
            "[i=3265 t=3.05e+02 wall=58] [dt=1.9e-03 dgrad=1.5e-06 dout=7.4e-04] [train aL=5.65e-01 err=0.24 nd=293/400] [test aL=7.61e-01 err=0.33]\n",
            "[3291 +28] [dt=1.5e-03 dgrad=1.8e-04 dout=6.0e-03]\n",
            "[i=3293 t=3.05e+02 wall=58] [dt=1.8e-03 dgrad=2.7e-06 dout=7.2e-04] [train aL=5.55e-01 err=0.23 nd=291/400] [test aL=7.52e-01 err=0.32]\n",
            "[3316 +25] [dt=1.1e-03 dgrad=1.1e-04 dout=4.5e-03]\n",
            "[i=3322 t=3.05e+02 wall=59] [dt=2.0e-03 dgrad=3.6e-06 dout=7.9e-04] [train aL=5.48e-01 err=0.23 nd=289/400] [test aL=7.45e-01 err=0.31]\n",
            "[3341 +25] [dt=1.0e-03 dgrad=1.8e-04 dout=3.8e-03]\n",
            "[i=3351 t=3.06e+02 wall=59] [dt=2.6e-03 dgrad=7.9e-06 dout=9.9e-04] [train aL=5.42e-01 err=0.23 nd=289/400] [test aL=7.39e-01 err=0.31]\n",
            "[3369 +28] [dt=1.5e-03 dgrad=1.5e-04 dout=5.4e-03]\n",
            "[i=3380 t=3.06e+02 wall=60] [dt=4.1e-03 dgrad=2.4e-06 dout=1.5e-03] [train aL=5.35e-01 err=0.22 nd=287/400] [test aL=7.31e-01 err=0.30]\n",
            "[i=3409 t=3.06e+02 wall=61] [dt=1.7e-02 dgrad=6.5e-05 dout=5.0e-03] [train aL=5.18e-01 err=0.21 nd=285/400] [test aL=7.15e-01 err=0.29]\n",
            "[3415 +46] [dt=2.1e-03 dgrad=1.3e-04 dout=6.2e-03]\n",
            "[3437 +22] [dt=1.7e-03 dgrad=1.0e-04 dout=5.1e-03]\n",
            "[i=3438 t=3.06e+02 wall=61] [dt=1.9e-03 dgrad=8.5e-07 dout=5.6e-04] [train aL=5.06e-01 err=0.21 nd=280/400] [test aL=7.02e-01 err=0.29]\n",
            "[i=3468 t=3.07e+02 wall=62] [dt=2.2e-02 dgrad=4.1e-05 dout=6.8e-03] [train aL=4.91e-01 err=0.20 nd=278/400] [test aL=6.87e-01 err=0.28]\n",
            "[3478 +41] [dt=2.7e-03 dgrad=1.6e-04 dout=8.5e-03]\n",
            "[i=3498 t=3.07e+02 wall=62] [dt=1.8e-02 dgrad=4.3e-05 dout=6.0e-03] [train aL=4.70e-01 err=0.20 nd=269/400] [test aL=6.66e-01 err=0.26]\n",
            "[3507 +29] [dt=2.7e-03 dgrad=1.3e-04 dout=9.0e-03]\n",
            "[i=3528 t=3.07e+02 wall=63] [dt=2.0e-02 dgrad=6.0e-05 dout=6.9e-03] [train aL=4.50e-01 err=0.19 nd=257/400] [test aL=6.46e-01 err=0.25]\n",
            "[3551 +44] [dt=3.5e-03 dgrad=1.5e-04 dout=1.3e-02]\n",
            "[i=3558 t=3.08e+02 wall=63] [dt=6.8e-03 dgrad=9.6e-06 dout=2.5e-03] [train aL=4.15e-01 err=0.17 nd=238/400] [test aL=6.09e-01 err=0.23]\n",
            "[3580 +29] [dt=3.4e-03 dgrad=1.2e-04 dout=1.3e-02]\n",
            "[i=3588 t=3.08e+02 wall=63] [dt=7.4e-03 dgrad=1.1e-05 dout=2.8e-03] [train aL=3.89e-01 err=0.16 nd=232/400] [test aL=5.82e-01 err=0.22]\n",
            "[3597 +17] [dt=1.7e-03 dgrad=1.0e-04 dout=6.7e-03]\n",
            "[i=3619 t=3.09e+02 wall=64] [dt=1.4e-02 dgrad=1.3e-05 dout=5.5e-03] [train aL=3.77e-01 err=0.15 nd=229/400] [test aL=5.69e-01 err=0.21]\n",
            "[3645 +48] [dt=2.8e-03 dgrad=1.4e-04 dout=1.2e-02]\n",
            "[i=3650 t=3.09e+02 wall=64] [dt=4.4e-03 dgrad=4.1e-06 dout=1.9e-03] [train aL=3.43e-01 err=0.14 nd=219/400] [test aL=5.31e-01 err=0.18]\n",
            "[3666 +21] [dt=2.0e-03 dgrad=2.9e-04 dout=8.6e-03]\n",
            "[i=3681 t=3.10e+02 wall=65] [dt=8.5e-03 dgrad=2.9e-05 dout=3.7e-03] [train aL=3.31e-01 err=0.14 nd=215/400] [test aL=5.17e-01 err=0.17]\n",
            "[3696 +30] [dt=1.8e-03 dgrad=1.1e-04 dout=7.6e-03]\n",
            "[i=3712 t=3.10e+02 wall=65] [dt=8.4e-03 dgrad=1.8e-05 dout=3.5e-03] [train aL=3.16e-01 err=0.14 nd=205/400] [test aL=5.01e-01 err=0.16]\n",
            "[3726 +30] [dt=2.0e-03 dgrad=1.0e-04 dout=8.1e-03]\n",
            "[i=3744 t=3.10e+02 wall=66] [dt=1.1e-02 dgrad=1.7e-05 dout=4.5e-03] [train aL=3.01e-01 err=0.13 nd=196/400] [test aL=4.83e-01 err=0.15]\n",
            "[3755 +29] [dt=1.8e-03 dgrad=1.1e-04 dout=6.8e-03]\n",
            "[i=3776 t=3.10e+02 wall=66] [dt=1.2e-02 dgrad=3.9e-05 dout=4.4e-03] [train aL=2.88e-01 err=0.13 nd=191/400] [test aL=4.66e-01 err=0.14]\n",
            "[3797 +42] [dt=1.7e-03 dgrad=1.1e-04 dout=5.8e-03]\n",
            "[i=3808 t=3.11e+02 wall=67] [dt=5.0e-03 dgrad=8.6e-06 dout=1.7e-03] [train aL=2.73e-01 err=0.12 nd=177/400] [test aL=4.46e-01 err=0.13]\n",
            "[3822 +25] [dt=1.7e-03 dgrad=1.4e-04 dout=5.4e-03]\n",
            "[i=3840 t=3.11e+02 wall=67] [dt=9.6e-03 dgrad=1.5e-05 dout=2.9e-03] [train aL=2.64e-01 err=0.12 nd=173/400] [test aL=4.34e-01 err=0.13]\n",
            "[3866 +44] [dt=2.1e-03 dgrad=1.4e-04 dout=5.2e-03]\n",
            "[i=3872 t=3.12e+02 wall=68] [dt=3.6e-03 dgrad=4.5e-06 dout=9.3e-04] [train aL=2.46e-01 err=0.10 nd=159/400] [test aL=4.09e-01 err=0.12]\n",
            "[3890 +24] [dt=1.5e-03 dgrad=1.4e-04 dout=3.6e-03]\n",
            "[i=3905 t=3.12e+02 wall=68] [dt=6.3e-03 dgrad=1.1e-05 dout=1.5e-03] [train aL=2.39e-01 err=0.09 nd=155/400] [test aL=3.99e-01 err=0.12]\n",
            "[i=3938 t=3.12e+02 wall=68] [dt=2.4e-02 dgrad=8.4e-05 dout=5.7e-03] [train aL=2.23e-01 err=0.09 nd=143/400] [test aL=3.74e-01 err=0.11]\n",
            "[3941 +51] [dt=2.4e-03 dgrad=1.3e-04 dout=5.7e-03]\n",
            "[3970 +29] [dt=2.4e-03 dgrad=1.1e-04 dout=5.6e-03]\n",
            "[i=3971 t=3.13e+02 wall=69] [dt=2.6e-03 dgrad=1.2e-06 dout=6.1e-04] [train aL=2.12e-01 err=0.08 nd=135/400] [test aL=3.58e-01 err=0.10]\n",
            "[3993 +23] [dt=1.9e-03 dgrad=1.2e-04 dout=4.4e-03]\n",
            "[i=4004 t=3.13e+02 wall=69] [dt=5.5e-03 dgrad=9.7e-06 dout=1.3e-03] [train aL=2.06e-01 err=0.08 nd=133/400] [test aL=3.48e-01 err=0.10]\n",
            "[4023 +30] [dt=1.9e-03 dgrad=2.1e-04 dout=4.4e-03]\n",
            "[i=4037 t=3.13e+02 wall=70] [dt=7.2e-03 dgrad=3.1e-05 dout=1.7e-03] [train aL=1.98e-01 err=0.07 nd=130/400] [test aL=3.36e-01 err=0.10]\n",
            "[4068 +45] [dt=2.3e-03 dgrad=1.3e-04 dout=4.7e-03]\n",
            "[i=4071 t=3.14e+02 wall=70] [dt=3.0e-03 dgrad=1.8e-06 dout=6.2e-04] [train aL=1.86e-01 err=0.07 nd=121/400] [test aL=3.17e-01 err=0.09]\n",
            "[i=4105 t=3.14e+02 wall=71] [dt=2.2e-02 dgrad=6.6e-05 dout=5.0e-03] [train aL=1.74e-01 err=0.06 nd=114/400] [test aL=2.98e-01 err=0.09]\n",
            "[4110 +42] [dt=2.2e-03 dgrad=4.8e-04 dout=5.1e-03]\n",
            "[4129 +19] [dt=9.3e-04 dgrad=1.0e-04 dout=2.0e-03]\n",
            "[i=4139 t=3.14e+02 wall=72] [dt=2.4e-03 dgrad=8.5e-06 dout=5.2e-04] [train aL=1.69e-01 err=0.05 nd=108/400] [test aL=2.91e-01 err=0.09]\n",
            "[4170 +41] [dt=1.6e-03 dgrad=1.2e-04 dout=3.6e-03]\n",
            "[i=4173 t=3.15e+02 wall=72] [dt=2.2e-03 dgrad=1.9e-06 dout=4.8e-04] [train aL=1.63e-01 err=0.05 nd=108/400] [test aL=2.81e-01 err=0.09]\n",
            "[4207 +37] [dt=1.8e-03 dgrad=2.2e-04 dout=3.9e-03]\n",
            "[i=4208 t=3.15e+02 wall=73] [dt=1.9e-03 dgrad=1.9e-06 dout=4.2e-04] [train aL=1.56e-01 err=0.05 nd=104/400] [test aL=2.69e-01 err=0.08]\n",
            "[i=4243 t=3.15e+02 wall=74] [dt=2.3e-02 dgrad=7.9e-05 dout=5.6e-03] [train aL=1.48e-01 err=0.04 nd=98/400] [test aL=2.57e-01 err=0.08]\n",
            "[4249 +42] [dt=2.3e-03 dgrad=1.1e-04 dout=5.7e-03]\n",
            "[4265 +16] [dt=1.1e-03 dgrad=1.2e-04 dout=2.6e-03]\n",
            "[i=4278 t=3.16e+02 wall=74] [dt=3.7e-03 dgrad=2.4e-06 dout=8.9e-04] [train aL=1.44e-01 err=0.04 nd=95/400] [test aL=2.50e-01 err=0.07]\n",
            "[4292 +27] [dt=1.3e-03 dgrad=1.2e-04 dout=2.9e-03]\n",
            "[i=4313 t=3.16e+02 wall=75] [dt=9.4e-03 dgrad=2.0e-05 dout=2.0e-03] [train aL=1.41e-01 err=0.03 nd=91/400] [test aL=2.44e-01 err=0.07]\n",
            "[4325 +33] [dt=1.7e-03 dgrad=1.7e-04 dout=3.5e-03]\n",
            "[4346 +21] [dt=9.2e-04 dgrad=1.2e-04 dout=1.8e-03]\n",
            "[i=4349 t=3.16e+02 wall=75] [dt=1.2e-03 dgrad=2.2e-06 dout=2.4e-04] [train aL=1.36e-01 err=0.03 nd=88/400] [test aL=2.37e-01 err=0.07]\n",
            "[i=4385 t=3.16e+02 wall=76] [dt=1.6e-02 dgrad=5.3e-05 dout=3.1e-03] [train aL=1.32e-01 err=0.02 nd=85/400] [test aL=2.30e-01 err=0.06]\n",
            "[4391 +45] [dt=1.8e-03 dgrad=1.0e-04 dout=3.4e-03]\n",
            "[4421 +30] [dt=1.4e-03 dgrad=1.1e-04 dout=2.7e-03]\n",
            "[i=4421 t=3.17e+02 wall=76] [dt=1.4e-03 dgrad=1.1e-06 dout=2.7e-04] [train aL=1.27e-01 err=0.02 nd=80/400] [test aL=2.22e-01 err=0.06]\n",
            "[4455 +34] [dt=2.1e-03 dgrad=1.9e-04 dout=3.7e-03]\n",
            "[i=4457 t=3.17e+02 wall=77] [dt=2.5e-03 dgrad=2.6e-06 dout=4.6e-04] [train aL=1.22e-01 err=0.01 nd=79/400] [test aL=2.14e-01 err=0.06]\n",
            "[i=4493 t=3.17e+02 wall=77] [dt=2.5e-02 dgrad=5.6e-05 dout=4.3e-03] [train aL=1.14e-01 err=0.00 nd=71/400] [test aL=2.03e-01 err=0.04]\n",
            "[4496 +41] [dt=2.5e-03 dgrad=2.4e-04 dout=4.3e-03]\n",
            "[i=4530 t=3.18e+02 wall=78] [dt=2.4e-02 dgrad=4.7e-05 dout=4.3e-03] [train aL=1.06e-01 err=0.00 nd=67/400] [test aL=1.93e-01 err=0.03]\n",
            "[i=4567 t=3.19e+02 wall=78] [dt=3.6e-02 dgrad=5.3e-05 dout=7.7e-03] [train aL=8.26e-02 err=0.00 nd=66/400] [test aL=1.69e-01 err=0.02]\n",
            "[4601 +105] [dt=3.9e-03 dgrad=1.3e-04 dout=1.3e-02]\n",
            "[i=4604 t=3.20e+02 wall=79] [dt=5.2e-03 dgrad=2.3e-06 dout=1.7e-03] [train aL=4.37e-02 err=0.00 nd=69/400] [test aL=1.39e-01 err=0.00]\n",
            "[4626 +25] [dt=2.9e-03 dgrad=1.0e-04 dout=1.0e-02]\n",
            "[i=4641 t=3.21e+02 wall=79] [dt=1.2e-02 dgrad=3.6e-05 dout=4.3e-03] [train aL=2.65e-02 err=0.00 nd=71/400] [test aL=1.27e-01 err=0.00]\n",
            "[4646 +20] [dt=1.5e-03 dgrad=1.0e-04 dout=5.1e-03]\n",
            "[4679 +33] [dt=8.2e-04 dgrad=1.0e-04 dout=2.5e-03]\n",
            "[i=4679 t=3.21e+02 wall=80] [dt=8.2e-04 dgrad=1.0e-06 dout=2.5e-04] [train aL=1.81e-02 err=0.00 nd=70/400] [test aL=1.21e-01 err=0.00]\n",
            "[i=4717 t=3.21e+02 wall=80] [dt=5.5e-03 dgrad=5.7e-05 dout=1.4e-03] [train aL=1.44e-02 err=0.00 nd=57/400] [test aL=1.17e-01 err=0.00]\n",
            "[i=4755 t=3.21e+02 wall=81] [dt=6.7e-03 dgrad=5.3e-05 dout=1.1e-03] [train aL=1.07e-02 err=0.00 nd=32/400] [test aL=1.12e-01 err=0.00]\n",
            "[i=4793 t=3.22e+02 wall=82] [dt=9.7e-03 dgrad=5.8e-05 dout=1.3e-03] [train aL=7.64e-03 err=0.00 nd=22/400] [test aL=1.06e-01 err=0.00]\n",
            "[i=4832 t=3.22e+02 wall=82] [dt=1.2e-02 dgrad=5.2e-05 dout=1.1e-03] [train aL=5.31e-03 err=0.00 nd=9/400] [test aL=9.90e-02 err=0.00]\n",
            "[i=4871 t=3.23e+02 wall=83] [dt=1.4e-02 dgrad=5.0e-05 dout=1.1e-03] [train aL=3.70e-03 err=0.00 nd=7/400] [test aL=9.29e-02 err=0.00]\n",
            "[i=4910 t=3.23e+02 wall=83] [dt=1.9e-02 dgrad=5.6e-05 dout=1.2e-03] [train aL=2.58e-03 err=0.00 nd=4/400] [test aL=8.75e-02 err=0.00]\n",
            "[i=4949 t=3.24e+02 wall=84] [dt=2.3e-02 dgrad=5.2e-05 dout=1.3e-03] [train aL=1.81e-03 err=0.00 nd=2/400] [test aL=8.25e-02 err=0.00]\n",
            "[i=4989 t=3.25e+02 wall=84] [dt=2.8e-02 dgrad=5.0e-05 dout=1.3e-03] [train aL=1.27e-03 err=0.00 nd=1/400] [test aL=7.79e-02 err=0.00]\n",
            "[i=5029 t=3.27e+02 wall=85] [dt=3.4e-02 dgrad=5.2e-05 dout=1.1e-03] [train aL=9.20e-04 err=0.00 nd=0/400] [test aL=7.38e-02 err=0.00]\n",
            "[i=5069 t=3.28e+02 wall=86] [dt=4.5e-02 dgrad=5.8e-05 dout=8.8e-04] [train aL=6.82e-04 err=0.00 nd=0/400] [test aL=7.02e-02 err=0.00]\n",
            "[i=5071 t=3.28e+02 wall=86] [dt=4.5e-02 dgrad=5.7e-05 dout=8.5e-04] [train aL=6.72e-04 err=0.00 nd=0/400] [test aL=7.00e-02 err=0.00]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# ANSI escape sequences for colors\n",
        "COLORS = {\n",
        "    'HEADER': '\\033[95m',  # Magenta\n",
        "    'OKCYAN': '\\033[96m',  # Cyan\n",
        "    'OKGREEN': '\\033[92m',  # Green\n",
        "    'WARNING': '\\033[93m',  # Yellow\n",
        "    'FAIL': '\\033[91m',    # Red\n",
        "    'ENDC': '\\033[0m',     # Reset to default\n",
        "    'BOLD': '\\033[1m',     # Bold\n",
        "    'UNDERLINE': '\\033[4m' # Underline\n",
        "}\n",
        "\n",
        "def color_text(text, color_name):\n",
        "    return f\"{COLORS[color_name]}{text}{COLORS['ENDC']}\"\n",
        "\n",
        "def load_and_print_summary(file_path, verbose=False):\n",
        "    \"\"\"\n",
        "    保存された pickle ファイルを読み込み，その中身の概要を出力する関数\n",
        "\n",
        "    Parameters:\n",
        "    - file_path: 読み込む pickle ファイルのパス\n",
        "    - verbose: データの詳細表示を行うかどうかのフラグ\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            # 初めに HyperParams オブジェクトを読み込む\n",
        "            hyper = pickle.load(f)\n",
        "\n",
        "            # ハイパーパラメータの概要表示\n",
        "            print(color_text(f\"Loaded HyperParams object:\", 'HEADER'))\n",
        "            print(f\"  Type: {type(hyper).__name__}\")\n",
        "            print(f\"  Attributes:\")\n",
        "            for attribute in dir(hyper):\n",
        "                if not attribute.startswith('__') and not callable(getattr(hyper, attribute)):\n",
        "                    value = getattr(hyper, attribute)\n",
        "                    print(f\"    {attribute}: {value} ({type(value).__name__})\")\n",
        "\n",
        "            # 次に，実験結果が続く場合\n",
        "            results = []\n",
        "            while True:\n",
        "                try:\n",
        "                    result = pickle.load(f)\n",
        "                    results.append(result)\n",
        "                except EOFError:\n",
        "                    break\n",
        "\n",
        "            # 実験結果の概要を表示\n",
        "            print(color_text(\"\\nExperiment results summary:\", 'OKCYAN'))\n",
        "            print(f\"  Number of results: {len(results)}\")\n",
        "            if len(results) > 0:\n",
        "                if isinstance(results[0], dict):\n",
        "                    print(\"  Sample keys from result dictionaries:\")\n",
        "                    sample_result = results[0]\n",
        "                    sample_keys = {}\n",
        "                    if isinstance(sample_result, dict):\n",
        "                        for key, value in sample_result.items():\n",
        "                            if isinstance(value, (list, dict)):\n",
        "                                entry_count = len(value)\n",
        "                                sample_keys[key] = (f\"{entry_count} entries\", type(value).__name__)\n",
        "                            else:\n",
        "                                sample_keys[key] = (f\"{value}\", type(value).__name__)\n",
        "                    print(f\"    Number of keys: {len(sample_keys)}\")\n",
        "                    for key, (entry_or_value, dtype) in sample_keys.items():\n",
        "                        print(f\"    {key}: {entry_or_value}, Type: {dtype}\")\n",
        "                else:\n",
        "                    print(f\"  Type of results: {type(results[0]).__name__}\")\n",
        "\n",
        "            # `regular` に含まれるキーとその数を表示\n",
        "            if len(results) > 0 and isinstance(results[0], dict):\n",
        "                regular_keys = set()\n",
        "                key_data_counts = {}\n",
        "\n",
        "                for result in results:\n",
        "                    if 'regular' in result and isinstance(result['regular'], dict):\n",
        "                        for key, value in result['regular'].items():\n",
        "                            if key not in key_data_counts:\n",
        "                                key_data_counts[key] = (0, None)\n",
        "                            count, dtype = key_data_counts[key]\n",
        "                            key_data_counts[key] = (count + 1, dtype if dtype else type(value).__name__)\n",
        "                            regular_keys.add(key)\n",
        "\n",
        "                print(color_text(f\"\\nKeys in 'regular' and their count:\", 'OKGREEN'))\n",
        "                print(f\"  Number of keys: {len(key_data_counts)}\")\n",
        "                for key, (count, dtype) in key_data_counts.items():\n",
        "                    print(f\"    {key}: {count} entries, Type: {dtype}\")\n",
        "\n",
        "                # 各キー (`dynamics`, `train`, `test`) の詳細を表示\n",
        "                for key in ['dynamics', 'train', 'test']:\n",
        "                    if key in regular_keys:\n",
        "                        if key == 'dynamics':\n",
        "                            for i, dynamics_list in enumerate([result['regular'][key] for result in results if 'regular' in result and key in result['regular']], 1):\n",
        "                                print(color_text(f\"\\nDetails of 'dynamics' list {i}:\", 'HEADER'))\n",
        "                                if len(dynamics_list) > 0 and isinstance(dynamics_list[0], dict):\n",
        "                                    entry_description = {subkey: type(value).__name__ for subkey, value in dynamics_list[0].items()}\n",
        "                                    print(f\"  Example entry data types in 'dynamics' list {i}: {entry_description}\")\n",
        "                                print(f\"  Number of entries in 'dynamics' list {i}: {len(dynamics_list)}\")\n",
        "                        else:\n",
        "                            key_data_counts = {}\n",
        "                            for result in results:\n",
        "                                if 'regular' in result and key in result['regular']:\n",
        "                                    if isinstance(result['regular'][key], dict):\n",
        "                                        for subkey, value in result['regular'][key].items():\n",
        "                                            if subkey not in key_data_counts:\n",
        "                                                key_data_counts[subkey] = (0, None)\n",
        "                                            count, dtype = key_data_counts[subkey]\n",
        "                                            key_data_counts[subkey] = (count + 1, dtype if dtype else type(value).__name__)\n",
        "\n",
        "                            print(color_text(f\"\\nKeys in 'regular[{key}]' and their count:\", 'OKGREEN'))\n",
        "                            print(f\"  Number of keys: {len(key_data_counts)}\")\n",
        "                            for subkey, (count, dtype) in key_data_counts.items():\n",
        "                                print(f\"    {subkey}: {count} entries, Type: {dtype}\")\n",
        "\n",
        "                # `dynamics` 内の `train` と `test` キーのサブキーを表示\n",
        "                if 'dynamics' in regular_keys:\n",
        "                    dynamics_train_keys = {}\n",
        "                    dynamics_test_keys = {}\n",
        "                    for result in results:\n",
        "                        if 'regular' in result and 'dynamics' in result['regular']:\n",
        "                            for entry in result['regular']['dynamics']:\n",
        "                                if isinstance(entry, dict):\n",
        "                                    if 'train' in entry and isinstance(entry['train'], dict):\n",
        "                                        for subkey, value in entry['train'].items():\n",
        "                                            if subkey not in dynamics_train_keys:\n",
        "                                                dynamics_train_keys[subkey] = (0, None)\n",
        "                                            count, dtype = dynamics_train_keys[subkey]\n",
        "                                            dynamics_train_keys[subkey] = (count + 1, dtype if dtype else type(value).__name__)\n",
        "                                    if 'test' in entry and isinstance(entry['test'], dict):\n",
        "                                        for subkey, value in entry['test'].items():\n",
        "                                            if subkey not in dynamics_test_keys:\n",
        "                                                dynamics_test_keys[subkey] = (0, None)\n",
        "                                            count, dtype = dynamics_test_keys[subkey]\n",
        "                                            dynamics_test_keys[subkey] = (count + 1, dtype if dtype else type(value).__name__)\n",
        "\n",
        "                    print(color_text(f\"\\nKeys in 'dynamics[train]' and their count:\", 'OKGREEN'))\n",
        "                    print(f\"  Number of keys: {len(dynamics_train_keys)}\")\n",
        "                    for subkey, (count, dtype) in dynamics_train_keys.items():\n",
        "                        print(f\"    {subkey}: {count} entries, Type: {dtype}\")\n",
        "\n",
        "                    print(color_text(f\"\\nKeys in 'dynamics[test]' and their count:\", 'OKGREEN'))\n",
        "                    print(f\"  Number of keys: {len(dynamics_test_keys)}\")\n",
        "                    for subkey, (count, dtype) in dynamics_test_keys.items():\n",
        "                        print(f\"    {subkey}: {count} entries, Type: {dtype}\")\n",
        "\n",
        "            # 詳細なデータを表示（verbose が True の場合のみ）\n",
        "            if verbose:\n",
        "                print(color_text(\"\\nLoaded experiment results:\", 'HEADER'))\n",
        "                for i, result in enumerate(results):\n",
        "                    print(color_text(f\"\\nResult {i+1}:\", 'HEADER'))\n",
        "                    if isinstance(result, dict):\n",
        "                        print(f\"  Type: dict\")\n",
        "                        print(f\"  Keys: {list(result.keys())}\")\n",
        "                        print(f\"  All Values:\")\n",
        "                        for key in result.keys():\n",
        "                            print(f\"    {key}: {result[key]}\")\n",
        "                    else:\n",
        "                        print(f\"  Type: {type(result).__name__}\")\n",
        "                        print(f\"  Content: {result}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the file: {e}\")\n",
        "\n",
        "# 使用例\n",
        "file_path = 'F10k3Lsp_h_init/F10k3Lsp_h_init.pickle'\n",
        "load_and_print_summary(file_path, verbose=False)  # 詳細表示を無効にする場合"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqrROxCU3v1m",
        "outputId": "2bff4a6a-edbe-409e-8a37-233ef1a4389c"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[95mLoaded HyperParams object:\u001b[0m\n",
            "  Type: HyperParams\n",
            "  Attributes:\n",
            "    L: 2 (int)\n",
            "    alpha: 0.0001 (float)\n",
            "    arch: fc_softplus (str)\n",
            "    batch_seed: 0 (int)\n",
            "    bias: True (bool)\n",
            "    chunk: 100 (int)\n",
            "    data_seed: 0 (int)\n",
            "    delta_kernel: 0 (int)\n",
            "    device: cuda (str)\n",
            "    directory: F10k3Lsp_h_init (str)\n",
            "    dtype: float64 (str)\n",
            "    f0: 1 (int)\n",
            "    final_kernel: 0 (int)\n",
            "    h: 100 (int)\n",
            "    init_kernel: 0 (int)\n",
            "    init_seed: 0 (int)\n",
            "    k: 3 (int)\n",
            "    loss: softhinge (str)\n",
            "    lossbeta: 20 (int)\n",
            "    max_dgrad: 0.0001 (float)\n",
            "    max_dout: 0.1 (float)\n",
            "    n: 30 (int)\n",
            "    normalize: True (bool)\n",
            "    pickle: F10k3Lsp_h_init.pickle (str)\n",
            "    regular: 1 (int)\n",
            "    save_outputs: 0 (int)\n",
            "    spbeta: 5 (int)\n",
            "    store_kernel: 0 (int)\n",
            "    tau_alpha_crit: 1000.0 (float)\n",
            "    tau_over_h: 0.001 (float)\n",
            "    test_size: 400 (int)\n",
            "    train_size: 700 (int)\n",
            "    train_time: 18000 (int)\n",
            "\u001b[96m\n",
            "Experiment results summary:\u001b[0m\n",
            "  Number of results: 2\n",
            "  Sample keys from result dictionaries:\n",
            "    Number of keys: 3\n",
            "    hyper: <__main__.HyperParams object at 0x7cffae7c47f0>, Type: HyperParams\n",
            "    N: 13301, Type: int\n",
            "    regular: 3 entries, Type: dict\n",
            "\u001b[92m\n",
            "Keys in 'regular' and their count:\u001b[0m\n",
            "  Number of keys: 3\n",
            "    dynamics: 2 entries, Type: list\n",
            "    train: 2 entries, Type: dict\n",
            "    test: 2 entries, Type: dict\n",
            "\u001b[95m\n",
            "Details of 'dynamics' list 1:\u001b[0m\n",
            "  Example entry data types in 'dynamics' list 1: {'step': 'int', 'wall': 'float', 't': 'float', 'dt': 'float', 'dgrad': 'float', 'dout': 'float', 'norm': 'float', 'dnorm': 'float', 'grad_norm': 'float', 'wnorm': 'list', 'dwnorm': 'list', 'train': 'dict', 'test': 'dict'}\n",
            "  Number of entries in 'dynamics' list 1: 479\n",
            "\u001b[95m\n",
            "Details of 'dynamics' list 2:\u001b[0m\n",
            "  Example entry data types in 'dynamics' list 2: {'step': 'int', 'wall': 'float', 't': 'float', 'dt': 'float', 'dgrad': 'float', 'dout': 'float', 'norm': 'float', 'dnorm': 'float', 'grad_norm': 'float', 'wnorm': 'list', 'dwnorm': 'list', 'train': 'dict', 'test': 'dict'}\n",
            "  Number of entries in 'dynamics' list 2: 479\n",
            "\u001b[92m\n",
            "Keys in 'regular[train]' and their count:\u001b[0m\n",
            "  Number of keys: 3\n",
            "    f0: 2 entries, Type: Tensor\n",
            "    outputs: 2 entries, Type: Tensor\n",
            "    labels: 2 entries, Type: Tensor\n",
            "\u001b[92m\n",
            "Keys in 'regular[test]' and their count:\u001b[0m\n",
            "  Number of keys: 3\n",
            "    f0: 2 entries, Type: Tensor\n",
            "    outputs: 2 entries, Type: Tensor\n",
            "    labels: 2 entries, Type: Tensor\n",
            "\u001b[92m\n",
            "Keys in 'dynamics[train]' and their count:\u001b[0m\n",
            "  Number of keys: 8\n",
            "    loss: 958 entries, Type: float\n",
            "    aloss: 958 entries, Type: float\n",
            "    err: 958 entries, Type: float\n",
            "    nd: 958 entries, Type: int\n",
            "    dfnorm: 958 entries, Type: Tensor\n",
            "    fnorm: 958 entries, Type: Tensor\n",
            "    outputs: 958 entries, Type: NoneType\n",
            "    labels: 958 entries, Type: NoneType\n",
            "\u001b[92m\n",
            "Keys in 'dynamics[test]' and their count:\u001b[0m\n",
            "  Number of keys: 8\n",
            "    loss: 958 entries, Type: float\n",
            "    aloss: 958 entries, Type: float\n",
            "    err: 958 entries, Type: float\n",
            "    nd: 958 entries, Type: int\n",
            "    dfnorm: 958 entries, Type: Tensor\n",
            "    fnorm: 958 entries, Type: Tensor\n",
            "    outputs: 958 entries, Type: NoneType\n",
            "    labels: 958 entries, Type: NoneType\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def load_dynamics_data(file_path):\n",
        "    \"\"\"\n",
        "    Load dynamics data from a pickle file.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path: The path to the pickle file containing the data.\n",
        "\n",
        "    Returns:\n",
        "    - A list of dynamics data entries.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'rb') as f:\n",
        "        # Skip the HyperParams object\n",
        "        pickle.load(f)\n",
        "\n",
        "        # Read the remaining results\n",
        "        results = []\n",
        "        while True:\n",
        "            try:\n",
        "                result = pickle.load(f)\n",
        "                results.append(result)\n",
        "            except EOFError:\n",
        "                break\n",
        "\n",
        "    # Extract 'dynamics' from the results\n",
        "    for result in results:\n",
        "        if 'regular' in result and 'dynamics' in result['regular']:\n",
        "            return result['regular']['dynamics']\n",
        "\n",
        "    raise ValueError(\"No dynamics data found in the provided file.\")\n",
        "\n",
        "def plot_losses_and_errors(dynamics):\n",
        "    \"\"\"\n",
        "    Plot training and test losses and errors from dynamics data.\n",
        "\n",
        "    Parameters:\n",
        "    - dynamics: A list of dynamics data entries.\n",
        "    \"\"\"\n",
        "    if not dynamics:\n",
        "        print(\"No dynamics data available.\")\n",
        "        return\n",
        "\n",
        "    # Print the first and last entries\n",
        "    print(\"First entry:\", dynamics[0])\n",
        "    print(\"Last entry:\", dynamics[-1])\n",
        "\n",
        "    steps = []\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    train_errors = []\n",
        "    test_errors = []\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for entry in dynamics:\n",
        "        steps.append(entry['step'])\n",
        "\n",
        "        # Extract loss and error values\n",
        "        if 'train' in entry:\n",
        "            train_losses.append(entry['train'].get('loss', None))\n",
        "            train_errors.append(entry['train'].get('err', None))\n",
        "            # Calculate accuracy from error\n",
        "            train_accuracies.append(1 - entry['train'].get('err', 0))\n",
        "\n",
        "        if 'test' in entry:\n",
        "            test_losses.append(entry['test'].get('loss', None))\n",
        "            test_errors.append(entry['test'].get('err', None))\n",
        "            # Calculate accuracy from error\n",
        "            test_accuracies.append(1 - entry['test'].get('err', 0))\n",
        "\n",
        "    # Plot training and test losses\n",
        "    plt.figure(figsize=(8, 3))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(steps, train_losses, label='Train Loss', color='blue')\n",
        "    plt.plot(steps, test_losses, label='Test Loss', color='red')\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('Steps')\n",
        "    # plt.xlim(10 ** 2, np.max(steps))\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Test Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    print(f\"step len\", len(steps))\n",
        "\n",
        "    # Plot training and test accuracies\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(steps, train_accuracies, label='Train Accuracy', color='blue')\n",
        "    plt.plot(steps, test_accuracies, label='Test Accuracy', color='red')\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('Steps')\n",
        "    # plt.xlim(10 ** 2, np.max(steps))\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Test Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "file_path = 'F10k3Lsp_h_init/F10k3Lsp_h_init.pickle'\n",
        "dynamics_data = load_dynamics_data(file_path)\n",
        "plot_losses_and_errors(dynamics_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "pdNLh3umSL3u",
        "outputId": "9f0b4655-71ef-40b7-fbcc-316f9bd41e10"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First entry: {'step': 0, 'wall': 0.04582039999877452, 't': 0.1, 'dt': 0.1, 'dgrad': 4.5984031528972735e-07, 'dout': 1.9359676033938735e-07, 'norm': 115.27988388886278, 'dnorm': 0.003391136560863102, 'grad_norm': 0.05365238713061271, 'wnorm': [54.611653033120426, 101.09842643636655, 9.28047149571096], 'dwnorm': [0.001719496677742854, 0.0017362157918947938, 0.001483597927293187], 'train': {'loss': 9999.999831274337, 'aloss': 0.9999999831274338, 'err': 0.4275, 'nd': 400, 'dfnorm': tensor(0.0008), 'fnorm': tensor(0.6722), 'outputs': None, 'labels': None}, 'test': {'loss': 9999.999914374877, 'aloss': 0.9999999914374877, 'err': 0.44, 'nd': 400, 'dfnorm': tensor(0.0009), 'fnorm': tensor(0.6370), 'outputs': None, 'labels': None}}\n",
            "Last entry: {'step': 5071, 'wall': 86.01845839100133, 't': 328.13292968360673, 'dt': 0.04478806832704514, 'dgrad': 5.6975277481757904e-05, 'dout': 0.0008535093512889944, 'norm': 623.5565604715115, 'dnorm': 597.5151334708667, 'grad_norm': 1.1369009728517232, 'wnorm': [342.5815000862641, 366.5303127533323, 352.6765755750726], 'dwnorm': [327.2729449324227, 342.4755139744882, 346.2539729600019], 'train': {'loss': 6.721979269091887, 'aloss': 0.0006721979269091888, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(14305.7559), 'fnorm': tensor(14305.8171), 'outputs': None, 'labels': None}, 'test': {'loss': 700.378477644996, 'aloss': 0.0700378477644996, 'err': 0.0, 'nd': 118, 'dfnorm': tensor(12508.6158), 'fnorm': tensor(12508.6758), 'outputs': None, 'labels': None}}\n",
            "step len 479\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def load_dynamics_data(file_path):\n",
        "    \"\"\"\n",
        "    Load dynamics data from a pickle file.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path: The path to the pickle file containing the data.\n",
        "\n",
        "    Returns:\n",
        "    - A list of dynamics data entries.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'rb') as f:\n",
        "        # Skip the HyperParams object\n",
        "        pickle.load(f)\n",
        "\n",
        "        # Read the remaining results\n",
        "        results = []\n",
        "        while True:\n",
        "            try:\n",
        "                result = pickle.load(f)\n",
        "                results.append(result)\n",
        "            except EOFError:\n",
        "                break\n",
        "\n",
        "    # Extract 'dynamics' from the results\n",
        "    for result in results:\n",
        "        if 'regular' in result and 'dynamics' in result['regular']:\n",
        "            return result['regular']['dynamics']\n",
        "\n",
        "    raise ValueError(\"No dynamics data found in the provided file.\")\n",
        "\n",
        "def plot_losses_and_errors(dynamics):\n",
        "    \"\"\"\n",
        "    Plot training and test losses and errors from dynamics data.\n",
        "\n",
        "    Parameters:\n",
        "    - dynamics: A list of dynamics data entries.\n",
        "    \"\"\"\n",
        "    times = []\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    train_errors = []\n",
        "    test_errors = []\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for entry in dynamics:\n",
        "        times.append(entry['t'])\n",
        "\n",
        "        # Extract loss and error values\n",
        "        if 'train' in entry:\n",
        "            train_losses.append(entry['train'].get('loss', None))\n",
        "            train_errors.append(entry['train'].get('err', None))\n",
        "            # Calculate accuracy from error\n",
        "            train_accuracies.append(1 - entry['train'].get('err', 0))\n",
        "\n",
        "        if 'test' in entry:\n",
        "            test_losses.append(entry['test'].get('loss', None))\n",
        "            test_errors.append(entry['test'].get('err', None))\n",
        "            # Calculate accuracy from error\n",
        "            test_accuracies.append(1 - entry['test'].get('err', 0))\n",
        "\n",
        "    # Plot training and test losses\n",
        "    plt.figure(figsize=(8, 3))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.semilogx(times, train_losses, label='Train Loss', color='blue')\n",
        "    plt.semilogx(times, test_losses, label='Test Loss', color='red')\n",
        "    plt.xlabel('Time (t)')\n",
        "    # plt.xlim(10 ** 2, np.max(times))\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Test Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    print(f\"t\", len(times))\n",
        "    print(f\"t max\", np.max(times))\n",
        "\n",
        "    # Plot training and test accuracies\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.semilogx(times, train_accuracies, label='Train Accuracy', color='blue')\n",
        "    plt.semilogx(times, test_accuracies, label='Test Accuracy', color='red')\n",
        "    plt.xlabel('Time (t)')\n",
        "    # plt.xlim(10 ** 2, np.max(times))\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Test Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "file_path = 'F10k3Lsp_h_init/F10k3Lsp_h_init.pickle'\n",
        "dynamics_data = load_dynamics_data(file_path)\n",
        "plot_losses_and_errors(dynamics_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "zUmtWbsSOxN6",
        "outputId": "0274a83a-d13f-4eb0-aa49-0dd22c1eec19"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t 479\n",
            "t max 328.13292968360673\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# メイン (Adam)"
      ],
      "metadata": {
        "id": "bNabgUuFqfUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import copy\n",
        "import itertools\n",
        "from time import perf_counter\n",
        "from functools import partial\n",
        "\n",
        "class SplitEval(torch.nn.Module):\n",
        "    def __init__(self, f, size):\n",
        "        super().__init__()\n",
        "        self.f = f\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([self.f(x[i: i + self.size]) for i in range(0, len(x), self.size)])\n",
        "\n",
        "def hinge(out, y, alpha):\n",
        "    return (1 - alpha * out * y).relu().mean() / alpha\n",
        "\n",
        "def quad_hinge(out, y, alpha):\n",
        "    return 0.5 * (1 - alpha * out * y).relu().pow(2).mean() / alpha ** 2\n",
        "\n",
        "def mse(out, y, alpha):\n",
        "    return 0.5 * (1.1 - alpha * out * y).pow(2).mean() / alpha ** 2\n",
        "\n",
        "def softhinge(out, y, alpha, beta):\n",
        "    sp = partial(torch.nn.functional.softplus, beta=beta)\n",
        "    return sp(1 - alpha * out * y).mean() / alpha\n",
        "\n",
        "def cross_entropy(out, y, alpha):\n",
        "    return torch.nn.functional.binary_cross_entropy_with_logits(out, (y + 1) / 2)\n",
        "\n",
        "def run_regular(hyper, f0, loss, xtr, ytr, xte, yte):\n",
        "    with torch.no_grad():\n",
        "        otr0 = f0(xtr)\n",
        "        ote0 = f0(xte)\n",
        "\n",
        "    f = copy.deepcopy(f0)\n",
        "    optimizer = torch.optim.Adam(f.parameters(), lr=hyper.learning_rate, weight_decay=hyper.weight_decay)\n",
        "\n",
        "    dynamics = []\n",
        "    checkpoint_generator = loglinspace(0.1, 1000)\n",
        "    checkpoint = next(checkpoint_generator)\n",
        "    wall = perf_counter()\n",
        "\n",
        "    for step in range(hyper.max_steps):\n",
        "        batch = torch.randperm(len(xtr))[:hyper.bs]\n",
        "        xb = xtr[batch]\n",
        "\n",
        "        loss_value = loss(f(xb) - otr0[batch], ytr[batch], hyper.alpha)\n",
        "\n",
        "        if hyper.regularization == 'l1':\n",
        "            l1_loss = sum(p.abs().sum() for p in f.parameters())\n",
        "            loss_value += hyper.weight_decay * l1_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        save = False\n",
        "\n",
        "        if step == checkpoint:\n",
        "            checkpoint = next(checkpoint_generator)\n",
        "            assert checkpoint > step\n",
        "            save = True\n",
        "\n",
        "        if save:\n",
        "            assert len(xtr) < len(xte)\n",
        "            j = torch.randperm(len(xtr))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                otr = f(xtr[j]) - otr0[j] if hyper.loss != 'cross_entropy' else f(xtr[j])\n",
        "                ote = f(xte[j]) - ote0[j] if hyper.loss != 'cross_entropy' else f(xte[j])\n",
        "\n",
        "            state = {\n",
        "                'step': step,\n",
        "                'wall': perf_counter() - wall,\n",
        "                'batch_loss': loss_value.item(),\n",
        "                'norm': sum(p.norm().pow(2) for p in f.parameters()).sqrt().item(),\n",
        "                'dnorm': sum((p0 - p).norm().pow(2) for p0, p in zip(f0.parameters(), f.parameters())).sqrt().item(),\n",
        "                'train': {\n",
        "                    'loss': loss(otr, ytr[j], hyper.alpha).item(),\n",
        "                    'aloss': hyper.alpha * loss(otr, ytr[j], hyper.alpha).item() if hyper.loss != 'cross_entropy' else loss(otr, ytr[j], hyper.alpha).item(),\n",
        "                    'aaloss': hyper.alpha ** 2 * loss(otr, ytr[j], hyper.alpha).item() if hyper.loss != 'cross_entropy' else loss(otr, ytr[j], hyper.alpha).item(),\n",
        "                    'err': (otr * ytr[j] <= 0).double().mean().item() if hyper.loss != 'cross_entropy' else ((otr > 0).float() != (ytr[j] + 1) / 2).float().mean().item(),\n",
        "                    'nd': (hyper.alpha * otr * ytr[j] < 1).long().sum().item() if hyper.loss != 'cross_entropy' else 0,\n",
        "                    'dfnorm': otr.pow(2).mean().sqrt(),\n",
        "                    'fnorm': (otr + otr0[j]).pow(2).mean().sqrt() if hyper.loss != 'cross_entropy' else otr.pow(2).mean().sqrt(),\n",
        "                },\n",
        "                'test': {\n",
        "                    'loss': loss(ote, yte[j], hyper.alpha).item(),\n",
        "                    'aloss': hyper.alpha * loss(ote, yte[j], hyper.alpha).item() if hyper.loss != 'cross_entropy' else loss(ote, yte[j], hyper.alpha).item(),\n",
        "                    'aaloss': hyper.alpha ** 2 * loss(ote, yte[j], hyper.alpha).item() if hyper.loss != 'cross_entropy' else loss(ote, yte[j], hyper.alpha).item(),\n",
        "                    'err': (ote * yte[j] <= 0).double().mean().item() if hyper.loss != 'cross_entropy' else ((ote > 0).float() != (yte[j] + 1) / 2).float().mean().item(),\n",
        "                    'nd': (hyper.alpha * ote * yte[j] < 1).long().sum().item() if hyper.loss != 'cross_entropy' else 0,\n",
        "                    'dfnorm': ote.pow(2).mean().sqrt(),\n",
        "                    'fnorm': (ote + ote0[j]).pow(2).mean().sqrt() if hyper.loss != 'cross_entropy' else ote.pow(2).mean().sqrt(),\n",
        "                },\n",
        "            }\n",
        "\n",
        "            if hyper.arch.split('_')[0] == 'fc':\n",
        "                def getw(f, i):\n",
        "                    return torch.cat(list(getattr(f.f, \"W{}\".format(i))))\n",
        "                state['wnorm'] = [getw(f, i).norm().item() for i in range(f.f.L + 1)]\n",
        "                state['dwnorm'] = [(getw(f, i) - getw(f0, i)).norm().item() for i in range(f.f.L + 1)]\n",
        "\n",
        "            print(\"[i={d[step]:d} wall={d[wall]:.0f}] [train aL={d[train][aloss]:.2e} err={d[train][err]:.2f} nd={d[train][nd]}/{p}] [test aL={d[test][aloss]:.2e} err={d[test][err]:.2f}]\".format(d=state, p=len(j)), flush=True)\n",
        "\n",
        "            dynamics.append(state)\n",
        "\n",
        "            if hyper.loss != 'cross_entropy' and state['test']['nd'] == 0:\n",
        "                break\n",
        "\n",
        "        if perf_counter() > wall + hyper.train_time:\n",
        "            break\n",
        "\n",
        "    with torch.no_grad():\n",
        "        otr = f(xtr) - otr0 if hyper.loss != 'cross_entropy' else f(xtr)\n",
        "        ote = f(xte) - ote0 if hyper.loss != 'cross_entropy' else f(xte)\n",
        "\n",
        "    out = {\n",
        "        'dynamics': dynamics,\n",
        "        'train': {\n",
        "            'f0': otr0,\n",
        "            'outputs': otr,\n",
        "            'labels': ytr,\n",
        "        },\n",
        "        'test': {\n",
        "            'f0': ote0,\n",
        "            'outputs': ote,\n",
        "            'labels': yte,\n",
        "        }\n",
        "    }\n",
        "    return f, out\n",
        "\n",
        "def run_exp(hyper, f0, xtr, ytr, xte, yte):\n",
        "    run = {\n",
        "        'hyper': hyper,\n",
        "        'N': sum(p.numel() for p in f0.parameters()),\n",
        "    }\n",
        "\n",
        "    if hyper.loss == 'hinge':\n",
        "        loss = hinge\n",
        "    elif hyper.loss == 'quad_hinge':\n",
        "        loss = quad_hinge\n",
        "    elif hyper.loss == 'mse':\n",
        "        loss = mse\n",
        "    elif hyper.loss == 'softhinge':\n",
        "        loss = partial(softhinge, beta=hyper.lossbeta)\n",
        "    elif hyper.loss == 'cross_entropy':\n",
        "        loss = cross_entropy\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid loss function: {hyper.loss}\")\n",
        "\n",
        "    _f, out = run_regular(hyper, f0, loss, xtr, ytr, xte, yte)\n",
        "    run['regular'] = out\n",
        "\n",
        "    yield run\n",
        "\n",
        "class HyperParams:\n",
        "    def __init__(self):\n",
        "        self.device = 'cuda'\n",
        "        self.dtype = 'float64'\n",
        "        self.init_seed = 0\n",
        "        self.batch_seed = 0\n",
        "        self.n = 30\n",
        "        self.k = 3\n",
        "        self.train_size = 700\n",
        "        self.test_size = 1000\n",
        "        self.normalize = True\n",
        "        self.data_seed = 0\n",
        "        self.alpha = 1e-4\n",
        "        self.f0 = 1\n",
        "        self.tau_over_h = 1e-3\n",
        "        self.tau_alpha_crit = 1e3\n",
        "        self.L = 3\n",
        "        self.h = 50\n",
        "        self.arch = 'fc_softplus'\n",
        "        self.spbeta = 5\n",
        "        self.bs = 32\n",
        "        self.bias = True\n",
        "        self.max_dgrad = 1e-4\n",
        "        self.max_dout = 0.1\n",
        "        self.loss = 'cross_entropy'\n",
        "        self.lossbeta = 20\n",
        "        self.train_time = 18000\n",
        "        self.chunk = 100\n",
        "        self.init_kernel = 0\n",
        "        self.delta_kernel = 0\n",
        "        self.final_kernel = 0\n",
        "        self.store_kernel = 0\n",
        "        self.save_outputs = 0\n",
        "        self.regular = 1\n",
        "        self.directory = 'C10k3Lsp_adam'\n",
        "        self.pickle = 'C10k3Lsp_adam.pickle'\n",
        "        # 新しいハイパーパラメータ\n",
        "        self.weight_decay = 2e-05\n",
        "        self.learning_rate = 0.006\n",
        "        self.max_steps = 10000\n",
        "        self.regularization = 'l1'\n",
        "\n",
        "import pickle\n",
        "\n",
        "def run_and_save_experiment(hyper):\n",
        "    if not os.path.exists(hyper.directory):\n",
        "        os.makedirs(hyper.directory)\n",
        "\n",
        "    pickle_path = os.path.join(hyper.directory, hyper.pickle)\n",
        "\n",
        "    if hyper.test_size is None:\n",
        "        hyper.test_size = hyper.train_size\n",
        "\n",
        "    if hyper.chunk is None:\n",
        "        hyper.chunk = hyper.train_size\n",
        "\n",
        "    try:\n",
        "        with open(pickle_path, 'wb') as f:\n",
        "            pickle.dump(hyper, f)\n",
        "\n",
        "        with open(pickle_path, 'ab') as f:\n",
        "            for res in execute(hyper):\n",
        "                pickle.dump(res, f)\n",
        "    except Exception as e:\n",
        "        if os.path.exists(pickle_path):\n",
        "            os.remove(pickle_path)\n",
        "        print(f\"An error occurred during saving: {e}\")\n",
        "        raise e\n",
        "\n",
        "######## 実験の実行と結果の保存\n",
        "hyper = HyperParams()\n",
        "run_and_save_experiment(hyper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ke3mzAkC64TM",
        "outputId": "f5a9e073-45aa-4a69-ce34-5ed78994c4d2"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available. Running on CPU.\n",
            "[i=0 wall=0] [train aL=7.10e-01 err=0.50 nd=0/700] [test aL=7.04e-01 err=0.48]\n",
            "[i=1 wall=0] [train aL=7.10e-01 err=0.49 nd=0/700] [test aL=7.07e-01 err=0.48]\n",
            "[i=2 wall=0] [train aL=7.11e-01 err=0.49 nd=0/700] [test aL=7.12e-01 err=0.47]\n",
            "[i=3 wall=0] [train aL=7.12e-01 err=0.49 nd=0/700] [test aL=7.16e-01 err=0.49]\n",
            "[i=4 wall=0] [train aL=7.11e-01 err=0.48 nd=0/700] [test aL=7.16e-01 err=0.49]\n",
            "[i=5 wall=0] [train aL=7.11e-01 err=0.48 nd=0/700] [test aL=7.19e-01 err=0.49]\n",
            "[i=6 wall=0] [train aL=7.11e-01 err=0.47 nd=0/700] [test aL=7.20e-01 err=0.49]\n",
            "[i=7 wall=0] [train aL=7.10e-01 err=0.48 nd=0/700] [test aL=7.21e-01 err=0.49]\n",
            "[i=8 wall=0] [train aL=7.07e-01 err=0.47 nd=0/700] [test aL=7.19e-01 err=0.49]\n",
            "[i=9 wall=0] [train aL=7.05e-01 err=0.47 nd=0/700] [test aL=7.17e-01 err=0.48]\n",
            "[i=10 wall=0] [train aL=7.03e-01 err=0.47 nd=0/700] [test aL=7.15e-01 err=0.48]\n",
            "[i=11 wall=0] [train aL=7.02e-01 err=0.47 nd=0/700] [test aL=7.16e-01 err=0.47]\n",
            "[i=13 wall=0] [train aL=6.99e-01 err=0.46 nd=0/700] [test aL=7.13e-01 err=0.46]\n",
            "[i=15 wall=0] [train aL=6.97e-01 err=0.45 nd=0/700] [test aL=7.12e-01 err=0.45]\n",
            "[i=17 wall=0] [train aL=6.95e-01 err=0.45 nd=0/700] [test aL=7.12e-01 err=0.46]\n",
            "[i=19 wall=0] [train aL=6.94e-01 err=0.44 nd=0/700] [test aL=7.14e-01 err=0.46]\n",
            "[i=21 wall=0] [train aL=6.92e-01 err=0.44 nd=0/700] [test aL=7.16e-01 err=0.46]\n",
            "[i=24 wall=0] [train aL=6.91e-01 err=0.44 nd=0/700] [test aL=7.21e-01 err=0.47]\n",
            "[i=27 wall=0] [train aL=6.91e-01 err=0.43 nd=0/700] [test aL=7.27e-01 err=0.48]\n",
            "[i=30 wall=0] [train aL=6.90e-01 err=0.43 nd=0/700] [test aL=7.30e-01 err=0.48]\n",
            "[i=33 wall=0] [train aL=6.84e-01 err=0.42 nd=0/700] [test aL=7.21e-01 err=0.47]\n",
            "[i=37 wall=0] [train aL=6.79e-01 err=0.42 nd=0/700] [test aL=7.12e-01 err=0.43]\n",
            "[i=41 wall=1] [train aL=6.73e-01 err=0.41 nd=0/700] [test aL=7.10e-01 err=0.45]\n",
            "[i=46 wall=1] [train aL=6.67e-01 err=0.40 nd=0/700] [test aL=7.11e-01 err=0.45]\n",
            "[i=51 wall=1] [train aL=6.67e-01 err=0.39 nd=0/700] [test aL=7.21e-01 err=0.48]\n",
            "[i=57 wall=1] [train aL=6.68e-01 err=0.39 nd=0/700] [test aL=7.32e-01 err=0.47]\n",
            "[i=63 wall=1] [train aL=6.58e-01 err=0.37 nd=0/700] [test aL=7.16e-01 err=0.45]\n",
            "[i=70 wall=1] [train aL=6.54e-01 err=0.36 nd=0/700] [test aL=7.27e-01 err=0.45]\n",
            "[i=77 wall=1] [train aL=6.53e-01 err=0.37 nd=0/700] [test aL=7.46e-01 err=0.46]\n",
            "[i=85 wall=1] [train aL=6.36e-01 err=0.34 nd=0/700] [test aL=7.32e-01 err=0.46]\n",
            "[i=94 wall=1] [train aL=6.33e-01 err=0.36 nd=0/700] [test aL=7.56e-01 err=0.48]\n",
            "[i=104 wall=1] [train aL=6.30e-01 err=0.36 nd=0/700] [test aL=7.67e-01 err=0.48]\n",
            "[i=115 wall=1] [train aL=6.19e-01 err=0.33 nd=0/700] [test aL=7.40e-01 err=0.47]\n",
            "[i=127 wall=1] [train aL=6.10e-01 err=0.34 nd=0/700] [test aL=7.69e-01 err=0.49]\n",
            "[i=140 wall=1] [train aL=5.95e-01 err=0.31 nd=0/700] [test aL=7.48e-01 err=0.47]\n",
            "[i=154 wall=1] [train aL=5.83e-01 err=0.30 nd=0/700] [test aL=7.60e-01 err=0.47]\n",
            "[i=170 wall=1] [train aL=5.65e-01 err=0.29 nd=0/700] [test aL=7.71e-01 err=0.47]\n",
            "[i=187 wall=1] [train aL=5.41e-01 err=0.26 nd=0/700] [test aL=7.51e-01 err=0.46]\n",
            "[i=206 wall=1] [train aL=5.30e-01 err=0.25 nd=0/700] [test aL=7.61e-01 err=0.46]\n",
            "[i=227 wall=1] [train aL=5.01e-01 err=0.25 nd=0/700] [test aL=7.94e-01 err=0.47]\n",
            "[i=250 wall=2] [train aL=4.92e-01 err=0.23 nd=0/700] [test aL=8.25e-01 err=0.46]\n",
            "[i=275 wall=2] [train aL=4.61e-01 err=0.22 nd=0/700] [test aL=8.63e-01 err=0.47]\n",
            "[i=303 wall=2] [train aL=4.21e-01 err=0.17 nd=0/700] [test aL=8.46e-01 err=0.47]\n",
            "[i=333 wall=2] [train aL=3.86e-01 err=0.14 nd=0/700] [test aL=8.46e-01 err=0.47]\n",
            "[i=366 wall=2] [train aL=3.50e-01 err=0.12 nd=0/700] [test aL=9.24e-01 err=0.44]\n",
            "[i=402 wall=2] [train aL=3.20e-01 err=0.11 nd=0/700] [test aL=9.55e-01 err=0.44]\n",
            "[i=442 wall=2] [train aL=2.95e-01 err=0.11 nd=0/700] [test aL=1.05e+00 err=0.44]\n",
            "[i=486 wall=2] [train aL=2.32e-01 err=0.07 nd=0/700] [test aL=1.01e+00 err=0.45]\n",
            "[i=534 wall=3] [train aL=2.00e-01 err=0.05 nd=0/700] [test aL=1.11e+00 err=0.44]\n",
            "[i=586 wall=3] [train aL=1.79e-01 err=0.06 nd=0/700] [test aL=1.25e+00 err=0.45]\n",
            "[i=643 wall=3] [train aL=1.17e-01 err=0.02 nd=0/700] [test aL=1.29e+00 err=0.44]\n",
            "[i=706 wall=3] [train aL=8.95e-02 err=0.01 nd=0/700] [test aL=1.42e+00 err=0.43]\n",
            "[i=775 wall=3] [train aL=6.02e-02 err=0.00 nd=0/700] [test aL=1.52e+00 err=0.43]\n",
            "[i=850 wall=4] [train aL=3.93e-02 err=0.00 nd=0/700] [test aL=1.62e+00 err=0.42]\n",
            "[i=932 wall=4] [train aL=2.46e-02 err=0.00 nd=0/700] [test aL=1.70e+00 err=0.43]\n",
            "[i=1021 wall=4] [train aL=2.06e-02 err=0.00 nd=0/700] [test aL=1.86e+00 err=0.43]\n",
            "[i=1119 wall=5] [train aL=1.38e-02 err=0.00 nd=0/700] [test aL=1.88e+00 err=0.42]\n",
            "[i=1225 wall=5] [train aL=1.21e-02 err=0.00 nd=0/700] [test aL=1.99e+00 err=0.42]\n",
            "[i=1341 wall=6] [train aL=8.64e-03 err=0.00 nd=0/700] [test aL=1.99e+00 err=0.42]\n",
            "[i=1467 wall=6] [train aL=7.76e-03 err=0.00 nd=0/700] [test aL=2.01e+00 err=0.41]\n",
            "[i=1604 wall=7] [train aL=6.41e-03 err=0.00 nd=0/700] [test aL=2.03e+00 err=0.40]\n",
            "[i=1753 wall=8] [train aL=6.00e-03 err=0.00 nd=0/700] [test aL=2.02e+00 err=0.40]\n",
            "[i=1914 wall=8] [train aL=5.39e-03 err=0.00 nd=0/700] [test aL=1.97e+00 err=0.40]\n",
            "[i=2089 wall=9] [train aL=5.47e-03 err=0.00 nd=0/700] [test aL=1.93e+00 err=0.40]\n",
            "[i=2278 wall=10] [train aL=4.68e-03 err=0.00 nd=0/700] [test aL=1.80e+00 err=0.38]\n",
            "[i=2482 wall=10] [train aL=4.56e-03 err=0.00 nd=0/700] [test aL=1.65e+00 err=0.36]\n",
            "[i=2702 wall=11] [train aL=3.54e-03 err=0.00 nd=0/700] [test aL=1.41e+00 err=0.33]\n",
            "[i=2939 wall=12] [train aL=3.39e-03 err=0.00 nd=0/700] [test aL=1.11e+00 err=0.29]\n",
            "[i=3194 wall=13] [train aL=2.74e-03 err=0.00 nd=0/700] [test aL=7.23e-01 err=0.23]\n",
            "[i=3468 wall=14] [train aL=2.40e-03 err=0.00 nd=0/700] [test aL=3.60e-01 err=0.14]\n",
            "[i=3762 wall=15] [train aL=2.11e-03 err=0.00 nd=0/700] [test aL=1.21e-01 err=0.05]\n",
            "[i=4076 wall=16] [train aL=1.66e-03 err=0.00 nd=0/700] [test aL=2.51e-02 err=0.01]\n",
            "[i=4411 wall=17] [train aL=1.39e-03 err=0.00 nd=0/700] [test aL=5.37e-03 err=0.00]\n",
            "[i=4768 wall=18] [train aL=1.27e-03 err=0.00 nd=0/700] [test aL=2.27e-03 err=0.00]\n",
            "[i=5148 wall=20] [train aL=1.13e-03 err=0.00 nd=0/700] [test aL=1.39e-03 err=0.00]\n",
            "[i=5551 wall=22] [train aL=1.06e-03 err=0.00 nd=0/700] [test aL=1.17e-03 err=0.00]\n",
            "[i=5977 wall=23] [train aL=1.01e-03 err=0.00 nd=0/700] [test aL=1.13e-03 err=0.00]\n",
            "[i=6427 wall=24] [train aL=9.31e-04 err=0.00 nd=0/700] [test aL=1.01e-03 err=0.00]\n",
            "[i=6902 wall=26] [train aL=8.71e-04 err=0.00 nd=0/700] [test aL=9.47e-04 err=0.00]\n",
            "[i=7401 wall=27] [train aL=8.26e-04 err=0.00 nd=0/700] [test aL=8.91e-04 err=0.00]\n",
            "[i=7924 wall=29] [train aL=8.01e-04 err=0.00 nd=0/700] [test aL=8.87e-04 err=0.00]\n",
            "[i=8472 wall=30] [train aL=7.70e-04 err=0.00 nd=0/700] [test aL=8.74e-04 err=0.00]\n",
            "[i=9044 wall=32] [train aL=7.48e-04 err=0.00 nd=0/700] [test aL=8.62e-04 err=0.00]\n",
            "[i=9640 wall=35] [train aL=6.98e-04 err=0.00 nd=0/700] [test aL=7.78e-04 err=0.00]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# ANSI escape sequences for colors\n",
        "COLORS = {\n",
        "    'HEADER': '\\033[95m',  # Magenta\n",
        "    'OKCYAN': '\\033[96m',  # Cyan\n",
        "    'OKGREEN': '\\033[92m',  # Green\n",
        "    'WARNING': '\\033[93m',  # Yellow\n",
        "    'FAIL': '\\033[91m',    # Red\n",
        "    'ENDC': '\\033[0m',     # Reset to default\n",
        "    'BOLD': '\\033[1m',     # Bold\n",
        "    'UNDERLINE': '\\033[4m' # Underline\n",
        "}\n",
        "\n",
        "def color_text(text, color_name):\n",
        "    return f\"{COLORS[color_name]}{text}{COLORS['ENDC']}\"\n",
        "\n",
        "def load_and_print_summary(file_path, verbose=False):\n",
        "    \"\"\"\n",
        "    保存された pickle ファイルを読み込み，その中身の概要を出力する関数\n",
        "\n",
        "    Parameters:\n",
        "    - file_path: 読み込む pickle ファイルのパス\n",
        "    - verbose: データの詳細表示を行うかどうかのフラグ\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            # 初めに HyperParams オブジェクトを読み込む\n",
        "            hyper = pickle.load(f)\n",
        "\n",
        "            # ハイパーパラメータの概要表示\n",
        "            print(color_text(f\"Loaded HyperParams object:\", 'HEADER'))\n",
        "            print(f\"  Type: {type(hyper).__name__}\")\n",
        "            print(f\"  Attributes:\")\n",
        "            for attribute in dir(hyper):\n",
        "                if not attribute.startswith('__') and not callable(getattr(hyper, attribute)):\n",
        "                    value = getattr(hyper, attribute)\n",
        "                    print(f\"    {attribute}: {value} ({type(value).__name__})\")\n",
        "\n",
        "            # 次に，実験結果が続く場合\n",
        "            results = []\n",
        "            while True:\n",
        "                try:\n",
        "                    result = pickle.load(f)\n",
        "                    results.append(result)\n",
        "                except EOFError:\n",
        "                    break\n",
        "\n",
        "            # 実験結果の概要を表示\n",
        "            print(color_text(\"\\nExperiment results summary:\", 'OKCYAN'))\n",
        "            print(f\"  Number of results: {len(results)}\")\n",
        "            if len(results) > 0:\n",
        "                if isinstance(results[0], dict):\n",
        "                    print(\"  Sample keys from result dictionaries:\")\n",
        "                    sample_result = results[0]\n",
        "                    sample_keys = {}\n",
        "                    if isinstance(sample_result, dict):\n",
        "                        for key, value in sample_result.items():\n",
        "                            if isinstance(value, (list, dict)):\n",
        "                                entry_count = len(value)\n",
        "                                sample_keys[key] = (f\"{entry_count} entries\", type(value).__name__)\n",
        "                            else:\n",
        "                                sample_keys[key] = (f\"{value}\", type(value).__name__)\n",
        "                    print(f\"    Number of keys: {len(sample_keys)}\")\n",
        "                    for key, (entry_or_value, dtype) in sample_keys.items():\n",
        "                        print(f\"    {key}: {entry_or_value}, Type: {dtype}\")\n",
        "                else:\n",
        "                    print(f\"  Type of results: {type(results[0]).__name__}\")\n",
        "\n",
        "            # regular に含まれるキーとその数を表示\n",
        "            if len(results) > 0 and isinstance(results[0], dict):\n",
        "                regular_keys = set()\n",
        "                key_data_counts = {}\n",
        "\n",
        "                for result in results:\n",
        "                    if 'regular' in result and isinstance(result['regular'], dict):\n",
        "                        for key, value in result['regular'].items():\n",
        "                            if key not in key_data_counts:\n",
        "                                key_data_counts[key] = (0, None)\n",
        "                            count, dtype = key_data_counts[key]\n",
        "                            key_data_counts[key] = (count + 1, dtype if dtype else type(value).__name__)\n",
        "                            regular_keys.add(key)\n",
        "\n",
        "                print(color_text(f\"\\nKeys in 'regular' and their count:\", 'OKGREEN'))\n",
        "                print(f\"  Number of keys: {len(key_data_counts)}\")\n",
        "                for key, (count, dtype) in key_data_counts.items():\n",
        "                    print(f\"    {key}: {count} entries, Type: {dtype}\")\n",
        "\n",
        "                # 各キー (dynamics, train, test) の詳細を表示\n",
        "                for key in ['dynamics', 'train', 'test']:\n",
        "                    if key in regular_keys:\n",
        "                        if key == 'dynamics':\n",
        "                            for i, dynamics_list in enumerate([result['regular'][key] for result in results if 'regular' in result and key in result['regular']], 1):\n",
        "                                print(color_text(f\"\\nDetails of 'dynamics' list {i}:\", 'HEADER'))\n",
        "                                if len(dynamics_list) > 0 and isinstance(dynamics_list[0], dict):\n",
        "                                    entry_description = {subkey: type(value).__name__ for subkey, value in dynamics_list[0].items()}\n",
        "                                    print(f\"  Example entry data types in 'dynamics' list {i}: {entry_description}\")\n",
        "                                print(f\"  Number of entries in 'dynamics' list {i}: {len(dynamics_list)}\")\n",
        "                        else:\n",
        "                            key_data_counts = {}\n",
        "                            for result in results:\n",
        "                                if 'regular' in result and key in result['regular']:\n",
        "                                    if isinstance(result['regular'][key], dict):\n",
        "                                        for subkey, value in result['regular'][key].items():\n",
        "                                            if subkey not in key_data_counts:\n",
        "                                                key_data_counts[subkey] = (0, None)\n",
        "                                            count, dtype = key_data_counts[subkey]\n",
        "                                            key_data_counts[subkey] = (count + 1, dtype if dtype else type(value).__name__)\n",
        "\n",
        "                            print(color_text(f\"\\nKeys in 'regular[{key}]' and their count:\", 'OKGREEN'))\n",
        "                            print(f\"  Number of keys: {len(key_data_counts)}\")\n",
        "                            for subkey, (count, dtype) in key_data_counts.items():\n",
        "                                print(f\"    {subkey}: {count} entries, Type: {dtype}\")\n",
        "\n",
        "                # dynamics 内の train と test キーのサブキーを表示\n",
        "                if 'dynamics' in regular_keys:\n",
        "                    dynamics_train_keys = {}\n",
        "                    dynamics_test_keys = {}\n",
        "                    for result in results:\n",
        "                        if 'regular' in result and 'dynamics' in result['regular']:\n",
        "                            for entry in result['regular']['dynamics']:\n",
        "                                if isinstance(entry, dict):\n",
        "                                    if 'train' in entry and isinstance(entry['train'], dict):\n",
        "                                        for subkey, value in entry['train'].items():\n",
        "                                            if subkey not in dynamics_train_keys:\n",
        "                                                dynamics_train_keys[subkey] = (0, None)\n",
        "                                            count, dtype = dynamics_train_keys[subkey]\n",
        "                                            dynamics_train_keys[subkey] = (count + 1, dtype if dtype else type(value).__name__)\n",
        "                                    if 'test' in entry and isinstance(entry['test'], dict):\n",
        "                                        for subkey, value in entry['test'].items():\n",
        "                                            if subkey not in dynamics_test_keys:\n",
        "                                                dynamics_test_keys[subkey] = (0, None)\n",
        "                                            count, dtype = dynamics_test_keys[subkey]\n",
        "                                            dynamics_test_keys[subkey] = (count + 1, dtype if dtype else type(value).__name__)\n",
        "\n",
        "                    print(color_text(f\"\\nKeys in 'dynamics[train]' and their count:\", 'OKGREEN'))\n",
        "                    print(f\"  Number of keys: {len(dynamics_train_keys)}\")\n",
        "                    for subkey, (count, dtype) in dynamics_train_keys.items():\n",
        "                        print(f\"    {subkey}: {count} entries, Type: {dtype}\")\n",
        "\n",
        "                    print(color_text(f\"\\nKeys in 'dynamics[test]' and their count:\", 'OKGREEN'))\n",
        "                    print(f\"  Number of keys: {len(dynamics_test_keys)}\")\n",
        "                    for subkey, (count, dtype) in dynamics_test_keys.items():\n",
        "                        print(f\"    {subkey}: {count} entries, Type: {dtype}\")\n",
        "\n",
        "            # 詳細なデータを表示（verbose が True の場合のみ）\n",
        "            if verbose:\n",
        "                print(color_text(\"\\nLoaded experiment results:\", 'HEADER'))\n",
        "                for i, result in enumerate(results):\n",
        "                    print(color_text(f\"\\nResult {i+1}:\", 'HEADER'))\n",
        "                    if isinstance(result, dict):\n",
        "                        print(f\"  Type: dict\")\n",
        "                        print(f\"  Keys: {list(result.keys())}\")\n",
        "                        print(f\"  All Values:\")\n",
        "                        for key in result.keys():\n",
        "                            print(f\"    {key}: {result[key]}\")\n",
        "                    else:\n",
        "                        print(f\"  Type: {type(result).__name__}\")\n",
        "                        print(f\"  Content: {result}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the file: {e}\")\n",
        "\n",
        "# 使用例\n",
        "file_path = 'C10k3Lsp_adam/C10k3Lsp_adam.pickle'\n",
        "load_and_print_summary(file_path, verbose=True)  # 詳細表示を無効にする場合"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHjlA6q6C4Cz",
        "outputId": "3b0af35a-2c85-4a21-bc2b-252003c4eb62"
      },
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[95mLoaded HyperParams object:\u001b[0m\n",
            "  Type: HyperParams\n",
            "  Attributes:\n",
            "    L: 3 (int)\n",
            "    alpha: 0.0001 (float)\n",
            "    arch: fc_softplus (str)\n",
            "    batch_seed: 0 (int)\n",
            "    bias: True (bool)\n",
            "    bs: 32 (int)\n",
            "    chunk: 100 (int)\n",
            "    data_seed: 0 (int)\n",
            "    delta_kernel: 0 (int)\n",
            "    device: cuda (str)\n",
            "    directory: C10k3Lsp_adam (str)\n",
            "    dtype: float64 (str)\n",
            "    f0: 1 (int)\n",
            "    final_kernel: 0 (int)\n",
            "    h: 50 (int)\n",
            "    init_kernel: 0 (int)\n",
            "    init_seed: 0 (int)\n",
            "    k: 3 (int)\n",
            "    learning_rate: 0.006 (float)\n",
            "    loss: cross_entropy (str)\n",
            "    lossbeta: 20 (int)\n",
            "    max_dgrad: 0.0001 (float)\n",
            "    max_dout: 0.1 (float)\n",
            "    max_steps: 10000 (int)\n",
            "    n: 30 (int)\n",
            "    normalize: True (bool)\n",
            "    pickle: C10k3Lsp_adam.pickle (str)\n",
            "    regular: 1 (int)\n",
            "    regularization: l1 (str)\n",
            "    save_outputs: 0 (int)\n",
            "    spbeta: 5 (int)\n",
            "    store_kernel: 0 (int)\n",
            "    tau_alpha_crit: 1000.0 (float)\n",
            "    tau_over_h: 0.001 (float)\n",
            "    test_size: 1000 (int)\n",
            "    train_size: 700 (int)\n",
            "    train_time: 18000 (int)\n",
            "    weight_decay: 2e-05 (float)\n",
            "\u001b[96m\n",
            "Experiment results summary:\u001b[0m\n",
            "  Number of results: 1\n",
            "  Sample keys from result dictionaries:\n",
            "    Number of keys: 3\n",
            "    hyper: <__main__.HyperParams object at 0x7cffaa0f5750>, Type: HyperParams\n",
            "    N: 6701, Type: int\n",
            "    regular: 3 entries, Type: dict\n",
            "\u001b[92m\n",
            "Keys in 'regular' and their count:\u001b[0m\n",
            "  Number of keys: 3\n",
            "    dynamics: 1 entries, Type: list\n",
            "    train: 1 entries, Type: dict\n",
            "    test: 1 entries, Type: dict\n",
            "\u001b[95m\n",
            "Details of 'dynamics' list 1:\u001b[0m\n",
            "  Example entry data types in 'dynamics' list 1: {'step': 'int', 'wall': 'float', 'batch_loss': 'float', 'norm': 'float', 'dnorm': 'float', 'train': 'dict', 'test': 'dict', 'wnorm': 'list', 'dwnorm': 'list'}\n",
            "  Number of entries in 'dynamics' list 1: 84\n",
            "\u001b[92m\n",
            "Keys in 'regular[train]' and their count:\u001b[0m\n",
            "  Number of keys: 3\n",
            "    f0: 1 entries, Type: Tensor\n",
            "    outputs: 1 entries, Type: Tensor\n",
            "    labels: 1 entries, Type: Tensor\n",
            "\u001b[92m\n",
            "Keys in 'regular[test]' and their count:\u001b[0m\n",
            "  Number of keys: 3\n",
            "    f0: 1 entries, Type: Tensor\n",
            "    outputs: 1 entries, Type: Tensor\n",
            "    labels: 1 entries, Type: Tensor\n",
            "\u001b[92m\n",
            "Keys in 'dynamics[train]' and their count:\u001b[0m\n",
            "  Number of keys: 7\n",
            "    loss: 84 entries, Type: float\n",
            "    aloss: 84 entries, Type: float\n",
            "    aaloss: 84 entries, Type: float\n",
            "    err: 84 entries, Type: float\n",
            "    nd: 84 entries, Type: int\n",
            "    dfnorm: 84 entries, Type: Tensor\n",
            "    fnorm: 84 entries, Type: Tensor\n",
            "\u001b[92m\n",
            "Keys in 'dynamics[test]' and their count:\u001b[0m\n",
            "  Number of keys: 7\n",
            "    loss: 84 entries, Type: float\n",
            "    aloss: 84 entries, Type: float\n",
            "    aaloss: 84 entries, Type: float\n",
            "    err: 84 entries, Type: float\n",
            "    nd: 84 entries, Type: int\n",
            "    dfnorm: 84 entries, Type: Tensor\n",
            "    fnorm: 84 entries, Type: Tensor\n",
            "\u001b[95m\n",
            "Loaded experiment results:\u001b[0m\n",
            "\u001b[95m\n",
            "Result 1:\u001b[0m\n",
            "  Type: dict\n",
            "  Keys: ['hyper', 'N', 'regular']\n",
            "  All Values:\n",
            "    hyper: <__main__.HyperParams object at 0x7cffaa0f5750>\n",
            "    N: 6701\n",
            "    regular: {'dynamics': [{'step': 0, 'wall': 0.01184870100041735, 'batch_loss': 0.7968352136670752, 'norm': 80.4820279441315, 'dnorm': 0.49106236378435025, 'train': {'loss': 0.7097318003053928, 'aloss': 0.7097318003053928, 'aaloss': 0.7097318003053928, 'err': 0.49714285135269165, 'nd': 0, 'dfnorm': tensor(0.3723), 'fnorm': tensor(0.3723)}, 'test': {'loss': 0.7035889364935375, 'aloss': 0.7035889364935375, 'aaloss': 0.7035889364935375, 'err': 0.4814285635948181, 'nd': 0, 'dfnorm': tensor(0.3714), 'fnorm': tensor(0.3714)}, 'wnorm': [39.05115856885033, 49.396493420512364, 49.82594658141633, 5.45159479580254], 'dwnorm': [0.23234094148568268, 0.29996399522477873, 0.2999099302735704, 0.04242559785271834]}, {'step': 1, 'wall': 0.030823730001429794, 'batch_loss': 0.8056516073308618, 'norm': 80.44266897120889, 'dnorm': 0.7310278632254721, 'train': {'loss': 0.7095593339754044, 'aloss': 0.7095593339754044, 'aaloss': 0.7095593339754044, 'err': 0.4928571283817291, 'nd': 0, 'dfnorm': tensor(0.3995), 'fnorm': tensor(0.3995)}, 'test': {'loss': 0.7066464685860764, 'aloss': 0.7066464685860764, 'aaloss': 0.7066464685860764, 'err': 0.47857141494750977, 'nd': 0, 'dfnorm': tensor(0.4009), 'fnorm': tensor(0.4009)}, 'wnorm': [39.04522270274903, 49.38557241218766, 49.777206974030896, 5.456954206049023], 'dwnorm': [0.3575841929286767, 0.44265056477413806, 0.4442251859116082, 0.05594873652506063]}, {'step': 2, 'wall': 0.04978480900172144, 'batch_loss': 0.7882972226191797, 'norm': 80.40330813641239, 'dnorm': 0.9621305531654645, 'train': {'loss': 0.7108466904909431, 'aloss': 0.7108466904909431, 'aaloss': 0.7108466904909431, 'err': 0.49000000953674316, 'nd': 0, 'dfnorm': tensor(0.4435), 'fnorm': tensor(0.4435)}, 'test': {'loss': 0.7119876132472771, 'aloss': 0.7119876132472771, 'aaloss': 0.7119876132472771, 'err': 0.4714285731315613, 'nd': 0, 'dfnorm': tensor(0.4481), 'fnorm': tensor(0.4481)}, 'wnorm': [39.04092638447281, 49.37253019101071, 49.7293782436928, 5.461011084231502], 'dwnorm': [0.46119548340204203, 0.5781347161745584, 0.5966244479575691, 0.0721606059636587]}, {'step': 3, 'wall': 0.06740809199982323, 'batch_loss': 0.7955859568689583, 'norm': 80.36257888863292, 'dnorm': 1.1675768684658476, 'train': {'loss': 0.712033385535563, 'aloss': 0.712033385535563, 'aaloss': 0.712033385535563, 'err': 0.48571428656578064, 'nd': 0, 'dfnorm': tensor(0.4776), 'fnorm': tensor(0.4776)}, 'test': {'loss': 0.7158343225690927, 'aloss': 0.7158343225690927, 'aaloss': 0.7158343225690927, 'err': 0.48571428656578064, 'nd': 0, 'dfnorm': tensor(0.4839), 'fnorm': tensor(0.4839)}, 'wnorm': [39.035696710282714, 49.35815332490589, 49.68143175478725, 5.464571988396748], 'dwnorm': [0.5510117995727636, 0.6929227791064623, 0.7395688391698939, 0.0869343840556951]}, {'step': 4, 'wall': 0.08795687699966948, 'batch_loss': 0.801584685723206, 'norm': 80.31990208925747, 'dnorm': 1.3478710640120015, 'train': {'loss': 0.7110512879765861, 'aloss': 0.7110512879765861, 'aaloss': 0.7110512879765861, 'err': 0.4828571379184723, 'nd': 0, 'dfnorm': tensor(0.4859), 'fnorm': tensor(0.4859)}, 'test': {'loss': 0.7161502110187143, 'aloss': 0.7161502110187143, 'aaloss': 0.7161502110187143, 'err': 0.488571435213089, 'nd': 0, 'dfnorm': tensor(0.4933), 'fnorm': tensor(0.4933)}, 'wnorm': [39.028938621625976, 49.34411951201552, 49.63132848410935, 5.4669040965886735], 'dwnorm': [0.6331192254569852, 0.7897252276003753, 0.8670090262154254, 0.0993080187371128]}, {'step': 5, 'wall': 0.11591784600022947, 'batch_loss': 0.7623579977553994, 'norm': 80.27766415827516, 'dnorm': 1.5312491622754274, 'train': {'loss': 0.7112740263999446, 'aloss': 0.7112740263999446, 'aaloss': 0.7112740263999446, 'err': 0.47857141494750977, 'nd': 0, 'dfnorm': tensor(0.5090), 'fnorm': tensor(0.5090)}, 'test': {'loss': 0.7188843447451547, 'aloss': 0.7188843447451547, 'aaloss': 0.7188843447451547, 'err': 0.4871428608894348, 'nd': 0, 'dfnorm': tensor(0.5180), 'fnorm': tensor(0.5180)}, 'wnorm': [39.02278026717717, 49.33041621537438, 49.581043923319285, 5.469690077735865], 'dwnorm': [0.7110312937338487, 0.8873962404592471, 1.0001788532759748, 0.11285298589129084]}, {'step': 6, 'wall': 0.13670536000063294, 'batch_loss': 0.7926382733484039, 'norm': 80.23591259488775, 'dnorm': 1.6962478402493206, 'train': {'loss': 0.7106044155322369, 'aloss': 0.7106044155322369, 'aaloss': 0.7106044155322369, 'err': 0.47285714745521545, 'nd': 0, 'dfnorm': tensor(0.5204), 'fnorm': tensor(0.5204)}, 'test': {'loss': 0.7199066762453726, 'aloss': 0.7199066762453726, 'aaloss': 0.7199066762453726, 'err': 0.488571435213089, 'nd': 0, 'dfnorm': tensor(0.5308), 'fnorm': tensor(0.5308)}, 'wnorm': [39.01580644791754, 49.316571185399866, 49.532444175891335, 5.471357160635249], 'dwnorm': [0.7805718595039383, 0.9748105320075576, 1.1210571211592655, 0.12406133169607107]}, {'step': 7, 'wall': 0.15607922700291965, 'batch_loss': 0.7993812158125906, 'norm': 80.1946622277596, 'dnorm': 1.8498626077811933, 'train': {'loss': 0.7097833241505428, 'aloss': 0.7097833241505428, 'aaloss': 0.7097833241505428, 'err': 0.4757142961025238, 'nd': 0, 'dfnorm': tensor(0.5292), 'fnorm': tensor(0.5292)}, 'test': {'loss': 0.7208553800412323, 'aloss': 0.7208553800412323, 'aaloss': 0.7208553800412323, 'err': 0.49142858386039734, 'nd': 0, 'dfnorm': tensor(0.5408), 'fnorm': tensor(0.5408)}, 'wnorm': [39.00940346293598, 49.30275228333163, 49.48418213184535, 5.472937665942616], 'dwnorm': [0.8437217645694983, 1.0528455425410774, 1.237745483636619, 0.1343391496029478]}, {'step': 8, 'wall': 0.17698480200124322, 'batch_loss': 0.7997855295836673, 'norm': 80.15203599326748, 'dnorm': 1.9905921121847048, 'train': {'loss': 0.7073061751587085, 'aloss': 0.7073061751587085, 'aaloss': 0.7073061751587085, 'err': 0.4657142758369446, 'nd': 0, 'dfnorm': tensor(0.5214), 'fnorm': tensor(0.5214)}, 'test': {'loss': 0.7193454793337345, 'aloss': 0.7193454793337345, 'aaloss': 0.7193454793337345, 'err': 0.49142858386039734, 'nd': 0, 'dfnorm': tensor(0.5340), 'fnorm': tensor(0.5340)}, 'wnorm': [39.00269441712932, 49.28835248041475, 49.434654208432626, 5.4730944453248345], 'dwnorm': [0.9022771884970233, 1.122666963567078, 1.3454634645540806, 0.14313442967721343]}, {'step': 9, 'wall': 0.19580262800081982, 'batch_loss': 0.7873189097473727, 'norm': 80.11010707403683, 'dnorm': 2.1248695947735303, 'train': {'loss': 0.7047775874655517, 'aloss': 0.7047775874655517, 'aaloss': 0.7047775874655517, 'err': 0.4657142758369446, 'nd': 0, 'dfnorm': tensor(0.5104), 'fnorm': tensor(0.5104)}, 'test': {'loss': 0.7171272215415699, 'aloss': 0.7171272215415699, 'aaloss': 0.7171272215415699, 'err': 0.48428571224212646, 'nd': 0, 'dfnorm': tensor(0.5238), 'fnorm': tensor(0.5238)}, 'wnorm': [38.996417471619736, 49.27459359960851, 49.38529444947774, 5.472931920378225], 'dwnorm': [0.9574398632620539, 1.1873837359510861, 1.450119422071629, 0.15112745896883034]}, {'step': 10, 'wall': 0.2186548310019134, 'batch_loss': 0.778634911878376, 'norm': 80.06929384921094, 'dnorm': 2.259586322453359, 'train': {'loss': 0.702661703710318, 'aloss': 0.702661703710318, 'aaloss': 0.702661703710318, 'err': 0.4699999988079071, 'nd': 0, 'dfnorm': tensor(0.5000), 'fnorm': tensor(0.5000)}, 'test': {'loss': 0.7147129702209559, 'aloss': 0.7147129702209559, 'aaloss': 0.7147129702209559, 'err': 0.4757142961025238, 'nd': 0, 'dfnorm': tensor(0.5133), 'fnorm': tensor(0.5133)}, 'wnorm': [38.99067041736792, 49.26108355961976, 49.33694172677594, 5.473736409416868], 'dwnorm': [1.012137700080831, 1.2520386197251032, 1.5553569042131774, 0.15841907507808678]}, {'step': 11, 'wall': 0.23811615399972652, 'batch_loss': 0.7622775479667372, 'norm': 80.02818404899226, 'dnorm': 2.396811885100954, 'train': {'loss': 0.7017221814074334, 'aloss': 0.7017221814074334, 'aaloss': 0.7017221814074334, 'err': 0.4657142758369446, 'nd': 0, 'dfnorm': tensor(0.5080), 'fnorm': tensor(0.5080)}, 'test': {'loss': 0.7155886002466638, 'aloss': 0.7155886002466638, 'aaloss': 0.7155886002466638, 'err': 0.47285714745521545, 'nd': 0, 'dfnorm': tensor(0.5222), 'fnorm': tensor(0.5222)}, 'wnorm': [38.98506246184102, 49.24685492357706, 49.288684732799204, 5.474687375807627], 'dwnorm': [1.0679381216185238, 1.3129958044227368, 1.666297783788743, 0.16711219128748145]}, {'step': 13, 'wall': 0.2607042370000272, 'batch_loss': 0.7962824891323059, 'norm': 79.94473943195862, 'dnorm': 2.652889283248464, 'train': {'loss': 0.6986957961211021, 'aloss': 0.6986957961211021, 'aaloss': 0.6986957961211021, 'err': 0.46000000834465027, 'nd': 0, 'dfnorm': tensor(0.5018), 'fnorm': tensor(0.5018)}, 'test': {'loss': 0.713197169436088, 'aloss': 0.713197169436088, 'aaloss': 0.713197169436088, 'err': 0.46142858266830444, 'nd': 0, 'dfnorm': tensor(0.5153), 'fnorm': tensor(0.5153)}, 'wnorm': [38.971682016611766, 49.217330854366416, 49.193150374082315, 5.47445650018662], 'dwnorm': [1.1708153759905595, 1.4222816623587367, 1.877000996727865, 0.1816742236489443]}, {'step': 15, 'wall': 0.2848353959998349, 'batch_loss': 0.7675464806879283, 'norm': 79.86035875668868, 'dnorm': 2.89273080825867, 'train': {'loss': 0.6966463867562819, 'aloss': 0.6966463867562819, 'aaloss': 0.6966463867562819, 'err': 0.4514285624027252, 'nd': 0, 'dfnorm': tensor(0.5011), 'fnorm': tensor(0.5011)}, 'test': {'loss': 0.7115617356383581, 'aloss': 0.7115617356383581, 'aaloss': 0.7115617356383581, 'err': 0.45428571105003357, 'nd': 0, 'dfnorm': tensor(0.5143), 'fnorm': tensor(0.5143)}, 'wnorm': [38.95822557486307, 49.18626138715366, 49.09778577180742, 5.473234050848835], 'dwnorm': [1.2668816892848727, 1.5211042853580707, 2.0772695548146816, 0.19417099183460476]}, {'step': 17, 'wall': 0.3066765920011676, 'batch_loss': 0.7263328551493673, 'norm': 79.78020354346692, 'dnorm': 3.1537281060508766, 'train': {'loss': 0.6950847914183483, 'aloss': 0.6950847914183483, 'aaloss': 0.6950847914183483, 'err': 0.44999998807907104, 'nd': 0, 'dfnorm': tensor(0.5136), 'fnorm': tensor(0.5136)}, 'test': {'loss': 0.7122927315753871, 'aloss': 0.7122927315753871, 'aaloss': 0.7122927315753871, 'err': 0.45571428537368774, 'nd': 0, 'dfnorm': tensor(0.5283), 'fnorm': tensor(0.5283)}, 'wnorm': [38.948005185422964, 49.15743764381114, 49.00394833428854, 5.475713596011714], 'dwnorm': [1.3650809952396965, 1.632545562391832, 2.294200798011257, 0.21045297698236984]}, {'step': 19, 'wall': 0.33096558400211507, 'batch_loss': 0.739962785399102, 'norm': 79.70008772905766, 'dnorm': 3.4301480562355966, 'train': {'loss': 0.6935550474573128, 'aloss': 0.6935550474573128, 'aaloss': 0.6935550474573128, 'err': 0.44428572058677673, 'nd': 0, 'dfnorm': tensor(0.5328), 'fnorm': tensor(0.5328)}, 'test': {'loss': 0.7140556658348504, 'aloss': 0.7140556658348504, 'aaloss': 0.7140556658348504, 'err': 0.4628571569919586, 'nd': 0, 'dfnorm': tensor(0.5500), 'fnorm': tensor(0.5500)}, 'wnorm': [38.93823236482046, 49.128908323095025, 48.90924133870466, 5.4795453796657085], 'dwnorm': [1.4657117464642937, 1.7582406430559787, 2.5189283736179076, 0.2282302466276716]}, {'step': 21, 'wall': 0.35455812100190087, 'batch_loss': 0.7892990065780259, 'norm': 79.61967013192235, 'dnorm': 3.68629614655243, 'train': {'loss': 0.6921932912817945, 'aloss': 0.6921932912817945, 'aaloss': 0.6921932912817945, 'err': 0.44428572058677673, 'nd': 0, 'dfnorm': tensor(0.5513), 'fnorm': tensor(0.5513)}, 'test': {'loss': 0.7161197611040572, 'aloss': 0.7161197611040572, 'aaloss': 0.7161197611040572, 'err': 0.4628571569919586, 'nd': 0, 'dfnorm': tensor(0.5703), 'fnorm': tensor(0.5703)}, 'wnorm': [38.929280255911294, 49.10061495392075, 48.81317945366651, 5.48253492389379], 'dwnorm': [1.5589437932920511, 1.8730234552007978, 2.7282701601730563, 0.24259914824274312]}, {'step': 24, 'wall': 0.3827652300024056, 'batch_loss': 0.7076120140697376, 'norm': 79.50126494111403, 'dnorm': 4.069687917493822, 'train': {'loss': 0.691454486534632, 'aloss': 0.691454486534632, 'aaloss': 0.691454486534632, 'err': 0.44428572058677673, 'nd': 0, 'dfnorm': tensor(0.5892), 'fnorm': tensor(0.5892)}, 'test': {'loss': 0.7208134256669686, 'aloss': 0.7208134256669686, 'aaloss': 0.7208134256669686, 'err': 0.4699999988079071, 'nd': 0, 'dfnorm': tensor(0.6111), 'fnorm': tensor(0.6111)}, 'wnorm': [38.918237825873, 49.05909393046452, 48.669400381534835, 5.4897366134903045], 'dwnorm': [1.699731731770064, 2.04848960396928, 3.0378139092359495, 0.26305109176173075]}, {'step': 27, 'wall': 0.40752138900279533, 'batch_loss': 0.8198524230252991, 'norm': 79.38378983619391, 'dnorm': 4.444817284772317, 'train': {'loss': 0.691444351135979, 'aloss': 0.691444351135979, 'aaloss': 0.691444351135979, 'err': 0.4342857003211975, 'nd': 0, 'dfnorm': tensor(0.6309), 'fnorm': tensor(0.6309)}, 'test': {'loss': 0.7265315015982908, 'aloss': 0.7265315015982908, 'aaloss': 0.7265315015982908, 'err': 0.4757142961025238, 'nd': 0, 'dfnorm': tensor(0.6577), 'fnorm': tensor(0.6577)}, 'wnorm': [38.907915649224215, 49.01681641359047, 48.52704987244534, 5.497802054070871], 'dwnorm': [1.8355275641506008, 2.22002365987987, 3.3419659245779805, 0.28017395864656397]}, {'step': 30, 'wall': 0.43148433600072167, 'batch_loss': 0.7876820671698953, 'norm': 79.25940627750624, 'dnorm': 4.774459915386334, 'train': {'loss': 0.6904514819054874, 'aloss': 0.6904514819054874, 'aaloss': 0.6904514819054874, 'err': 0.43142858147621155, 'nd': 0, 'dfnorm': tensor(0.6521), 'fnorm': tensor(0.6521)}, 'test': {'loss': 0.7299091901309467, 'aloss': 0.7299091901309467, 'aaloss': 0.7299091901309467, 'err': 0.47999998927116394, 'nd': 0, 'dfnorm': tensor(0.6813), 'fnorm': tensor(0.6813)}, 'wnorm': [38.89181533141453, 48.969011455495576, 48.384387106867564, 5.499164705104709], 'dwnorm': [1.964908892233342, 2.34406258147446, 3.6232730743411463, 0.2923880492290261]}, {'step': 33, 'wall': 0.45497343000170076, 'batch_loss': 0.7622297425832175, 'norm': 79.13387850875704, 'dnorm': 5.080420239088225, 'train': {'loss': 0.6837498256347537, 'aloss': 0.6837498256347537, 'aaloss': 0.6837498256347537, 'err': 0.4228571355342865, 'nd': 0, 'dfnorm': tensor(0.6091), 'fnorm': tensor(0.6091)}, 'test': {'loss': 0.7206577499953283, 'aloss': 0.7206577499953283, 'aaloss': 0.7206577499953283, 'err': 0.46857142448425293, 'nd': 0, 'dfnorm': tensor(0.6345), 'fnorm': tensor(0.6345)}, 'wnorm': [38.87280117575844, 48.91998087727263, 48.24371390370467, 5.4972602102789985], 'dwnorm': [2.092645638117804, 2.4398202584503, 3.89258667038923, 0.3013696153221727]}, {'step': 37, 'wall': 0.48272862000158057, 'batch_loss': 0.7691687537366554, 'norm': 78.96949121462578, 'dnorm': 5.481750323744054, 'train': {'loss': 0.6788727244920484, 'aloss': 0.6788727244920484, 'aaloss': 0.6788727244920484, 'err': 0.4171428680419922, 'nd': 0, 'dfnorm': tensor(0.5781), 'fnorm': tensor(0.5781)}, 'test': {'loss': 0.7124253701648476, 'aloss': 0.7124253701648476, 'aaloss': 0.7124253701648476, 'err': 0.4342857003211975, 'nd': 0, 'dfnorm': tensor(0.5943), 'fnorm': tensor(0.5943)}, 'wnorm': [38.84845090390392, 48.85512306470453, 48.0596796503872, 5.492685697840405], 'dwnorm': [2.256390548892783, 2.558744388803995, 4.249529383367334, 0.3156966140158763]}, {'step': 41, 'wall': 0.5128461420026724, 'batch_loss': 0.7815180582551822, 'norm': 78.80869224581677, 'dnorm': 5.876164764090828, 'train': {'loss': 0.6725866129199953, 'aloss': 0.6725866129199953, 'aaloss': 0.6725866129199953, 'err': 0.40857142210006714, 'nd': 0, 'dfnorm': tensor(0.5561), 'fnorm': tensor(0.5561)}, 'test': {'loss': 0.7097502247058284, 'aloss': 0.7097502247058284, 'aaloss': 0.7097502247058284, 'err': 0.4457142949104309, 'nd': 0, 'dfnorm': tensor(0.5702), 'fnorm': tensor(0.5702)}, 'wnorm': [38.82636188761797, 48.79144692708768, 47.87822051487481, 5.489708502666056], 'dwnorm': [2.4127103503033327, 2.678167417184309, 4.601016299605812, 0.32985706453738006]}, {'step': 46, 'wall': 0.5451432000008936, 'batch_loss': 0.7590749991410496, 'norm': 78.62479053202584, 'dnorm': 6.391908944968507, 'train': {'loss': 0.6672420068781149, 'aloss': 0.6672420068781149, 'aaloss': 0.6672420068781149, 'err': 0.39571428298950195, 'nd': 0, 'dfnorm': tensor(0.5574), 'fnorm': tensor(0.5574)}, 'test': {'loss': 0.71070224886308, 'aloss': 0.71070224886308, 'aaloss': 0.71070224886308, 'err': 0.44857141375541687, 'nd': 0, 'dfnorm': tensor(0.5718), 'fnorm': tensor(0.5718)}, 'wnorm': [38.81045705381345, 48.719226691105604, 47.660686940477134, 5.49843420795299], 'dwnorm': [2.6145488493072877, 2.851969772777422, 5.049134569009502, 0.3520526554390038]}, {'step': 51, 'wall': 0.5745207380023203, 'batch_loss': 0.7133129481615984, 'norm': 78.45142054109978, 'dnorm': 6.933580357678541, 'train': {'loss': 0.6670695917831386, 'aloss': 0.6670695917831386, 'aaloss': 0.6670695917831386, 'err': 0.38999998569488525, 'nd': 0, 'dfnorm': tensor(0.6131), 'fnorm': tensor(0.6131)}, 'test': {'loss': 0.721164827505564, 'aloss': 0.721164827505564, 'aaloss': 0.721164827505564, 'err': 0.4757142961025238, 'nd': 0, 'dfnorm': tensor(0.6328), 'fnorm': tensor(0.6328)}, 'wnorm': [38.79912970583058, 48.65280915295316, 47.44981878262891, 5.512111159768693], 'dwnorm': [2.8204594537057313, 3.0568118155157973, 5.5085801015803515, 0.3776019920612751]}, {'step': 57, 'wall': 0.6107961110028555, 'batch_loss': 0.7581896501194162, 'norm': 78.25244132192904, 'dnorm': 7.594362989722948, 'train': {'loss': 0.667855114811731, 'aloss': 0.667855114811731, 'aaloss': 0.667855114811731, 'err': 0.38999998569488525, 'nd': 0, 'dfnorm': tensor(0.6742), 'fnorm': tensor(0.6742)}, 'test': {'loss': 0.7324594513154102, 'aloss': 0.7324594513154102, 'aaloss': 0.7324594513154102, 'err': 0.47428572177886963, 'nd': 0, 'dfnorm': tensor(0.6964), 'fnorm': tensor(0.6964)}, 'wnorm': [38.78533217279978, 48.57968709315593, 47.20457206842083, 5.52797675651535], 'dwnorm': [3.062136276371027, 3.3205356486588573, 6.06440832769862, 0.41016271192198034]}, {'step': 63, 'wall': 0.6509139260015218, 'batch_loss': 0.7604618229494238, 'norm': 78.06332637515118, 'dnorm': 8.223704707735255, 'train': {'loss': 0.6579730513276255, 'aloss': 0.6579730513276255, 'aaloss': 0.6579730513276255, 'err': 0.3700000047683716, 'nd': 0, 'dfnorm': tensor(0.6014), 'fnorm': tensor(0.6014)}, 'test': {'loss': 0.7156332574651081, 'aloss': 0.7156332574651081, 'aaloss': 0.7156332574651081, 'err': 0.4514285624027252, 'nd': 0, 'dfnorm': tensor(0.6179), 'fnorm': tensor(0.6179)}, 'wnorm': [38.7747283643488, 48.50578574880702, 46.97346155899151, 5.542467283702249], 'dwnorm': [3.310234495440586, 3.569309922210034, 6.586111434731185, 0.43330135417790294]}, {'step': 70, 'wall': 0.6878893230023095, 'batch_loss': 0.7028940744810848, 'norm': 77.86104294122362, 'dnorm': 8.94188995430039, 'train': {'loss': 0.6539159543027504, 'aloss': 0.6539159543027504, 'aaloss': 0.6539159543027504, 'err': 0.36142855882644653, 'nd': 0, 'dfnorm': tensor(0.6572), 'fnorm': tensor(0.6572)}, 'test': {'loss': 0.7273361888329944, 'aloss': 0.7273361888329944, 'aaloss': 0.7273361888329944, 'err': 0.4471428692340851, 'nd': 0, 'dfnorm': tensor(0.6730), 'fnorm': tensor(0.6730)}, 'wnorm': [38.77164472984744, 48.43022562875953, 46.71406854180631, 5.566524490339921], 'dwnorm': [3.58758140572788, 3.859795543303427, 7.179723582671047, 0.4645207181096429]}, {'step': 77, 'wall': 0.7269311350028147, 'batch_loss': 0.7298591935141328, 'norm': 77.66465670306094, 'dnorm': 9.649092782052175, 'train': {'loss': 0.652646262914595, 'aloss': 0.652646262914595, 'aaloss': 0.652646262914595, 'err': 0.36571428179740906, 'nd': 0, 'dfnorm': tensor(0.7509), 'fnorm': tensor(0.7509)}, 'test': {'loss': 0.7459878529375445, 'aloss': 0.7459878529375445, 'aaloss': 0.7459878529375445, 'err': 0.4585714340209961, 'nd': 0, 'dfnorm': tensor(0.7648), 'fnorm': tensor(0.7648)}, 'wnorm': [38.77110398708672, 48.35379596336447, 46.46158883723772, 5.595758485843194], 'dwnorm': [3.853329171432913, 4.137997225948766, 7.7688871579304895, 0.509072736358593]}, {'step': 85, 'wall': 0.7681991230019776, 'batch_loss': 0.7202721920995065, 'norm': 77.44840312760718, 'dnorm': 10.41461507626257, 'train': {'loss': 0.6355340614655236, 'aloss': 0.6355340614655236, 'aaloss': 0.6355340614655236, 'err': 0.34142857789993286, 'nd': 0, 'dfnorm': tensor(0.6781), 'fnorm': tensor(0.6781)}, 'test': {'loss': 0.7324781954126146, 'aloss': 0.7324781954126146, 'aaloss': 0.7324781954126146, 'err': 0.46000000834465027, 'nd': 0, 'dfnorm': tensor(0.6948), 'fnorm': tensor(0.6948)}, 'wnorm': [38.763750271203854, 48.2741748016541, 46.18517835067309, 5.619926146286395], 'dwnorm': [4.176201619411774, 4.4233100390745825, 8.401413291626255, 0.5454724997654307]}, {'step': 94, 'wall': 0.8138389530031418, 'batch_loss': 0.685619804245221, 'norm': 77.24060565447735, 'dnorm': 11.319032450789237, 'train': {'loss': 0.633079091849094, 'aloss': 0.633079091849094, 'aaloss': 0.633079091849094, 'err': 0.35857143998146057, 'nd': 0, 'dfnorm': tensor(0.7870), 'fnorm': tensor(0.7870)}, 'test': {'loss': 0.7564105789158306, 'aloss': 0.7564105789158306, 'aaloss': 0.7564105789158306, 'err': 0.47999998927116394, 'nd': 0, 'dfnorm': tensor(0.8060), 'fnorm': tensor(0.8060)}, 'wnorm': [38.777002910403475, 48.19378036730739, 45.90204943067513, 5.668893430399842], 'dwnorm': [4.555458866910938, 4.816081246180255, 9.117519700946769, 0.6035227117002758]}, {'step': 104, 'wall': 0.8594770060008159, 'batch_loss': 0.7049189785056089, 'norm': 77.02610045903086, 'dnorm': 12.246517683834306, 'train': {'loss': 0.6301122946343338, 'aloss': 0.6301122946343338, 'aaloss': 0.6301122946343338, 'err': 0.35857143998146057, 'nd': 0, 'dfnorm': tensor(0.8429), 'fnorm': tensor(0.8429)}, 'test': {'loss': 0.7673252800243212, 'aloss': 0.7673252800243212, 'aaloss': 0.7673252800243212, 'err': 0.4828571379184723, 'nd': 0, 'dfnorm': tensor(0.8627), 'fnorm': tensor(0.8627)}, 'wnorm': [38.79124234831784, 48.105132341877336, 45.615283329302734, 5.715349722616355], 'dwnorm': [4.9522054453888265, 5.219452071923826, 9.850743711788185, 0.660612818378389]}, {'step': 115, 'wall': 0.9214123900019331, 'batch_loss': 0.6615021897993574, 'norm': 76.80537132292339, 'dnorm': 13.14726021291683, 'train': {'loss': 0.6188466466867932, 'aloss': 0.6188466466867932, 'aaloss': 0.6188466466867932, 'err': 0.334285706281662, 'nd': 0, 'dfnorm': tensor(0.7607), 'fnorm': tensor(0.7607)}, 'test': {'loss': 0.7397816940721553, 'aloss': 0.7397816940721553, 'aaloss': 0.7397816940721553, 'err': 0.47285714745521545, 'nd': 0, 'dfnorm': tensor(0.7751), 'fnorm': tensor(0.7751)}, 'wnorm': [38.80230191232402, 48.01623218531656, 45.32067398253005, 5.75908385545823], 'dwnorm': [5.331215795767328, 5.578140673061031, 10.585836756980791, 0.7040284322455613]}, {'step': 127, 'wall': 0.9843717300027492, 'batch_loss': 0.6628858584528062, 'norm': 76.60974545674414, 'dnorm': 14.15762167570301, 'train': {'loss': 0.6100704990177612, 'aloss': 0.6100704990177612, 'aaloss': 0.6100704990177612, 'err': 0.33571428060531616, 'nd': 0, 'dfnorm': tensor(0.8874), 'fnorm': tensor(0.8874)}, 'test': {'loss': 0.7688411205235913, 'aloss': 0.7688411205235913, 'aaloss': 0.7688411205235913, 'err': 0.49142858386039734, 'nd': 0, 'dfnorm': tensor(0.8972), 'fnorm': tensor(0.8972)}, 'wnorm': [38.837748777614514, 47.93873584316918, 45.0305974385825, 5.827982136937834], 'dwnorm': [5.738341975608148, 6.016578719409962, 11.3962324182342, 0.7723062458000607]}, {'step': 140, 'wall': 1.0637043660026393, 'batch_loss': 0.7183218404814739, 'norm': 76.42363898474771, 'dnorm': 15.167101137632743, 'train': {'loss': 0.595337476305223, 'aloss': 0.595337476305223, 'aaloss': 0.595337476305223, 'err': 0.31142857670783997, 'nd': 0, 'dfnorm': tensor(0.8199), 'fnorm': tensor(0.8199)}, 'test': {'loss': 0.7478925980235026, 'aloss': 0.7478925980235026, 'aaloss': 0.7478925980235026, 'err': 0.4699999988079071, 'nd': 0, 'dfnorm': tensor(0.8313), 'fnorm': tensor(0.8313)}, 'wnorm': [38.884355384293045, 47.86474110447086, 44.74003446300457, 5.911647009922338], 'dwnorm': [6.1107181120951255, 6.4808570006163, 12.209476131150602, 0.836336842337405]}, {'step': 154, 'wall': 1.164280463999603, 'batch_loss': 0.7182788122256631, 'norm': 76.25590161251071, 'dnorm': 16.24978687267341, 'train': {'loss': 0.582756960083828, 'aloss': 0.582756960083828, 'aaloss': 0.582756960083828, 'err': 0.2971428632736206, 'nd': 0, 'dfnorm': tensor(0.9009), 'fnorm': tensor(0.9009)}, 'test': {'loss': 0.759574751693709, 'aloss': 0.759574751693709, 'aaloss': 0.759574751693709, 'err': 0.46714285016059875, 'nd': 0, 'dfnorm': tensor(0.9055), 'fnorm': tensor(0.9055)}, 'wnorm': [38.94276160740042, 47.79359687844217, 44.463582426296796, 6.008607217742636], 'dwnorm': [6.562297030481736, 7.002484817254308, 13.03876064825083, 0.9302336066443407]}, {'step': 170, 'wall': 1.2428673350004829, 'batch_loss': 0.6640645264207367, 'norm': 76.08480954465844, 'dnorm': 17.394648378333436, 'train': {'loss': 0.5645315256662108, 'aloss': 0.5645315256662108, 'aaloss': 0.5645315256662108, 'err': 0.28999999165534973, 'nd': 0, 'dfnorm': tensor(0.9432), 'fnorm': tensor(0.9432)}, 'test': {'loss': 0.7707566160569342, 'aloss': 0.7707566160569342, 'aaloss': 0.7707566160569342, 'err': 0.4699999988079071, 'nd': 0, 'dfnorm': tensor(0.9500), 'fnorm': tensor(0.9500)}, 'wnorm': [39.00048817987595, 47.71748050755233, 44.186908478746055, 6.099999113206748], 'dwnorm': [7.053227238062564, 7.560643563391423, 13.907972359138226, 1.0106445465346876]}, {'step': 187, 'wall': 1.311371998002869, 'batch_loss': 0.5478244895319098, 'norm': 75.9569077358414, 'dnorm': 18.55580968923216, 'train': {'loss': 0.5414343383402892, 'aloss': 0.5414343383402892, 'aaloss': 0.5414343383402892, 'err': 0.25999999046325684, 'nd': 0, 'dfnorm': tensor(0.9104), 'fnorm': tensor(0.9104)}, 'test': {'loss': 0.7511878110091529, 'aloss': 0.7511878110091529, 'aaloss': 0.7511878110091529, 'err': 0.4628571569919586, 'nd': 0, 'dfnorm': tensor(0.9066), 'fnorm': tensor(0.9066)}, 'wnorm': [39.078005766987765, 47.68268137821042, 43.91686310129024, 6.218348556221843], 'dwnorm': [7.585400649554789, 8.156143758372826, 14.752883234492463, 1.1158199875416315]}, {'step': 206, 'wall': 1.3881885940027132, 'batch_loss': 0.5918002989251033, 'norm': 75.91407570547744, 'dnorm': 19.89311052644325, 'train': {'loss': 0.5301679592259, 'aloss': 0.5301679592259, 'aaloss': 0.5301679592259, 'err': 0.24857142567634583, 'nd': 0, 'dfnorm': tensor(1.0077), 'fnorm': tensor(1.0077)}, 'test': {'loss': 0.7612114082989369, 'aloss': 0.7612114082989369, 'aaloss': 0.7612114082989369, 'err': 0.4571428596973419, 'nd': 0, 'dfnorm': tensor(0.9767), 'fnorm': tensor(0.9767)}, 'wnorm': [39.20684842380242, 47.705180493197076, 43.675333814793234, 6.392575460469767], 'dwnorm': [8.2128138289647, 8.873057536285325, 15.694794139624378, 1.281384466358898]}, {'step': 227, 'wall': 1.470838447999995, 'batch_loss': 0.6081660338088988, 'norm': 75.95459648461646, 'dnorm': 21.340019562920464, 'train': {'loss': 0.5006504640342218, 'aloss': 0.5006504640342218, 'aaloss': 0.5006504640342218, 'err': 0.24571429193019867, 'nd': 0, 'dfnorm': tensor(1.1785), 'fnorm': tensor(1.1785)}, 'test': {'loss': 0.7944025329686073, 'aloss': 0.7944025329686073, 'aaloss': 0.7944025329686073, 'err': 0.4657142758369446, 'nd': 0, 'dfnorm': tensor(1.1238), 'fnorm': tensor(1.1238)}, 'wnorm': [39.3662182455585, 47.750731409696826, 43.51818889117297, 6.603887630346059], 'dwnorm': [8.966107734756942, 9.623778001875614, 16.682404802286168, 1.5034866799942637]}, {'step': 250, 'wall': 1.5597139290002815, 'batch_loss': 0.5228536693663899, 'norm': 76.04465233814261, 'dnorm': 22.68208465169492, 'train': {'loss': 0.49224827220916184, 'aloss': 0.49224827220916184, 'aaloss': 0.49224827220916184, 'err': 0.23428571224212646, 'nd': 0, 'dfnorm': tensor(1.3068), 'fnorm': tensor(1.3068)}, 'test': {'loss': 0.82505790108193, 'aloss': 0.82505790108193, 'aaloss': 0.82505790108193, 'err': 0.4642857015132904, 'nd': 0, 'dfnorm': tensor(1.2284), 'fnorm': tensor(1.2284)}, 'wnorm': [39.54948613111076, 47.83335438889724, 43.38598773404185, 6.802819451385329], 'dwnorm': [9.669780925845345, 10.31428762646029, 17.5966351547305, 1.7236540340562567]}, {'step': 275, 'wall': 1.64802740000232, 'batch_loss': 0.5754146336152469, 'norm': 76.21576127861661, 'dnorm': 24.112186463450996, 'train': {'loss': 0.4612344124953873, 'aloss': 0.4612344124953873, 'aaloss': 0.4612344124953873, 'err': 0.21857142448425293, 'nd': 0, 'dfnorm': tensor(1.4815), 'fnorm': tensor(1.4815)}, 'test': {'loss': 0.8625394420341208, 'aloss': 0.8625394420341208, 'aaloss': 0.8625394420341208, 'err': 0.46857142448425293, 'nd': 0, 'dfnorm': tensor(1.4200), 'fnorm': tensor(1.4200)}, 'wnorm': [39.76906523618814, 47.945446549122, 43.31849434116621, 7.0510771138014725], 'dwnorm': [10.47280237176788, 11.06433300702144, 18.521448563973212, 1.9915301800889482]}, {'step': 303, 'wall': 1.7409464869997464, 'batch_loss': 0.4758392410839181, 'norm': 76.41828088332238, 'dnorm': 25.518150817464917, 'train': {'loss': 0.4207951959304837, 'aloss': 0.4207951959304837, 'aaloss': 0.4207951959304837, 'err': 0.1728571355342865, 'nd': 0, 'dfnorm': tensor(1.5317), 'fnorm': tensor(1.5317)}, 'test': {'loss': 0.8463563692836302, 'aloss': 0.8463563692836302, 'aaloss': 0.8463563692836302, 'err': 0.47285714745521545, 'nd': 0, 'dfnorm': tensor(1.4298), 'fnorm': tensor(1.4298)}, 'wnorm': [40.00379726648498, 48.08455935280119, 43.2610496599652, 7.294475500677943], 'dwnorm': [11.235966575494613, 11.797445406946563, 19.44434197982424, 2.2516003952112698]}, {'step': 333, 'wall': 1.8493905770010315, 'batch_loss': 0.5368742299439633, 'norm': 76.67907738749152, 'dnorm': 26.929368608811632, 'train': {'loss': 0.3856467572866719, 'aloss': 0.3856467572866719, 'aaloss': 0.3856467572866719, 'err': 0.14428570866584778, 'nd': 0, 'dfnorm': tensor(1.6160), 'fnorm': tensor(1.6160)}, 'test': {'loss': 0.8457667726370539, 'aloss': 0.8457667726370539, 'aaloss': 0.8457667726370539, 'err': 0.4714285731315613, 'nd': 0, 'dfnorm': tensor(1.4966), 'fnorm': tensor(1.4966)}, 'wnorm': [40.26577472772262, 48.236133207664984, 43.26060452087936, 7.562923197048709], 'dwnorm': [11.996304142680003, 12.48811595678401, 20.39405412524334, 2.542254869777455]}, {'step': 366, 'wall': 1.9722300640023605, 'batch_loss': 0.4584379652742062, 'norm': 77.02070112473783, 'dnorm': 28.41322029400317, 'train': {'loss': 0.34972951418985654, 'aloss': 0.34972951418985654, 'aaloss': 0.34972951418985654, 'err': 0.12428571283817291, 'nd': 0, 'dfnorm': tensor(1.9742), 'fnorm': tensor(1.9742)}, 'test': {'loss': 0.9243257822430917, 'aloss': 0.9243257822430917, 'aaloss': 0.9243257822430917, 'err': 0.43857142329216003, 'nd': 0, 'dfnorm': tensor(1.8514), 'fnorm': tensor(1.8514)}, 'wnorm': [40.57021902993051, 48.44203243145623, 43.28729751946027, 7.906028263492205], 'dwnorm': [12.780563940374769, 13.217176572045924, 21.391336879496244, 2.8923016327519937]}, {'step': 402, 'wall': 2.1083424589996866, 'batch_loss': 0.39203538347597355, 'norm': 77.46288704014741, 'dnorm': 29.921640667766965, 'train': {'loss': 0.3198417661378717, 'aloss': 0.3198417661378717, 'aaloss': 0.3198417661378717, 'err': 0.11142857372760773, 'nd': 0, 'dfnorm': tensor(2.1636), 'fnorm': tensor(2.1636)}, 'test': {'loss': 0.9548616192519095, 'aloss': 0.9548616192519095, 'aaloss': 0.9548616192519095, 'err': 0.44428572058677673, 'nd': 0, 'dfnorm': tensor(1.9941), 'fnorm': tensor(1.9941)}, 'wnorm': [40.915839129230086, 48.680578158386524, 43.41391832421446, 8.264505438507028], 'dwnorm': [13.586505320861777, 13.991671916390779, 22.376447523909675, 3.2734131888307174]}, {'step': 442, 'wall': 2.2613711610028986, 'batch_loss': 0.4363462451010025, 'norm': 77.9276318172571, 'dnorm': 31.414864579767247, 'train': {'loss': 0.2953972268612722, 'aloss': 0.2953972268612722, 'aaloss': 0.2953972268612722, 'err': 0.10857142508029938, 'nd': 0, 'dfnorm': tensor(2.4722), 'fnorm': tensor(2.4722)}, 'test': {'loss': 1.0458252552304987, 'aloss': 1.0458252552304987, 'aaloss': 1.0458252552304987, 'err': 0.44428572058677673, 'nd': 0, 'dfnorm': tensor(2.2871), 'fnorm': tensor(2.2871)}, 'wnorm': [41.2760199620844, 48.93292793115996, 43.55412066817823, 8.599337825915532], 'dwnorm': [14.366769787268545, 14.799982658889755, 23.339960791238532, 3.6099700009431572]}, {'step': 486, 'wall': 2.4271102150014485, 'batch_loss': 0.31504452808636285, 'norm': 78.42830877832662, 'dnorm': 32.943154098135174, 'train': {'loss': 0.2321553664938518, 'aloss': 0.2321553664938518, 'aaloss': 0.2321553664938518, 'err': 0.0657142847776413, 'nd': 0, 'dfnorm': tensor(2.4728), 'fnorm': tensor(2.4728)}, 'test': {'loss': 1.012828895769518, 'aloss': 1.012828895769518, 'aaloss': 1.012828895769518, 'err': 0.45428571105003357, 'nd': 0, 'dfnorm': tensor(2.3160), 'fnorm': tensor(2.3160)}, 'wnorm': [41.66862931509479, 49.20679112475074, 43.69333741120539, 8.958709849858725], 'dwnorm': [15.29471305948676, 15.660137520232757, 24.210182371774387, 3.987464984516338]}, {'step': 534, 'wall': 2.592738424002164, 'batch_loss': 0.21669903055108988, 'norm': 78.99232084461146, 'dnorm': 34.52364414748573, 'train': {'loss': 0.1995731275393991, 'aloss': 0.1995731275393991, 'aaloss': 0.1995731275393991, 'err': 0.05285714194178581, 'nd': 0, 'dfnorm': tensor(2.9148), 'fnorm': tensor(2.9148)}, 'test': {'loss': 1.1069728064837525, 'aloss': 1.1069728064837525, 'aaloss': 1.1069728064837525, 'err': 0.4414285719394684, 'nd': 0, 'dfnorm': tensor(2.6575), 'fnorm': tensor(2.6575)}, 'wnorm': [42.080813832354885, 49.47874414683968, 43.907887490854584, 9.408117609876866], 'dwnorm': [16.117211135365686, 16.518010427027477, 25.199123110168618, 4.454947816300744]}, {'step': 586, 'wall': 2.7652668629998516, 'batch_loss': 0.35501552633850325, 'norm': 79.56546339033774, 'dnorm': 36.0348545895115, 'train': {'loss': 0.17905839478807617, 'aloss': 0.17905839478807617, 'aaloss': 0.17905839478807617, 'err': 0.055714286863803864, 'nd': 0, 'dfnorm': tensor(3.4145), 'fnorm': tensor(3.4145)}, 'test': {'loss': 1.2522605053321, 'aloss': 1.2522605053321, 'aaloss': 1.2522605053321, 'err': 0.4457142949104309, 'nd': 0, 'dfnorm': tensor(3.1309), 'fnorm': tensor(3.1309)}, 'wnorm': [42.48970359523112, 49.73514356975429, 44.156583012677345, 9.852860190539218], 'dwnorm': [16.928390151407946, 17.294105780987348, 26.15167287047766, 4.911557163035607]}, {'step': 643, 'wall': 2.9599334330014244, 'batch_loss': 0.18068911677681848, 'norm': 80.18846829961451, 'dnorm': 37.59633646148881, 'train': {'loss': 0.11717280749694357, 'aloss': 0.11717280749694357, 'aaloss': 0.11717280749694357, 'err': 0.02142857201397419, 'nd': 0, 'dfnorm': tensor(3.7192), 'fnorm': tensor(3.7192)}, 'test': {'loss': 1.2927190667587602, 'aloss': 1.2927190667587602, 'aaloss': 1.2927190667587602, 'err': 0.4399999976158142, 'nd': 0, 'dfnorm': tensor(3.4347), 'fnorm': tensor(3.4347)}, 'wnorm': [42.91345924724925, 50.035224368947986, 44.4194094438248, 10.324952248059114], 'dwnorm': [17.762348477008445, 18.11731732324243, 27.113486280236344, 5.40295797577813]}, {'step': 706, 'wall': 3.1671406220011704, 'batch_loss': 0.14815786227699662, 'norm': 80.7173197844794, 'dnorm': 39.057271077205414, 'train': {'loss': 0.0895285625530748, 'aloss': 0.0895285625530748, 'aaloss': 0.0895285625530748, 'err': 0.014285714365541935, 'nd': 0, 'dfnorm': tensor(4.2530), 'fnorm': tensor(4.2530)}, 'test': {'loss': 1.4208426915922963, 'aloss': 1.4208426915922963, 'aaloss': 1.4208426915922963, 'err': 0.4300000071525574, 'nd': 0, 'dfnorm': tensor(3.8950), 'fnorm': tensor(3.8950)}, 'wnorm': [43.293023193357314, 50.24324800667636, 44.649423536403106, 10.824185596852137], 'dwnorm': [18.462054155770392, 18.79820183339283, 28.112501682791233, 5.920808016905844]}, {'step': 775, 'wall': 3.413691170000675, 'batch_loss': 0.1545031352593384, 'norm': 81.08836742263686, 'dnorm': 40.39601677232563, 'train': {'loss': 0.060249098374331705, 'aloss': 0.060249098374331705, 'aaloss': 0.060249098374331705, 'err': 0.0028571428265422583, 'nd': 0, 'dfnorm': tensor(4.7126), 'fnorm': tensor(4.7126)}, 'test': {'loss': 1.524895193873128, 'aloss': 1.524895193873128, 'aaloss': 1.524895193873128, 'err': 0.4342857003211975, 'nd': 0, 'dfnorm': tensor(4.3133), 'fnorm': tensor(4.3133)}, 'wnorm': [43.607280411764336, 50.33395755140142, 44.79867206351034, 11.27008513244274], 'dwnorm': [19.087012728900778, 19.480655409793066, 29.000282835859316, 6.381729418917454]}, {'step': 850, 'wall': 3.693137977999868, 'batch_loss': 0.1285677878501164, 'norm': 81.31559645261265, 'dnorm': 41.61148388906, 'train': {'loss': 0.03929979694582518, 'aloss': 0.03929979694582518, 'aaloss': 0.03929979694582518, 'err': 0.0028571428265422583, 'nd': 0, 'dfnorm': tensor(5.1837), 'fnorm': tensor(5.1837)}, 'test': {'loss': 1.620995477688346, 'aloss': 1.620995477688346, 'aaloss': 1.620995477688346, 'err': 0.4228571355342865, 'nd': 0, 'dfnorm': tensor(4.7842), 'fnorm': tensor(4.7842)}, 'wnorm': [43.84489212283439, 50.35012164272117, 44.84639477393547, 11.6935384989435], 'dwnorm': [19.57834713108522, 20.076503978552587, 29.86309458191426, 6.823088754854834]}, {'step': 932, 'wall': 3.96409339100137, 'batch_loss': 0.13215087129328645, 'norm': 81.36889877182429, 'dnorm': 42.664397687451924, 'train': {'loss': 0.02460458511727229, 'aloss': 0.02460458511727229, 'aaloss': 0.02460458511727229, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(5.5705), 'fnorm': tensor(5.5705)}, 'test': {'loss': 1.6974864242819203, 'aloss': 1.6974864242819203, 'aaloss': 1.6974864242819203, 'err': 0.4285714328289032, 'nd': 0, 'dfnorm': tensor(5.0603), 'fnorm': tensor(5.0603)}, 'wnorm': [44.01622307868776, 50.25342900161333, 44.776288125676054, 12.08221659649185], 'dwnorm': [20.025254325716883, 20.57460543707083, 30.60254345524438, 7.227948103981937]}, {'step': 1021, 'wall': 4.28072320100182, 'batch_loss': 0.1145220850137014, 'norm': 81.1567961052863, 'dnorm': 43.57172229239899, 'train': {'loss': 0.020613143885856628, 'aloss': 0.020613143885856628, 'aaloss': 0.020613143885856628, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(6.1118), 'fnorm': tensor(6.1118)}, 'test': {'loss': 1.8649139886145152, 'aloss': 1.8649139886145152, 'aaloss': 1.8649139886145152, 'err': 0.42571428418159485, 'nd': 0, 'dfnorm': tensor(5.6175), 'fnorm': tensor(5.6175)}, 'wnorm': [44.0675058406459, 49.99796957268866, 44.527272287467504, 12.425763570362705], 'dwnorm': [20.369279274633822, 20.96327070765016, 31.288401835889392, 7.586744568803801]}, {'step': 1119, 'wall': 4.592753575001552, 'batch_loss': 0.10685824505501397, 'norm': 80.78476997703343, 'dnorm': 44.41596124263318, 'train': {'loss': 0.013832965171222969, 'aloss': 0.013832965171222969, 'aaloss': 0.013832965171222969, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(6.3058), 'fnorm': tensor(6.3058)}, 'test': {'loss': 1.8842234504137716, 'aloss': 1.8842234504137716, 'aaloss': 1.8842234504137716, 'err': 0.41857144236564636, 'nd': 0, 'dfnorm': tensor(5.7828), 'fnorm': tensor(5.7828)}, 'wnorm': [44.0612381418891, 49.64714603079671, 44.15400410988513, 12.74755979941073], 'dwnorm': [20.672851366483872, 21.366793363327243, 31.91002649686692, 7.921508376382849]}, {'step': 1225, 'wall': 4.907101419001265, 'batch_loss': 0.10547682936424797, 'norm': 80.18344244663642, 'dnorm': 45.13260268721766, 'train': {'loss': 0.012088036158392991, 'aloss': 0.012088036158392991, 'aaloss': 0.012088036158392991, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(6.7289), 'fnorm': tensor(6.7289)}, 'test': {'loss': 1.9939456046122852, 'aloss': 1.9939456046122852, 'aaloss': 1.9939456046122852, 'err': 0.4228571355342865, 'nd': 0, 'dfnorm': tensor(6.1598), 'fnorm': tensor(6.1598)}, 'wnorm': [43.95800908506899, 49.16504614969744, 43.60614277589493, 13.047598195012647], 'dwnorm': [20.901432724290732, 21.719804656718033, 32.44080037333146, 8.23321818537104]}, {'step': 1341, 'wall': 5.289769392002199, 'batch_loss': 0.10159745426213705, 'norm': 79.35091486345462, 'dnorm': 45.78708467661561, 'train': {'loss': 0.008639408459559924, 'aloss': 0.008639408459559924, 'aaloss': 0.008639408459559924, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(6.8686), 'fnorm': tensor(6.8686)}, 'test': {'loss': 1.9876126020847684, 'aloss': 1.9876126020847684, 'aaloss': 1.9876126020847684, 'err': 0.41999998688697815, 'nd': 0, 'dfnorm': tensor(6.2637), 'fnorm': tensor(6.2637)}, 'wnorm': [43.76207353644088, 48.53162316958457, 42.89863448621886, 13.319803231462673], 'dwnorm': [21.049346206716912, 22.081755489419653, 32.936559983739464, 8.51710486724459]}, {'step': 1467, 'wall': 5.669849068002804, 'batch_loss': 0.09882821346322714, 'norm': 78.31694279718836, 'dnorm': 46.452175344757556, 'train': {'loss': 0.007763842938256118, 'aloss': 0.007763842938256118, 'aaloss': 0.007763842938256118, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.0578), 'fnorm': tensor(7.0578)}, 'test': {'loss': 2.0130208370153535, 'aloss': 2.0130208370153535, 'aaloss': 2.0130208370153535, 'err': 0.4057142734527588, 'nd': 0, 'dfnorm': tensor(6.4470), 'fnorm': tensor(6.4470)}, 'wnorm': [43.48861368934498, 47.76472672894277, 42.04063916017419, 13.591151574180387], 'dwnorm': [21.193097287898876, 22.445282778696555, 33.44787725338208, 8.799196722127183]}, {'step': 1604, 'wall': 6.095425695002632, 'batch_loss': 0.09831805990174015, 'norm': 77.08817949572087, 'dnorm': 47.1711053813159, 'train': {'loss': 0.006410355368795233, 'aloss': 0.006410355368795233, 'aaloss': 0.006410355368795233, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.2179), 'fnorm': tensor(7.2179)}, 'test': {'loss': 2.029499566848199, 'aloss': 2.029499566848199, 'aaloss': 2.029499566848199, 'err': 0.404285728931427, 'nd': 0, 'dfnorm': tensor(6.5396), 'fnorm': tensor(6.5396)}, 'wnorm': [43.130702088363726, 46.86212746537711, 41.05001638415251, 13.86649810261947], 'dwnorm': [21.31530120641081, 22.885067408541342, 33.994508400271705, 9.084649455353949]}, {'step': 1753, 'wall': 6.7576819120004075, 'batch_loss': 0.09223180380207878, 'norm': 75.62574735424026, 'dnorm': 47.94970879507665, 'train': {'loss': 0.0059954121491563915, 'aloss': 0.0059954121491563915, 'aaloss': 0.0059954121491563915, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.2818), 'fnorm': tensor(7.2818)}, 'test': {'loss': 2.018529553087187, 'aloss': 2.018529553087187, 'aaloss': 2.018529553087187, 'err': 0.404285728931427, 'nd': 0, 'dfnorm': tensor(6.6046), 'fnorm': tensor(6.6046)}, 'wnorm': [42.67019771467068, 45.79949130924983, 39.897207614153096, 14.144031528465007], 'dwnorm': [21.42342641668499, 23.417964246609642, 34.56720222617081, 9.372575552849225]}, {'step': 1914, 'wall': 7.453363420001551, 'batch_loss': 0.0903072283196079, 'norm': 73.98505492439848, 'dnorm': 48.87204021360307, 'train': {'loss': 0.005388924891187465, 'aloss': 0.005388924891187465, 'aaloss': 0.005388924891187465, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.3456), 'fnorm': tensor(7.3456)}, 'test': {'loss': 1.9730840111669854, 'aloss': 1.9730840111669854, 'aaloss': 1.9730840111669854, 'err': 0.3985714316368103, 'nd': 0, 'dfnorm': tensor(6.6293), 'fnorm': tensor(6.6293)}, 'wnorm': [42.11590496600402, 44.60011185465374, 38.63924851325103, 14.440489245702134], 'dwnorm': [21.547029007156503, 24.0914922334994, 35.22430709340737, 9.67989561246497]}, {'step': 2089, 'wall': 8.240868843000499, 'batch_loss': 0.08406215356916936, 'norm': 72.16378498392353, 'dnorm': 49.98285438433586, 'train': {'loss': 0.005468387732845861, 'aloss': 0.005468387732845861, 'aaloss': 0.005468387732845861, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.4723), 'fnorm': tensor(7.4723)}, 'test': {'loss': 1.9260794552074474, 'aloss': 1.9260794552074474, 'aaloss': 1.9260794552074474, 'err': 0.4028571546077728, 'nd': 0, 'dfnorm': tensor(6.6377), 'fnorm': tensor(6.6377)}, 'wnorm': [41.47216156742956, 43.257757530534306, 37.26705025529703, 14.765666906133914], 'dwnorm': [21.734521385734325, 24.931449303051803, 35.977799771375494, 10.016832540889094]}, {'step': 2278, 'wall': 9.134962839001673, 'batch_loss': 0.081262026759836, 'norm': 70.09426387663366, 'dnorm': 51.23334502378171, 'train': {'loss': 0.0046782509070031775, 'aloss': 0.0046782509070031775, 'aaloss': 0.0046782509070031775, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.4689), 'fnorm': tensor(7.4689)}, 'test': {'loss': 1.8044658628146373, 'aloss': 1.8044658628146373, 'aaloss': 1.8044658628146373, 'err': 0.37857142090797424, 'nd': 0, 'dfnorm': tensor(6.5566), 'fnorm': tensor(6.5566)}, 'wnorm': [40.68050867662126, 41.75223793637372, 35.73685725208601, 15.100508881177287], 'dwnorm': [21.93823394073906, 25.89468008772313, 36.81997942992158, 10.364355118648158]}, {'step': 2482, 'wall': 9.808749866002472, 'batch_loss': 0.07724433413917038, 'norm': 67.76893092001607, 'dnorm': 52.618728184488866, 'train': {'loss': 0.004561063738399868, 'aloss': 0.004561063738399868, 'aaloss': 0.004561063738399868, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.4947), 'fnorm': tensor(7.4947)}, 'test': {'loss': 1.6530062871442208, 'aloss': 1.6530062871442208, 'aaloss': 1.6530062871442208, 'err': 0.36142855882644653, 'nd': 0, 'dfnorm': tensor(6.4766), 'fnorm': tensor(6.4766)}, 'wnorm': [39.73099024966565, 40.05562587502393, 34.07014075396876, 15.446617311059477], 'dwnorm': [22.197637018117366, 27.035515705408297, 37.68057833864456, 10.723786781705755]}, {'step': 2702, 'wall': 10.503764826000406, 'batch_loss': 0.07335468149561009, 'norm': 65.21020277606976, 'dnorm': 54.26251960033409, 'train': {'loss': 0.003540596519309051, 'aloss': 0.003540596519309051, 'aaloss': 0.003540596519309051, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.4401), 'fnorm': tensor(7.4401)}, 'test': {'loss': 1.411404736402542, 'aloss': 1.411404736402542, 'aaloss': 1.411404736402542, 'err': 0.33000001311302185, 'nd': 0, 'dfnorm': tensor(6.2743), 'fnorm': tensor(6.2743)}, 'wnorm': [38.60583178047967, 38.20770487829807, 32.27555860638795, 15.801500739435426], 'dwnorm': [22.598110183757505, 28.425820235687173, 38.62535880187008, 11.093618582817468]}, {'step': 2939, 'wall': 11.247154464999767, 'batch_loss': 0.06738228309962961, 'norm': 62.30815454181304, 'dnorm': 56.1403975898098, 'train': {'loss': 0.003391501245713719, 'aloss': 0.003391501245713719, 'aaloss': 0.003391501245713719, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.4040), 'fnorm': tensor(7.4040)}, 'test': {'loss': 1.1093008201342773, 'aloss': 1.1093008201342773, 'aaloss': 1.1093008201342773, 'err': 0.28857141733169556, 'nd': 0, 'dfnorm': tensor(6.0740), 'fnorm': tensor(6.0740)}, 'wnorm': [37.242026047860136, 36.14150445442937, 30.28579170615023, 16.142876608381236], 'dwnorm': [23.12255402143606, 30.068902224158695, 39.629927448433634, 11.450505533288853]}, {'step': 3194, 'wall': 12.05899100500028, 'batch_loss': 0.060974882584372087, 'norm': 59.098795340149735, 'dnorm': 58.295563892599915, 'train': {'loss': 0.002743447627267942, 'aloss': 0.002743447627267942, 'aaloss': 0.002743447627267942, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.2895), 'fnorm': tensor(7.2895)}, 'test': {'loss': 0.7226073119507938, 'aloss': 0.7226073119507938, 'aaloss': 0.7226073119507938, 'err': 0.23428571224212646, 'nd': 0, 'dfnorm': tensor(5.8172), 'fnorm': tensor(5.8172)}, 'wnorm': [35.619883921821916, 33.89221887657334, 28.142089191887155, 16.467329468977162], 'dwnorm': [23.87371037940257, 31.91026475448168, 40.73185361690111, 11.790291377040742]}, {'step': 3468, 'wall': 12.842814079001982, 'batch_loss': 0.054465857884333106, 'norm': 55.52739645691518, 'dnorm': 60.81097035973067, 'train': {'loss': 0.002402253151210792, 'aloss': 0.002402253151210792, 'aaloss': 0.002402253151210792, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.1528), 'fnorm': tensor(7.1528)}, 'test': {'loss': 0.36025319062623873, 'aloss': 0.36025319062623873, 'aaloss': 0.36025319062623873, 'err': 0.1371428519487381, 'nd': 0, 'dfnorm': tensor(5.6637), 'fnorm': tensor(5.6637)}, 'wnorm': [33.66635303114393, 31.43349345589554, 25.84754407945865, 16.749556316382634], 'dwnorm': [24.901337731861272, 34.03123697247901, 41.95859138501761, 12.086697897902656]}, {'step': 3762, 'wall': 13.721872858001007, 'batch_loss': 0.047628799061622616, 'norm': 51.63872386405449, 'dnorm': 63.65684936564993, 'train': {'loss': 0.0021065746663399545, 'aloss': 0.0021065746663399545, 'aaloss': 0.0021065746663399545, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.1309), 'fnorm': tensor(7.1309)}, 'test': {'loss': 0.12051070921279995, 'aloss': 0.12051070921279995, 'aaloss': 0.12051070921279995, 'err': 0.05285714194178581, 'nd': 0, 'dfnorm': tensor(5.7739), 'fnorm': tensor(5.7739)}, 'wnorm': [31.362447930681373, 28.79865307724961, 23.470536873643972, 16.971188443281747], 'dwnorm': [26.269363943997345, 36.31971013189997, 43.31796311759174, 12.32245152927989]}, {'step': 4076, 'wall': 14.620640911001828, 'batch_loss': 0.040389335640145914, 'norm': 47.47519116711932, 'dnorm': 66.65641275432094, 'train': {'loss': 0.0016594106546112552, 'aloss': 0.0016594106546112552, 'aaloss': 0.0016594106546112552, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.0000), 'fnorm': tensor(7.0000)}, 'test': {'loss': 0.025096316131819837, 'aloss': 0.025096316131819837, 'aaloss': 0.025096316131819837, 'err': 0.0057142856530845165, 'nd': 0, 'dfnorm': tensor(5.9856), 'fnorm': tensor(5.9856)}, 'wnorm': [28.695252843980345, 26.01651167077246, 21.074636712126058, 17.102494963953976], 'dwnorm': [27.999164980719318, 38.572399928568444, 44.708538274222185, 12.46831890315945]}, {'step': 4411, 'wall': 15.598111388000689, 'batch_loss': 0.03288439904922902, 'norm': 43.32708215753397, 'dnorm': 69.79507257638599, 'train': {'loss': 0.0013859034631514835, 'aloss': 0.0013859034631514835, 'aaloss': 0.0013859034631514835, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(6.9077), 'fnorm': tensor(6.9077)}, 'test': {'loss': 0.005370976085315501, 'aloss': 0.005370976085315501, 'aaloss': 0.005370976085315501, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(6.2891), 'fnorm': tensor(6.2891)}, 'wnorm': [25.81109058338455, 23.228551010715933, 18.89185313217527, 17.16130911972983], 'dwnorm': [30.12447750558021, 40.81468029388774, 46.04973500767611, 12.546169357032461]}, {'step': 4768, 'wall': 16.697310963001655, 'batch_loss': 0.026379340756337626, 'norm': 39.472198663683656, 'dnorm': 72.94427310948207, 'train': {'loss': 0.0012670088288834506, 'aloss': 0.0012670088288834506, 'aaloss': 0.0012670088288834506, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(6.9152), 'fnorm': tensor(6.9152)}, 'test': {'loss': 0.0022697121463858467, 'aloss': 0.0022697121463858467, 'aaloss': 0.0022697121463858467, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(6.5699), 'fnorm': tensor(6.5699)}, 'wnorm': [22.903546503068466, 20.598392945931256, 17.05511724504825, 17.156915992671294], 'dwnorm': [32.56745210386361, 42.93350029291983, 47.27590000642842, 12.569026683204836]}, {'step': 5148, 'wall': 17.835729529000673, 'batch_loss': 0.02088685193148557, 'norm': 36.249253784152295, 'dnorm': 75.93001124947918, 'train': {'loss': 0.001126430742550871, 'aloss': 0.001126430742550871, 'aaloss': 0.001126430742550871, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(6.8955), 'fnorm': tensor(6.8955)}, 'test': {'loss': 0.0013925869018219705, 'aloss': 0.0013925869018219705, 'aaloss': 0.0013925869018219705, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(6.7380), 'fnorm': tensor(6.7380)}, 'wnorm': [20.293248307656324, 18.344553180315007, 15.646473592664053, 17.103597231017776], 'dwnorm': [35.04081621926108, 44.939566195727735, 48.291137492353585, 12.553269269355587]}, {'step': 5551, 'wall': 19.092375620002713, 'batch_loss': 0.0161146545759533, 'norm': 33.86654705761968, 'dnorm': 78.62473404676147, 'train': {'loss': 0.001055953553763863, 'aloss': 0.001055953553763863, 'aaloss': 0.001055953553763863, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(6.9361), 'fnorm': tensor(6.9361)}, 'test': {'loss': 0.001169122696226859, 'aloss': 0.001169122696226859, 'aaloss': 0.001169122696226859, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(6.8573), 'fnorm': tensor(6.8573)}, 'wnorm': [18.257466576835245, 16.657368450144688, 14.64052551010247, 17.00250035832192], 'dwnorm': [37.31065382991172, 46.74455265015868, 49.15131186374492, 12.495969744151896]}, {'step': 5977, 'wall': 20.957389991999662, 'batch_loss': 0.013071619680500053, 'norm': 32.32123581364108, 'dnorm': 80.76784437284172, 'train': {'loss': 0.0010071899609596755, 'aloss': 0.0010071899609596755, 'aaloss': 0.0010071899609596755, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(6.9952), 'fnorm': tensor(6.9952)}, 'test': {'loss': 0.0011306783834558172, 'aloss': 0.0011306783834558172, 'aaloss': 0.0011306783834558172, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(6.9232), 'fnorm': tensor(6.9232)}, 'wnorm': [16.930461720369287, 15.612030575806484, 13.96204751286452, 16.828414978088553], 'dwnorm': [38.96781222037767, 48.24249616778852, 49.88369400578003, 12.372590710988245]}, {'step': 6427, 'wall': 22.75505672500003, 'batch_loss': 0.010683262051319423, 'norm': 31.41884430280525, 'dnorm': 82.14899272630687, 'train': {'loss': 0.000930611567608334, 'aloss': 0.000930611567608334, 'aaloss': 0.000930611567608334, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.0322), 'fnorm': tensor(7.0322)}, 'test': {'loss': 0.0010060106074421605, 'aloss': 0.0010060106074421605, 'aaloss': 0.0010060106074421605, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(6.9712), 'fnorm': tensor(6.9712)}, 'wnorm': [16.19334551941643, 15.089400920645701, 13.553540822503884, 16.585886346859073], 'dwnorm': [39.90485549156372, 49.31610470591317, 50.36739572156426, 12.192930561455325]}, {'step': 6902, 'wall': 24.17109687500124, 'batch_loss': 0.009263419761674616, 'norm': 30.89748106401177, 'dnorm': 82.91407930205686, 'train': {'loss': 0.0008709013633734017, 'aloss': 0.0008709013633734017, 'aaloss': 0.0008709013633734017, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.0956), 'fnorm': tensor(7.0956)}, 'test': {'loss': 0.0009465759127031738, 'aloss': 0.0009465759127031738, 'aaloss': 0.0009465759127031738, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.0279), 'fnorm': tensor(7.0279)}, 'wnorm': [15.76882279813153, 14.912744670627479, 13.341667106754516, 16.291890087103003], 'dwnorm': [40.2474543316203, 49.9989295195096, 50.71052387819813, 11.968873588632182]}, {'step': 7401, 'wall': 25.62506174200098, 'batch_loss': 0.008232096534258944, 'norm': 30.406042138198686, 'dnorm': 83.2602141216058, 'train': {'loss': 0.0008262619767330602, 'aloss': 0.0008262619767330602, 'aaloss': 0.0008262619767330602, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.1454), 'fnorm': tensor(7.1454)}, 'test': {'loss': 0.0008913560119880825, 'aloss': 0.0008913560119880825, 'aaloss': 0.0008913560119880825, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.0843), 'fnorm': tensor(7.0843)}, 'wnorm': [15.303767258800386, 14.826436124062699, 13.22252112120531, 15.926951918601558], 'dwnorm': [40.267576824223376, 50.35837197224144, 50.9460152713657, 11.71867640635021]}, {'step': 7924, 'wall': 27.145550933000777, 'batch_loss': 0.007131827298362456, 'norm': 29.987288511906016, 'dnorm': 83.43314479258072, 'train': {'loss': 0.0008008076297023945, 'aloss': 0.0008008076297023945, 'aaloss': 0.0008008076297023945, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.2281), 'fnorm': tensor(7.2281)}, 'test': {'loss': 0.000887185909491907, 'aloss': 0.000887185909491907, 'aaloss': 0.000887185909491907, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.1534), 'fnorm': tensor(7.1534)}, 'wnorm': [14.940244376505463, 14.911423755656045, 13.095967877441133, 15.48253460971555], 'dwnorm': [40.243642655145706, 50.60982679984395, 51.05396642604905, 11.45487058087058]}, {'step': 8472, 'wall': 28.714634938001836, 'batch_loss': 0.006558543006217981, 'norm': 29.485624242863846, 'dnorm': 83.45458518536978, 'train': {'loss': 0.0007703093779203396, 'aloss': 0.0007703093779203396, 'aaloss': 0.0007703093779203396, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.2815), 'fnorm': tensor(7.2815)}, 'test': {'loss': 0.0008737176501383137, 'aloss': 0.0008737176501383137, 'aaloss': 0.0008737176501383137, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.1981), 'fnorm': tensor(7.1981)}, 'wnorm': [14.697043037099794, 14.69776183041456, 13.03166225461254, 15.05863479377039], 'dwnorm': [40.226534079580865, 50.67659678982921, 51.07350424798178, 11.361575105413886]}, {'step': 9044, 'wall': 30.410059857000306, 'batch_loss': 0.006030357825506743, 'norm': 29.037144873651474, 'dnorm': 83.51220401244152, 'train': {'loss': 0.0007481135555858818, 'aloss': 0.0007481135555858818, 'aaloss': 0.0007481135555858818, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.3469), 'fnorm': tensor(7.3469)}, 'test': {'loss': 0.0008615000762193592, 'aloss': 0.0008615000762193592, 'aaloss': 0.0008615000762193592, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.2437), 'fnorm': tensor(7.2437)}, 'wnorm': [14.465084056089628, 14.324810294103184, 13.051928603755758, 14.828624236816243], 'dwnorm': [40.278490957239754, 50.70548966971369, 51.058821127459105, 11.63638642296189]}, {'step': 9640, 'wall': 32.14731873200071, 'batch_loss': 0.0051474246183460005, 'norm': 28.6719981629486, 'dnorm': 83.6087781083084, 'train': {'loss': 0.0006983444486166438, 'aloss': 0.0006983444486166438, 'aaloss': 0.0006983444486166438, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.3493), 'fnorm': tensor(7.3493)}, 'test': {'loss': 0.0007777651094558019, 'aloss': 0.0007777651094558019, 'aaloss': 0.0007777651094558019, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.2731), 'fnorm': tensor(7.2731)}, 'wnorm': [14.296173972360116, 13.98695149736319, 13.054147941477735, 14.643111141787887], 'dwnorm': [40.36475024883542, 50.77078123018841, 51.07932993080728, 11.709310286253592]}], 'train': {'f0': tensor([-2.7096e-01,  2.3099e-01,  5.0008e-01, -7.3009e-02, -1.4374e-02,\n",
            "         1.1677e-01,  3.2493e-02,  1.3398e-01, -4.0508e-01,  7.7487e-01,\n",
            "        -8.7579e-01,  4.5015e-01,  6.6423e-01,  2.0192e-01, -1.9213e-01,\n",
            "         1.8380e-01,  1.4813e-01, -2.7370e-01, -3.7086e-01, -2.1609e-01,\n",
            "         2.3692e-01, -4.3312e-01, -3.7920e-01, -1.8682e-01,  2.0681e-01,\n",
            "        -1.3476e-01, -3.4338e-02, -1.3159e-01,  1.9889e-01,  2.2256e-01,\n",
            "         2.1974e-02, -3.4879e-01, -9.4232e-01, -9.8504e-02,  1.7340e-01,\n",
            "         4.0750e-01, -1.6730e-01,  6.6644e-02, -1.8854e-01, -8.7707e-02,\n",
            "        -1.8185e-01,  1.6720e-01, -6.4156e-01, -1.0020e+00,  5.5581e-02,\n",
            "         3.9905e-01, -2.3888e-01, -2.6330e-01, -4.0460e-01,  1.6682e-01,\n",
            "        -8.1499e-01,  2.6128e-01, -3.6526e-01,  9.1307e-02, -3.4514e-02,\n",
            "        -8.8303e-01, -3.2758e-01, -1.3268e-01, -4.0304e-01, -5.1298e-02,\n",
            "        -4.0396e-01,  4.4266e-02,  3.9242e-01, -2.6253e-01,  1.8356e-01,\n",
            "         4.5963e-01,  3.3351e-02, -2.2502e-03, -6.0249e-01,  1.2030e-01,\n",
            "        -4.2826e-01, -6.1953e-02, -4.2026e-01, -3.5197e-02, -3.2652e-02,\n",
            "         1.2522e-01, -4.7813e-02, -3.1883e-01, -1.5769e-01,  4.8746e-02,\n",
            "         3.8892e-01,  1.0804e-01,  1.4097e-01, -3.6479e-01, -2.4750e-01,\n",
            "        -5.0062e-01,  1.7979e-02, -3.7852e-01, -4.9705e-01,  2.1636e-01,\n",
            "         4.0222e-03, -1.5504e-02,  2.1685e-01, -1.6644e-01,  4.7340e-02,\n",
            "        -7.5213e-01, -1.1284e+00, -3.0020e-01, -1.0334e-01,  3.2802e-01,\n",
            "        -7.0232e-01,  1.0043e-02, -9.7486e-01, -1.5496e-01,  2.2758e-01,\n",
            "        -1.0580e-01,  3.8899e-01, -1.3318e-01, -5.7223e-01, -1.5726e-01,\n",
            "         3.9969e-01,  4.1963e-01, -7.7402e-02,  4.7905e-01,  3.6938e-01,\n",
            "        -1.9369e-01,  1.7652e-01, -4.5762e-01, -7.3882e-01,  3.1494e-01,\n",
            "        -1.8708e-01, -1.1310e-01, -2.8615e-01, -3.0103e-01, -3.0977e-01,\n",
            "         5.4253e-02, -7.8798e-03,  2.0440e-01, -1.2653e-01, -1.3774e-01,\n",
            "        -3.6817e-03, -5.6216e-01, -1.4510e-02,  5.5995e-01,  3.3972e-02,\n",
            "         6.5494e-02,  1.9058e-02,  7.7276e-01, -4.8218e-02,  7.5424e-02,\n",
            "        -3.6596e-01,  3.7508e-01, -6.3881e-01, -1.6810e-01, -2.0985e-01,\n",
            "        -1.3979e-01, -1.9963e-03,  1.9860e-01, -7.4280e-02,  1.3410e-01,\n",
            "         2.0429e-01,  2.1027e-01, -2.5227e-01, -3.0027e-01,  1.4026e-01,\n",
            "         1.2007e-01,  2.4298e-02,  6.6316e-01, -2.0741e-01,  2.1020e-01,\n",
            "        -5.1518e-01, -2.3641e-01,  4.7548e-01,  5.2720e-02, -6.9356e-02,\n",
            "        -3.9404e-01,  4.8157e-02, -4.2185e-01, -2.1014e-01, -5.4240e-01,\n",
            "        -4.0492e-02, -4.2379e-02, -4.3156e-01,  1.7432e-01, -1.3993e+00,\n",
            "        -3.7020e-01, -1.4069e-04, -1.8719e-01, -1.0588e-01,  5.7121e-02,\n",
            "         1.4399e-01,  8.4463e-01,  3.9819e-01,  2.6343e-01, -5.4925e-01,\n",
            "        -2.6778e-01, -1.3609e-01,  6.1647e-02, -7.7807e-01, -4.2308e-01,\n",
            "        -3.3648e-02,  7.8487e-02, -4.8077e-01,  4.3272e-02,  6.8325e-01,\n",
            "        -2.8138e-01, -3.1990e-01,  4.9571e-02,  6.7999e-02,  5.4647e-01,\n",
            "         8.4649e-01, -3.4401e-01, -1.7057e-01,  5.0317e-01,  8.8408e-02,\n",
            "         3.1447e-01, -2.6108e-01,  2.7484e-01, -6.9685e-01, -4.8057e-02,\n",
            "        -2.8451e-01, -1.1739e-02, -6.2607e-03,  1.8459e-01, -5.5810e-01,\n",
            "        -4.0250e-01,  4.1951e-01,  5.2498e-01, -7.3018e-02,  3.1130e-01,\n",
            "         1.3223e-01, -2.9714e-01,  2.0111e-01,  4.1965e-01, -4.9109e-02,\n",
            "        -2.1170e-01, -4.1584e-01, -9.2511e-02,  1.9898e-02, -7.8529e-01,\n",
            "         6.9351e-01, -3.2219e-01,  9.9395e-02, -2.5400e-02, -5.5036e-01,\n",
            "        -4.6575e-01, -2.8512e-02,  9.0942e-02, -2.6776e-01,  3.6171e-01,\n",
            "        -4.7086e-01,  2.3002e-01, -3.9045e-01, -5.2807e-02, -1.8395e-01,\n",
            "        -1.8049e-01, -2.6759e-01, -3.3598e-01, -2.6152e-01,  1.3121e-01,\n",
            "        -6.4915e-01, -4.6866e-01,  7.1144e-02, -7.0085e-01,  1.6830e-01,\n",
            "         5.2116e-02, -3.9035e-01, -3.2252e-01,  7.6234e-01, -4.2805e-02,\n",
            "         2.8372e-01, -6.9643e-01, -2.2675e-02, -2.0086e-01,  1.2661e-01,\n",
            "         3.3578e-01,  1.8451e-01, -4.3202e-01, -2.2017e-02, -5.8066e-01,\n",
            "        -2.4254e-01, -2.0904e-01, -5.7893e-02,  1.1201e-01,  1.9366e-01,\n",
            "         4.9050e-01, -2.6802e-01,  2.2830e-01, -3.7355e-01,  3.2264e-01,\n",
            "         9.2748e-03, -1.0516e-01, -7.9746e-02, -2.9605e-01, -4.0128e-01,\n",
            "        -2.3279e-01,  5.4016e-01, -1.5793e-01,  4.5374e-01, -3.2198e-01,\n",
            "        -3.2477e-01,  6.9648e-02,  8.9473e-02, -3.1196e-01, -2.9812e-01,\n",
            "         2.4729e-01, -3.4088e-01, -7.1329e-02, -3.3755e-01, -4.6274e-01,\n",
            "        -6.8042e-01,  3.5986e-01, -6.5825e-01, -1.3931e-01, -1.2211e-01,\n",
            "         1.2837e-01, -1.0282e-01,  2.8820e-01,  8.8461e-01, -8.6948e-01,\n",
            "         2.7952e-01, -9.9109e-03, -2.3782e-01,  3.7238e-01, -8.7926e-02,\n",
            "        -1.9727e-02,  2.6532e-01, -2.8403e-01,  2.7055e-01, -6.8989e-01,\n",
            "         5.5012e-01, -1.1343e-01,  4.8376e-01, -1.5122e-01,  1.8439e-01,\n",
            "        -7.7291e-01, -1.7094e-01,  4.6784e-01, -2.5483e-01, -1.6729e-01,\n",
            "        -7.7282e-03,  7.7657e-01,  3.9579e-02,  3.3141e-01,  4.1138e-01,\n",
            "         1.7202e-01,  3.6839e-01,  1.0906e-01,  2.2940e-01,  4.0528e-01,\n",
            "        -2.5494e-01, -6.6937e-01, -4.0907e-01,  6.3174e-01, -2.0270e-01,\n",
            "        -7.8157e-01, -7.9276e-02, -2.2327e-01, -4.6275e-01, -1.6348e+00,\n",
            "        -5.5408e-01, -5.5288e-01, -5.5336e-01,  3.8827e-01, -1.5661e-02,\n",
            "         3.8268e-01, -8.2286e-01,  4.4319e-01, -4.2092e-01, -2.5527e-03,\n",
            "         7.0637e-02, -5.3627e-01, -6.6174e-01,  6.3106e-01, -4.3398e-01,\n",
            "        -5.6597e-01,  7.2906e-02, -4.8580e-01,  7.6270e-02,  4.8486e-02,\n",
            "        -1.3804e-01,  3.4960e-01, -3.5531e-01,  1.6573e-01, -1.2618e-01,\n",
            "         2.9463e-02, -3.4262e-01,  2.0531e-01, -3.1769e-01,  1.5654e-01,\n",
            "        -1.3593e-01, -4.3632e-01,  1.1439e-01,  4.6038e-01, -5.9098e-02,\n",
            "         1.2464e-01, -7.3437e-01,  2.1650e-01, -2.5021e-01,  2.5046e-02,\n",
            "        -3.3203e-01, -7.6041e-01,  3.1859e-01, -3.8435e-01,  1.0042e-02,\n",
            "         1.3670e-01, -6.4913e-01, -2.9313e-01,  1.3241e-01, -2.8510e-01,\n",
            "        -3.9743e-01,  1.3172e-01,  2.4770e-01,  1.0917e-01, -5.4705e-01,\n",
            "        -2.9215e-02, -2.6418e-01, -5.8970e-01,  1.6889e-01, -1.0446e-01,\n",
            "         3.1196e-02, -4.2123e-02, -1.6191e-01,  1.2813e-01,  4.1097e-01,\n",
            "        -5.9808e-01, -1.4760e-01, -2.1894e-02, -3.5325e-01, -3.9107e-01,\n",
            "        -1.1096e-01,  3.5361e-01, -2.7886e-01, -4.6276e-01, -6.0517e-02,\n",
            "        -3.6719e-01,  4.9600e-01, -2.6078e-01, -3.1807e-01,  2.6261e-01,\n",
            "         1.3984e-01,  9.2052e-02,  7.3088e-02, -7.3477e-01,  2.8353e-01,\n",
            "        -4.9400e-01,  6.7131e-02, -5.1826e-01, -5.5498e-03,  1.5901e-02,\n",
            "         4.6330e-01, -1.7126e-01, -6.3789e-01,  2.6677e-03, -4.9133e-01,\n",
            "         3.4442e-01,  7.3530e-01, -4.5076e-01,  5.4578e-02,  2.9476e-01,\n",
            "        -1.6176e-01,  2.9040e-01,  5.4685e-01, -8.4160e-01,  1.2665e-01,\n",
            "        -3.6294e-01, -3.0572e-01,  9.0803e-02, -2.8492e-01, -2.5427e-02,\n",
            "        -1.0585e-01, -2.0950e-01, -3.3682e-01,  3.2469e-01, -7.9777e-01,\n",
            "         9.0568e-01, -4.3167e-02, -1.9661e-01, -1.2958e-01, -4.8881e-01,\n",
            "        -5.3785e-01,  3.5672e-01, -3.7230e-01, -2.1683e-01, -4.2896e-01,\n",
            "         6.5144e-02, -3.4614e-01,  4.4434e-01,  3.3856e-01, -6.1635e-01,\n",
            "         1.7396e-01, -1.0955e-01,  1.0427e-01, -6.9302e-01,  3.8765e-02,\n",
            "        -3.6010e-01, -9.8612e-01,  3.5992e-01,  2.8731e-02,  8.1105e-02,\n",
            "        -1.8050e-01, -2.3492e-02,  3.0158e-01,  4.5014e-01, -4.6540e-01,\n",
            "        -7.2995e-02,  2.1100e-02,  5.1688e-01,  8.4820e-01,  6.1085e-01,\n",
            "        -4.5985e-01, -2.6793e-01, -6.7818e-01, -2.5275e-01,  2.6392e-02,\n",
            "        -6.8131e-01, -6.5953e-01, -3.4038e-01,  3.7472e-01,  4.6909e-01,\n",
            "        -4.3166e-02,  2.9537e-01, -6.3343e-03, -1.7662e-02,  4.0431e-01,\n",
            "        -1.9551e-02, -1.6916e-01, -1.0203e-01,  5.0396e-01, -2.8843e-01,\n",
            "         3.4727e-01, -6.5643e-01, -1.9024e-01,  2.7015e-01, -2.6282e-01,\n",
            "        -1.8262e-01, -6.2548e-02, -7.4558e-01,  4.0383e-01, -3.4319e-01,\n",
            "         2.2132e-01,  2.5936e-01,  8.7330e-02, -4.9384e-01, -3.5407e-01,\n",
            "         1.3904e-01, -3.1019e-01, -4.6876e-01, -1.2593e-01,  2.7194e-01,\n",
            "        -6.8850e-02,  6.2495e-01,  3.4161e-01, -6.2931e-01, -1.9703e-01,\n",
            "        -6.9631e-02,  7.0143e-01, -3.0521e-01, -6.4923e-01,  8.9639e-02,\n",
            "         2.9365e-01, -5.1402e-01,  2.5179e-01, -1.8586e-01,  3.3283e-03,\n",
            "        -4.6720e-01,  1.1434e-01, -2.9183e-01,  4.3665e-01, -5.8659e-01,\n",
            "         1.7988e-01,  7.4615e-01, -2.3371e-02, -2.9347e-01, -1.5090e-01,\n",
            "        -6.0541e-01, -1.9580e-01, -2.1374e-01, -3.0555e-01,  4.6431e-01,\n",
            "        -7.8785e-02,  8.6470e-02, -8.4710e-02, -6.8205e-01,  4.3582e-01,\n",
            "        -2.5719e-01, -1.3291e-01,  6.3345e-01,  1.3823e-01, -2.9120e-01,\n",
            "         1.9070e-01, -2.6699e-01, -1.7214e-02, -2.9773e-02,  1.7056e-01,\n",
            "        -8.7382e-01, -1.2670e-01,  2.1501e-02, -7.3499e-01, -9.8425e-02,\n",
            "        -3.8413e-01,  4.2314e-01,  5.9687e-01,  9.5213e-02, -5.0083e-01,\n",
            "        -2.6262e-01, -4.7301e-01,  1.4339e-01,  2.1627e-01, -1.0838e-01,\n",
            "         1.2736e-01, -4.2153e-01, -4.5448e-01,  2.8528e-01, -2.7297e-01,\n",
            "         7.5422e-02,  2.1254e-01,  5.1496e-01,  3.7192e-01, -7.5760e-01,\n",
            "         4.1436e-01, -2.8851e-01, -1.0941e-01,  4.9311e-01, -3.0617e-02,\n",
            "        -2.5931e-01, -1.1549e-02, -3.2411e-01,  5.5620e-01, -1.1882e+00,\n",
            "        -1.9597e-02, -1.8909e-01, -1.2181e-02, -1.4075e-01, -1.3380e-01,\n",
            "         1.6895e-01, -3.4279e-01, -2.0620e-01,  4.5704e-01,  8.0125e-01,\n",
            "         6.1745e-02, -1.0688e-01, -5.8187e-01, -8.8608e-01, -2.0136e-01,\n",
            "        -3.2362e-01, -2.3472e-01, -3.5694e-02,  2.9881e-01, -4.1714e-01,\n",
            "         1.0627e-01, -1.7226e-01, -2.8030e-01, -4.6479e-01,  3.4187e-01,\n",
            "         4.7686e-01, -4.7222e-01, -4.1031e-01, -2.2917e-01,  3.8655e-01,\n",
            "        -2.5340e-02, -4.1335e-01,  1.0746e-02, -3.2306e-02,  3.0254e-01,\n",
            "         1.3110e-01, -5.9567e-01,  6.9304e-02,  3.2804e-01,  4.2225e-01,\n",
            "         3.1118e-01, -4.1600e-01,  7.7110e-02,  1.4547e-01, -5.2323e-01,\n",
            "        -3.2569e-01,  1.0975e-01, -3.9253e-01, -2.4287e-01, -6.8731e-01,\n",
            "         2.1829e-01,  2.0999e-02, -4.3813e-01, -1.7875e-01, -1.1399e-01,\n",
            "         3.2736e-02, -3.7910e-02, -8.9423e-01, -2.0454e-01, -5.0472e-01,\n",
            "         3.3762e-01, -3.9245e-01, -2.1673e-01, -9.1291e-01, -2.6281e-01,\n",
            "        -5.6043e-02, -4.6978e-01,  3.2223e-01,  3.3954e-01,  2.0067e-01,\n",
            "         1.4167e-03, -1.5451e-02, -6.8896e-02, -3.9489e-02,  8.7972e-02,\n",
            "         1.4640e-01,  3.3376e-01, -2.1956e-01, -6.3566e-01,  4.4449e-03]), 'outputs': tensor([-7.4058, -7.6169,  7.3037,  7.2971, -7.0761,  7.2643,  7.3215,  7.3263,\n",
            "         7.3222,  7.3124, -7.9842, -7.0462,  7.3609, -7.3044,  6.4156,  7.2938,\n",
            "        -6.9389,  7.2538, -7.9501, -7.1665, -7.9115, -7.9651,  7.2869, -7.6828,\n",
            "        -7.6446,  7.2604, -7.9284,  7.3526, -7.2099,  7.3064, -7.8153,  7.2754,\n",
            "         6.4309,  7.2977, -7.9640, -6.9894,  6.0914,  7.0285, -7.2841, -7.6693,\n",
            "        -7.5205, -7.3955,  7.2573,  7.2672, -7.3994,  7.3114,  7.3726, -7.6981,\n",
            "         7.2684,  7.3494, -8.1203, -7.6497, -7.7209,  7.2691,  7.2854,  6.9685,\n",
            "        -8.0770,  6.6007,  7.1524,  7.3031, -7.6503, -7.6407, -7.3262,  7.2667,\n",
            "         7.2807, -6.9555, -7.9241,  6.9980,  6.4014, -7.7559, -7.7321,  7.3185,\n",
            "         7.3072,  7.3341,  6.8521, -7.8648, -7.6102,  7.3401, -7.0598, -7.4713,\n",
            "         7.2778,  7.2884,  7.2883, -7.5005, -8.0633, -7.9284, -7.1125,  7.2831,\n",
            "        -7.7655,  7.3179, -7.4793,  7.3086, -7.8307,  7.3305,  7.2839,  6.1787,\n",
            "         7.2328, -7.5639,  7.2878,  7.2964, -6.9214, -7.2834,  5.6117, -7.8123,\n",
            "        -6.9983, -7.3004,  7.3710,  7.3217, -7.8144,  7.3346,  7.3013,  7.3101,\n",
            "         7.3560,  7.3225, -7.1416,  7.3215, -7.0766,  7.2605, -7.5184, -7.1260,\n",
            "         7.2392, -6.9304, -7.9034, -8.0572,  7.2755,  7.2971,  7.2932,  7.3169,\n",
            "        -7.6857,  6.7902,  7.2956, -7.2224,  6.4130,  7.2951,  7.2758,  7.3661,\n",
            "        -7.5673, -7.0979,  7.2839, -7.0002,  7.2466,  7.3022,  7.2565,  7.2817,\n",
            "         6.9379, -8.0415, -7.4852,  7.2966, -7.9248, -7.3965, -7.4801,  7.2302,\n",
            "         7.2656, -7.5436,  7.2561, -7.4333, -6.9290,  7.2918,  7.3305, -6.8991,\n",
            "        -8.0183, -6.9951, -7.3089, -7.7005, -8.0416, -7.0992,  6.7190, -7.3639,\n",
            "        -7.6794, -7.9544, -8.1018, -7.5639, -7.8381, -6.8777,  7.2905,  7.2260,\n",
            "         7.2896, -7.6476,  7.2863, -7.3486,  6.8455,  7.1243,  7.2938, -7.2050,\n",
            "        -7.8723, -7.8134, -7.5041,  7.3034, -7.8495,  7.3593, -8.0174,  7.3361,\n",
            "        -7.9318, -7.0347, -7.2113, -7.8427,  7.2773,  7.2493, -7.7879,  7.3563,\n",
            "         7.3006, -7.0348, -7.9270,  7.2999,  6.6560, -7.8155,  7.2725,  7.2901,\n",
            "         7.3443, -7.6622, -8.0763, -7.5633,  7.2664,  7.3447, -7.8295, -8.1803,\n",
            "        -7.0609, -7.0697,  7.2967, -7.6408, -7.8052,  5.9896,  7.2658,  7.2746,\n",
            "        -8.1000, -8.0597, -7.7909,  7.3369,  7.3247,  7.2858, -7.0985,  7.3028,\n",
            "        -6.9967,  7.2549,  7.3150, -7.9195,  7.3539,  7.2621, -7.6303,  7.3466,\n",
            "         6.1899, -7.2621, -8.0212,  7.2548,  7.3060, -7.5973,  7.2930, -7.8521,\n",
            "        -8.0262, -7.6225,  7.2936,  7.3410, -7.9349, -7.9511,  7.2630,  7.3714,\n",
            "        -7.1158,  7.2715,  7.2644, -8.0370,  7.2859,  7.2733,  7.2584,  6.9782,\n",
            "         7.2845,  7.3601,  7.2941, -7.5732, -7.5898,  6.3962,  6.3134,  7.2626,\n",
            "        -7.2195,  7.2416, -7.6026,  7.3032,  7.2535, -7.7254,  7.0287,  7.3667,\n",
            "        -7.5775,  7.3581, -7.5720, -7.8783, -8.0175, -7.0247, -7.4818, -8.1973,\n",
            "         7.3072, -8.0567,  7.2894,  7.3499,  7.3522,  6.7684, -7.7474,  7.5426,\n",
            "        -7.9823,  7.2399, -8.3128,  7.2768,  7.2680,  7.3690,  7.3176, -7.3166,\n",
            "        -7.0556, -7.6793, -7.9603, -7.0103,  7.2802, -7.7517,  7.3699, -7.0580,\n",
            "         6.5406,  7.2729, -7.1475,  7.2565,  7.3016,  6.7361, -7.3108,  7.2650,\n",
            "        -7.1696, -8.2022, -6.8197,  7.2932, -7.7710, -7.1421,  7.2875, -7.1567,\n",
            "        -8.3777, -7.7259, -7.6565,  7.2855,  7.2725, -7.2081,  7.3061, -7.5705,\n",
            "        -8.0046, -7.1656,  7.2714, -7.1822, -7.1163, -7.7203, -7.8619, -7.3827,\n",
            "        -7.3081, -7.9749, -7.5471, -7.1086,  7.3176,  6.3285, -7.8609, -7.4040,\n",
            "         7.3275, -7.6911,  6.5692, -7.0325, -7.8203,  7.3686, -7.9874, -7.8503,\n",
            "         7.2619,  7.3158, -7.8274, -7.0863,  7.3187,  5.6267,  7.2644,  7.3197,\n",
            "         7.2938,  7.2848,  7.0009,  7.2574,  7.4244, -6.7463, -7.7118,  6.6032,\n",
            "         7.3025,  7.3459,  6.2867, -7.0064, -7.4758, -8.1122, -7.8454, -6.9978,\n",
            "        -7.7699, -7.9479, -7.3231,  6.6846,  7.2655, -7.7146, -7.9380,  7.2704,\n",
            "        -7.6703,  7.2512,  7.2847,  7.3112, -8.1049, -7.6753, -7.6090, -7.7747,\n",
            "        -7.5505, -7.7854,  7.3584, -7.1143,  7.3418, -8.0914,  7.3013, -7.9395,\n",
            "        -7.7468,  7.2833, -7.0425, -7.0384, -7.7415, -7.7552, -7.5662,  7.3167,\n",
            "         7.2933, -7.8822, -7.9858,  7.3237, -7.7456,  7.3370, -7.7155, -8.1428,\n",
            "         7.2538,  7.3302, -7.7962,  7.2715,  7.2866,  7.2991, -6.9289, -7.4895,\n",
            "        -7.4421, -7.8876,  7.1824, -7.0800, -6.9706, -7.5550,  6.2771, -7.4279,\n",
            "         7.2645, -7.9067, -7.8166, -7.5111,  6.7890, -8.1669, -7.4371, -7.5120,\n",
            "         7.2762, -7.1210, -7.4183,  7.3620, -7.1444,  6.3717, -7.7877, -8.2377,\n",
            "        -7.6379, -7.3311,  7.1891,  7.2847, -7.9934, -7.9830, -7.5336, -6.8807,\n",
            "         7.2476, -7.1436,  7.3120, -7.7303, -7.8435, -8.1592, -7.5766, -7.3028,\n",
            "        -8.2737,  6.4431,  7.3339, -7.9187, -7.0717,  7.2927, -7.7781, -8.0875,\n",
            "         7.3038,  7.3483,  7.2950,  6.3355,  7.3797,  7.2661, -7.5202,  7.3714,\n",
            "        -7.3910,  7.3637,  6.3501,  7.2816, -7.4345, -7.1682,  7.3353,  7.2749,\n",
            "         7.2959, -6.8093,  7.3510,  7.3018,  7.3492, -6.8890,  5.9368,  7.2831,\n",
            "        -7.4503, -8.1186, -7.9377, -7.5214, -7.5440,  7.3311,  7.4097, -7.4908,\n",
            "        -7.0635, -6.9592, -7.6881, -7.9003,  6.7216,  7.2584, -7.2890,  6.3357,\n",
            "         7.2869,  7.2775,  7.2371, -7.1761, -7.7476, -7.5887,  7.3123, -7.9595,\n",
            "         7.3651, -7.1145, -7.3480,  7.3468, -7.7682, -7.6375,  6.5451,  7.3143,\n",
            "         7.3502, -7.9600, -7.7933, -7.6554,  7.3649, -7.1012, -7.4824, -7.9751,\n",
            "         7.2430,  6.8745, -6.9537, -7.2736,  7.3163, -7.7153, -7.4139, -7.5834,\n",
            "        -7.7831, -7.5911,  7.2267,  7.3341,  7.0340, -7.6553,  7.3287,  6.0402,\n",
            "        -7.4847, -7.1392, -7.6560,  7.2699,  7.2692,  7.2499,  7.3208, -7.8574,\n",
            "        -7.9095, -7.5443,  7.2897, -7.8746,  7.2840, -7.9478,  7.2682, -7.3084,\n",
            "        -7.5104, -7.7421, -7.8933,  7.2726,  7.3414, -7.6500, -7.5478,  7.3259,\n",
            "        -7.1819,  7.2628, -7.9993, -7.7296,  6.3571, -7.4206,  6.5666,  7.2988,\n",
            "         7.3576, -7.3280,  7.3673,  6.6786,  6.4862,  7.3107, -6.9358, -7.0832,\n",
            "        -7.2032,  7.3177, -7.6442,  7.2576,  7.3002,  7.2771, -7.0488, -7.6273,\n",
            "         7.3733, -7.8189, -8.0577,  7.2938, -7.7081,  7.3042, -7.5012,  7.3235,\n",
            "         7.3394, -8.1317,  7.3551,  7.2618, -7.7724,  7.2899, -7.5716, -7.8258,\n",
            "        -7.4045,  7.2874, -7.8928,  7.1268,  7.3520, -7.5565,  7.3503, -8.0238,\n",
            "         6.9045,  7.2689,  7.2557, -7.5767,  7.3345, -6.8117,  7.3157, -6.8805,\n",
            "        -7.7761, -7.1469,  7.2428,  6.3561, -7.3070,  7.3001, -7.3248, -7.6476,\n",
            "         6.7989,  7.2650, -7.7083, -7.7320,  7.2907,  7.3376,  6.6794,  7.3578,\n",
            "         7.2657, -7.0659,  7.2723,  7.3280, -7.8537, -7.4521,  7.2329,  7.2886,\n",
            "        -7.8584,  7.2318,  7.3147,  7.3030, -7.8762, -7.5923, -8.2725, -7.7529,\n",
            "         7.2673,  7.3110, -7.1244,  7.2861, -7.1480, -8.0180, -7.5706, -7.5839,\n",
            "        -7.5913,  7.3011, -7.8827, -7.8870, -7.6466,  7.3326,  7.2405,  7.3545,\n",
            "        -7.4232, -6.9729,  7.3150, -7.3429,  7.3305, -6.9804,  7.3306, -7.6560,\n",
            "        -7.8360,  6.4002,  6.9579, -7.3844]), 'labels': tensor([-1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1.,\n",
            "         1.,  1., -1.,  1., -1., -1., -1., -1.,  1., -1., -1.,  1., -1.,  1.,\n",
            "        -1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1., -1., -1., -1.,\n",
            "         1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.,\n",
            "        -1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,\n",
            "        -1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.,  1.,  1.,  1., -1.,\n",
            "        -1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1.,\n",
            "         1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
            "         1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,\n",
            "         1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1.,\n",
            "         1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,  1., -1.,\n",
            "         1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1.,  1., -1.,\n",
            "        -1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,\n",
            "         1., -1., -1., -1., -1.,  1., -1.,  1., -1.,  1., -1., -1., -1., -1.,\n",
            "         1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1.,\n",
            "        -1., -1.,  1.,  1., -1., -1., -1., -1.,  1., -1., -1.,  1.,  1.,  1.,\n",
            "        -1., -1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.,\n",
            "        -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,\n",
            "        -1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
            "         1., -1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.,\n",
            "        -1.,  1., -1., -1., -1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
            "        -1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1., -1.,\n",
            "         1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.,\n",
            "        -1.,  1., -1., -1.,  1., -1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,\n",
            "        -1., -1.,  1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1., -1., -1.,\n",
            "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,\n",
            "         1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,\n",
            "        -1.,  1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1., -1.,  1., -1.,\n",
            "         1., -1., -1.,  1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1.,  1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1., -1., -1.,\n",
            "         1., -1., -1., -1.,  1., -1.,  1., -1., -1., -1.,  1., -1., -1., -1.,\n",
            "         1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1.,  1.,  1., -1., -1.,\n",
            "        -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,\n",
            "         1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
            "        -1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
            "        -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1., -1.,  1., -1., -1.,  1.,\n",
            "        -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
            "        -1., -1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,\n",
            "         1., -1., -1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,  1., -1., -1.,\n",
            "         1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,\n",
            "        -1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,\n",
            "         1., -1.,  1.,  1., -1.,  1., -1., -1., -1.,  1., -1.,  1.,  1., -1.,\n",
            "         1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
            "         1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1., -1., -1.,\n",
            "         1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1., -1., -1., -1.,  1.,\n",
            "         1.,  1., -1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,  1., -1.])}, 'test': {'f0': tensor([-2.6121e-01, -1.2141e+00, -2.0840e-01,  5.7613e-02,  6.5418e-01,\n",
            "         2.3492e-01,  1.5035e-02,  1.3897e-01,  4.2583e-01, -1.7254e-01,\n",
            "        -1.4175e-01, -2.4568e-01,  1.9365e-01, -6.7213e-01,  1.9302e-01,\n",
            "        -6.4369e-03, -2.4607e-01, -7.8501e-01,  7.8510e-02, -1.1420e-01,\n",
            "        -5.5384e-01, -1.1965e-01,  1.8571e-01, -8.2553e-02,  1.2102e-01,\n",
            "         1.8465e-02, -1.3738e-01, -1.5545e-01, -3.9382e-01, -1.4172e-01,\n",
            "        -1.0797e-02, -1.6578e+00, -2.5643e-01, -1.8404e-01,  5.2055e-01,\n",
            "         4.4488e-01, -9.9046e-02, -1.0045e-01, -4.1933e-01,  1.6821e-01,\n",
            "        -8.6631e-02, -3.0814e-01,  7.7847e-03, -4.4733e-01,  2.2132e-01,\n",
            "         2.5097e-01,  4.4969e-01,  7.0390e-02, -2.0360e-01,  8.8455e-02,\n",
            "        -5.9029e-02,  2.4318e-01, -6.4054e-02, -4.2118e-01, -3.3589e-01,\n",
            "        -1.2887e-01, -4.3008e-01, -8.3450e-01, -5.2827e-02,  1.6572e-01,\n",
            "        -4.1219e-01, -1.0460e-01, -2.4700e-01, -5.8928e-02, -2.2429e-02,\n",
            "        -2.1895e-01,  2.4322e-01, -1.1794e-01, -6.0856e-01,  1.6078e-01,\n",
            "        -3.1738e-01, -9.2573e-01, -7.9142e-02,  1.0027e-01,  3.0023e-01,\n",
            "        -8.3316e-02, -3.0660e-01,  2.7160e-01, -3.0683e-01,  4.4892e-01,\n",
            "         5.0578e-02, -2.3626e-01,  6.8468e-02, -4.1363e-01,  2.8673e-01,\n",
            "        -2.3427e-02,  6.4445e-03, -1.3642e-01, -1.8426e-01, -3.8873e-01,\n",
            "         3.3489e-01, -1.1494e-01, -2.8633e-01,  1.4211e-01,  1.9570e-01,\n",
            "        -4.2876e-01, -2.3173e-01, -5.6886e-01, -4.6399e-01,  5.2120e-02,\n",
            "         3.0353e-01, -9.4701e-02,  3.8085e-02, -2.4840e-01,  7.4952e-02,\n",
            "         1.8758e-01,  2.2575e-01,  2.1685e-02, -4.2748e-01,  9.5695e-02,\n",
            "        -4.5465e-01,  5.3925e-02,  1.9565e-01,  6.9678e-02,  2.5168e-01,\n",
            "        -3.3640e-02, -2.3894e-01,  2.8237e-02,  5.5020e-02, -1.5242e+00,\n",
            "         4.9727e-01, -2.0682e-01,  2.9603e-01,  3.1432e-01, -1.1179e-01,\n",
            "        -1.4613e-01, -9.9878e-01, -5.3549e-02,  4.6795e-02, -4.2140e-01,\n",
            "        -2.0132e-01, -1.2059e-01,  2.3453e-01, -1.7427e-01, -2.7114e-01,\n",
            "         2.2667e-01,  2.4717e-01,  8.2790e-02,  2.0027e-01, -5.4573e-01,\n",
            "         3.2082e-01,  3.7742e-01, -3.4683e-01,  2.0343e-01,  2.4747e-01,\n",
            "        -2.4230e-01, -1.9122e-01, -5.2590e-01, -4.7063e-01,  2.1794e-01,\n",
            "        -4.7510e-01,  2.6434e-01,  3.2057e-02, -5.9771e-02, -3.9939e-02,\n",
            "        -3.1071e-01,  1.1728e-01, -4.3652e-01, -4.5396e-02,  2.4530e-01,\n",
            "        -1.6229e-01, -2.8295e-01,  2.3923e-01,  4.3108e-02, -1.1330e-01,\n",
            "        -8.5025e-01, -7.7600e-01, -1.1971e+00, -3.6725e-01, -1.2563e-01,\n",
            "         5.9178e-02, -8.6790e-01, -4.0860e-01, -6.2955e-01, -4.2068e-01,\n",
            "         2.2450e-01,  9.0729e-02, -4.4404e-02, -2.3703e-01,  3.4917e-01,\n",
            "        -7.1472e-02,  1.2883e-01, -4.4544e-01,  3.7839e-02, -2.9906e-01,\n",
            "        -2.9594e-01, -3.4900e-01, -7.3058e-01, -8.1403e-01, -6.7737e-01,\n",
            "        -2.4373e-01,  4.1012e-01,  7.5872e-03,  2.7036e-01, -4.2403e-02,\n",
            "        -1.9319e-01, -9.1367e-02, -4.0235e-02, -6.0755e-01, -4.9145e-01,\n",
            "         8.6618e-01, -7.2334e-01, -5.7780e-01,  2.7172e-01, -4.8458e-02,\n",
            "         1.3838e-01,  5.1545e-01,  4.6300e-02, -7.3237e-02,  4.9299e-02,\n",
            "         2.1840e-01,  1.7117e-01,  3.2718e-01, -2.5536e-01, -1.5346e-01,\n",
            "        -1.6940e-01, -4.4765e-01,  2.3897e-01,  2.4097e-02, -3.9026e-01,\n",
            "        -4.2663e-02,  7.5042e-01, -3.4203e-01,  4.4560e-01,  2.4001e-01,\n",
            "        -8.6123e-01, -1.0676e+00,  4.6832e-01,  3.1898e-02, -2.6464e-01,\n",
            "        -1.4020e-02,  2.7712e-01, -1.5857e-01, -6.6285e-01,  5.5288e-01,\n",
            "        -3.4588e-01, -1.2019e+00,  9.9251e-02, -4.6444e-02, -2.6992e-02,\n",
            "        -1.5894e-01, -3.3581e-01,  2.4220e-01, -5.4563e-01, -3.5076e-01,\n",
            "        -6.8924e-01, -6.3476e-02, -5.0558e-01, -2.0476e-01, -4.0803e-01,\n",
            "        -6.8278e-01, -2.2648e-01, -2.6718e-02,  6.7081e-02, -8.6317e-01,\n",
            "         3.1163e-02, -1.0505e-01, -1.5499e-01, -1.0629e+00, -8.2382e-02,\n",
            "        -3.2060e-01,  1.7895e-01,  1.1186e-01,  4.3526e-01,  4.3923e-01,\n",
            "         2.4677e-01, -2.2420e-01, -6.7524e-02,  1.7191e-01,  8.8256e-02,\n",
            "        -2.0753e-02, -1.0391e-01,  1.4837e-01, -3.6387e-01, -1.6067e-01,\n",
            "        -2.2638e-02,  5.2728e-01, -1.4351e-01,  1.5945e-01, -1.4725e-01,\n",
            "        -7.4290e-01, -3.3471e-01, -3.6611e-01, -1.4414e-01,  2.0862e-01,\n",
            "        -6.2643e-04, -3.3634e-01,  6.2499e-01,  6.1201e-01, -7.2576e-02,\n",
            "         6.3170e-01,  3.3597e-02,  3.1514e-01, -5.3767e-01,  4.0516e-01,\n",
            "        -7.7304e-02, -5.1162e-01, -2.5158e-01, -1.0580e-01, -5.4782e-02,\n",
            "         1.2427e-01, -5.8429e-02,  3.3893e-03, -1.8681e-01, -9.3007e-02,\n",
            "        -8.0281e-02,  4.1149e-01, -3.1529e-01, -5.5938e-01,  2.5702e-01,\n",
            "        -2.1755e-01, -4.5191e-01, -4.7121e-02, -5.0236e-01, -2.0275e-01,\n",
            "         8.8973e-03, -2.7797e-03,  1.4417e-01, -1.2446e-01, -5.2259e-01,\n",
            "        -3.1207e-01,  6.5857e-02, -1.6755e-01,  3.5370e-01,  4.6643e-01,\n",
            "         6.3825e-02, -2.9691e-01,  1.0152e-01, -8.2456e-01, -1.6103e-01,\n",
            "         4.3382e-01, -2.4308e-01, -1.7413e-01, -2.4973e-01, -5.2999e-01,\n",
            "        -3.2934e-01,  9.6559e-02,  1.2081e-01, -5.9913e-02,  1.4520e-01,\n",
            "        -9.3543e-02, -7.1831e-01, -3.1311e-01, -9.2322e-02, -9.0618e-01,\n",
            "        -2.7296e-01,  2.6167e-01, -7.6994e-01, -6.3863e-01, -1.3786e-01,\n",
            "         1.0064e+00,  7.0309e-01, -1.2263e-01,  4.0856e-01, -5.3026e-02,\n",
            "         4.1676e-01, -1.6632e-02, -1.0903e-01,  1.0461e-01,  2.2414e-02,\n",
            "         5.9088e-02,  1.5363e-02, -1.8369e-01,  5.3023e-02, -2.4243e-02,\n",
            "        -7.0960e-03,  1.1794e-01, -2.2907e-02,  3.7457e-01, -8.6651e-02,\n",
            "         3.4967e-01, -2.7027e-01, -1.2152e-01, -1.2961e-01, -6.3131e-01,\n",
            "         1.9731e-01,  2.2104e-01,  9.9192e-02, -4.1463e-01, -5.2884e-01,\n",
            "        -8.6618e-02,  1.4192e-01,  6.8865e-02, -4.2174e-01, -5.0360e-01,\n",
            "         5.3972e-01,  2.0865e-01, -2.9194e-02, -1.6598e-02,  1.8556e-01,\n",
            "         3.8309e-02, -1.3065e-01, -3.2583e-02, -9.6578e-02, -1.7214e-01,\n",
            "        -3.7905e-01,  1.9920e-01, -1.2348e-01, -2.2517e-01, -2.9232e-02,\n",
            "        -6.9093e-02,  4.8675e-01,  2.7734e-02,  5.2266e-01,  8.1878e-02,\n",
            "        -1.5834e-01,  6.6255e-02, -5.3884e-01, -1.0871e+00, -2.8581e-01,\n",
            "        -6.4181e-01, -3.8521e-01,  3.8318e-01, -5.1552e-01, -2.3965e-01,\n",
            "        -4.0239e-01,  1.7419e-01, -7.3034e-02,  1.5889e-01,  6.8905e-01,\n",
            "        -3.8492e-01, -6.4999e-02, -2.5947e-01, -1.7450e-01,  1.1569e-01,\n",
            "         1.4068e-01, -6.6782e-02, -6.2755e-02, -3.4610e-02,  3.7152e-02,\n",
            "        -7.7913e-01,  8.8345e-02,  1.9436e-01,  2.2706e-01, -5.2846e-01,\n",
            "        -5.2076e-01, -1.1445e-01, -3.8454e-01, -8.2801e-05, -4.2227e-01,\n",
            "        -2.4295e-01, -2.8987e-01, -7.8112e-02, -3.7801e-01, -6.4562e-01,\n",
            "         1.9310e-01,  1.8210e-01,  7.0327e-03,  3.3427e-01,  3.7983e-01,\n",
            "         1.0055e-01, -3.8401e-01,  1.0334e-01,  6.4755e-02, -2.6633e-01,\n",
            "         7.2868e-02,  1.4268e-01, -1.1719e-03, -2.4998e-01,  4.7590e-01,\n",
            "         9.1195e-02,  1.2886e-01,  1.3832e-01, -4.1125e-01, -3.0105e-01,\n",
            "         5.6068e-02, -5.9889e-02, -2.0971e-02, -3.0884e-01, -3.1981e-01,\n",
            "        -1.6117e-01, -1.3587e-01,  2.4798e-01, -4.0303e-01, -9.1004e-01,\n",
            "        -9.2950e-02, -9.1557e-02,  5.4299e-01, -5.0128e-01, -1.2501e-01,\n",
            "        -5.5750e-01, -2.8379e-01, -4.6222e-01,  2.1610e-01, -5.2672e-01,\n",
            "        -1.8108e-01, -2.7932e-01, -7.0655e-01,  5.9079e-01, -2.9814e-01,\n",
            "        -6.5220e-01,  2.6344e-01, -1.6014e-02,  1.5255e-01, -4.9856e-01,\n",
            "        -4.6749e-01,  2.3153e-01, -5.6289e-01, -2.9145e-01,  1.3617e-01,\n",
            "         2.7377e-01,  2.2567e-01,  8.7935e-02, -7.1814e-01,  5.1950e-02,\n",
            "         1.9382e-01, -3.5294e-01, -7.6717e-01,  3.0259e-02,  1.8090e-01,\n",
            "         2.3899e-01,  2.5343e-01,  4.0236e-01, -3.4137e-01,  1.7600e-01,\n",
            "        -5.1143e-01,  3.8924e-01, -3.8541e-01, -2.1890e-01, -6.5613e-02,\n",
            "        -4.3253e-01, -5.3129e-01,  4.5741e-01, -4.3776e-01, -3.2444e-01,\n",
            "        -3.6426e-01, -6.0325e-01, -2.5868e-01,  5.4743e-01, -7.9632e-01,\n",
            "         2.1021e-01,  4.9332e-01, -1.3467e-01, -2.0898e-03,  2.6538e-01,\n",
            "        -1.3790e-01,  5.6276e-01,  2.8989e-01,  5.0951e-01,  1.1007e-01,\n",
            "         2.5435e-02, -9.2704e-02, -1.3034e-01,  1.1559e-01, -5.9255e-03,\n",
            "        -4.2103e-01, -2.8116e-01,  3.6863e-02,  4.6320e-01,  1.4474e-01,\n",
            "         5.5567e-01,  4.8387e-01, -5.6662e-01,  3.0046e-01, -1.2604e-01,\n",
            "        -8.4441e-02,  2.7825e-01,  2.2219e-01, -2.3690e-01,  4.4205e-02,\n",
            "         3.4786e-01,  7.8187e-02, -5.3108e-01, -7.2631e-01,  6.9955e-01,\n",
            "         4.1757e-01, -5.4099e-02, -4.1241e-01, -1.9097e-01,  8.6607e-02,\n",
            "         7.1010e-02, -4.3883e-01, -5.9656e-01, -2.0807e-01, -2.9673e-01,\n",
            "        -2.9678e-02,  3.3533e-01, -5.5598e-01, -6.1267e-01,  1.9302e-01,\n",
            "        -6.9895e-01,  3.0922e-01, -1.8618e-01, -8.0204e-02, -3.3710e-01,\n",
            "         1.6080e-01,  6.3348e-02, -1.7832e-01,  5.5189e-02,  2.6404e-01,\n",
            "        -4.7417e-02, -4.1208e-01, -1.0223e+00, -1.0408e+00, -2.9874e-01,\n",
            "        -9.9666e-01, -2.7719e-01,  9.3195e-02,  5.5154e-01,  2.2416e-01,\n",
            "        -2.8376e-01,  9.3812e-04,  8.1141e-01,  3.4060e-01, -2.0700e-01,\n",
            "        -5.2250e-01,  2.0095e-01, -1.1379e-01,  4.4784e-01, -1.4321e+00,\n",
            "        -4.4561e-01, -1.4627e-01,  3.7501e-01, -6.6766e-01, -9.4958e-02,\n",
            "         5.1718e-01, -1.0252e-01,  2.1216e-01, -2.1378e-01,  4.7156e-02,\n",
            "        -3.8615e-01, -1.5911e-01, -4.0104e-01,  1.7715e-01,  1.6796e-01,\n",
            "        -6.7389e-02, -3.1491e-01,  6.8483e-01,  2.5068e-01,  1.0966e-01,\n",
            "         1.1788e-01, -5.1663e-01, -2.8588e-01, -1.9566e-01, -1.8876e-01,\n",
            "         5.7766e-01, -3.2410e-01,  3.0168e-01, -3.7719e-02, -4.9306e-01,\n",
            "         3.5060e-02,  2.2300e-01, -2.1815e-01,  2.1886e-01,  1.0432e-01,\n",
            "        -2.3199e-01, -2.1966e-01,  2.5670e-01,  2.3883e-01, -5.6841e-01,\n",
            "         3.6074e-01, -1.4126e-01,  1.1646e-01, -4.0084e-01,  1.3368e-01,\n",
            "        -8.5294e-02,  6.5739e-01, -1.0002e-01, -1.7664e-01, -3.5748e-01,\n",
            "        -6.4355e-01, -5.3927e-02,  1.2022e-01,  4.6539e-01, -3.1003e-01,\n",
            "         4.9454e-01,  2.1564e-01, -9.2290e-01,  1.7516e-01, -3.1033e-01,\n",
            "        -9.4755e-02,  2.4728e-01,  8.3636e-01, -5.8676e-01,  1.7522e-01,\n",
            "        -7.0981e-02, -2.0161e-01,  7.6369e-02, -2.0744e-01, -5.6440e-01,\n",
            "         2.2448e-01, -2.7544e-01, -8.4588e-03, -1.7637e-02, -2.8651e-01,\n",
            "        -3.0018e-01, -2.3728e-01,  1.1316e-01, -1.1785e-01, -1.0168e+00,\n",
            "         1.4394e-01, -1.4252e-01, -3.4771e-02,  2.7419e-01,  4.0207e-01,\n",
            "        -8.9756e-01,  8.4846e-02, -4.5767e-01, -6.4279e-01,  1.4211e-01,\n",
            "        -5.9224e-01,  3.0759e-02, -8.3227e-01, -2.9763e-01, -2.5405e-01,\n",
            "        -2.1151e-01, -3.9425e-01, -1.9705e-01, -7.3989e-03, -2.1701e-01,\n",
            "         3.0191e-01,  3.3422e-01, -3.1051e-01,  3.3202e-01, -1.2840e+00,\n",
            "        -9.4011e-01, -1.3440e-01,  3.1095e-01, -5.5167e-02,  2.8436e-01,\n",
            "        -4.3858e-01,  1.5183e-01,  5.7251e-01,  2.6631e-01, -6.6509e-01,\n",
            "        -1.9618e-01, -4.2349e-01, -2.5062e-01,  1.6905e-01,  1.1645e-01,\n",
            "        -1.8794e-01,  1.0321e-01, -4.3985e-01, -6.0425e-02, -3.4122e-01,\n",
            "         1.1466e-01,  1.5334e-01,  1.9236e-02,  4.5022e-01, -8.5483e-02,\n",
            "         8.8533e-02, -9.5430e-03,  5.1070e-01,  1.1639e-01,  1.7090e-02,\n",
            "         6.2721e-01, -1.5911e-01, -2.0952e-01, -2.1122e-02, -2.7798e-01,\n",
            "        -3.3725e-01,  3.7273e-02,  1.5871e-01, -3.6057e-01,  2.0475e-01,\n",
            "         2.2255e-01, -4.2728e-01, -1.8718e-01, -4.8107e-01,  2.1117e-01,\n",
            "         3.1052e-01, -4.0669e-01,  9.4289e-02, -5.3066e-01, -8.5062e-01,\n",
            "         6.1446e-02,  5.6979e-01, -2.9333e-02,  3.1561e-01,  2.6269e-01,\n",
            "        -4.2065e-01,  6.1796e-02, -5.2037e-02, -3.4912e-01, -5.8652e-01,\n",
            "        -2.8742e-01, -6.9327e-02, -4.2555e-01,  1.3474e-01, -1.4299e-01,\n",
            "         3.2080e-01, -4.3165e-01,  4.5648e-01, -1.0393e-01, -5.6441e-01,\n",
            "         1.7341e-01, -1.9407e-01, -7.2464e-01, -5.7947e-01,  5.3529e-01,\n",
            "         2.8511e-01,  9.5814e-02, -5.3425e-02,  3.0443e-01, -2.2813e-01,\n",
            "        -4.5846e-01, -4.3440e-01,  2.7754e-01,  3.2358e-02, -2.4032e-01,\n",
            "         1.5092e-01, -5.1276e-01, -4.9388e-01,  7.0650e-01,  1.1320e-01,\n",
            "        -8.8370e-01,  6.6086e-01, -7.7758e-03, -4.2459e-02, -9.2860e-02,\n",
            "        -5.3598e-01,  4.1511e-01,  8.3281e-02,  7.5097e-02,  1.5320e-02,\n",
            "        -4.1595e-01, -4.3910e-01, -3.5083e-01,  2.2445e-01,  1.2863e-01,\n",
            "        -7.6776e-01,  2.3768e-01, -3.1267e-01, -1.9435e-01,  9.2688e-02,\n",
            "        -9.2372e-02, -1.3350e-01, -3.1159e-01,  5.3933e-02, -1.3673e-01,\n",
            "         6.8421e-02, -5.3632e-01, -3.7246e-01,  3.1657e-01, -1.6929e-01,\n",
            "        -3.8079e-01,  3.7504e-01, -6.2003e-01,  6.2995e-02, -2.9333e-02,\n",
            "         1.2166e-02,  6.6238e-02, -7.1892e-01,  4.0427e-02,  2.0936e-01,\n",
            "        -8.6697e-02, -7.5059e-01, -2.9198e-01, -1.3933e-01, -7.6138e-03,\n",
            "         5.0273e-02,  2.7877e-01, -2.8281e-01, -6.9352e-02, -2.0336e-01,\n",
            "        -3.3838e-01, -1.2421e-01, -4.4016e-02, -7.5633e-01, -1.7456e-01,\n",
            "         1.8192e-02,  1.5465e-01,  3.0141e-01,  2.6681e-01, -2.0775e-01,\n",
            "        -3.8517e-01, -5.3896e-01, -7.2866e-01, -7.4464e-01,  2.0732e-01,\n",
            "         2.8940e-01,  3.8412e-01, -3.5539e-01,  2.8258e-01, -5.3180e-01,\n",
            "         6.4663e-02, -1.6003e-01, -4.4078e-02, -1.5217e-01, -2.7804e-01,\n",
            "        -2.1196e-02,  1.9600e-01, -8.3131e-02, -3.6770e-02, -1.4147e-01,\n",
            "         2.2405e-01, -1.1912e-01,  3.2318e-01,  1.1050e-01, -5.2444e-01,\n",
            "         4.2610e-01, -8.8079e-01, -2.9264e-01, -1.1005e-01,  1.6215e-01,\n",
            "        -4.7668e-01, -6.3106e-01,  4.7530e-02, -1.8565e-01, -3.3218e-01,\n",
            "         4.8777e-02, -7.6603e-01,  3.3628e-01,  1.3757e-01,  5.3731e-01,\n",
            "         2.8793e-01, -8.2231e-02, -1.8803e-01,  5.1700e-02,  2.7082e-01,\n",
            "        -1.7776e-01, -6.2186e-02, -9.3304e-02, -1.4269e-01, -1.6343e-01,\n",
            "        -3.3977e-01, -5.5857e-01, -5.4002e-01,  1.6060e-01,  1.7004e-01,\n",
            "        -3.2279e-01, -4.5707e-01, -3.6517e-01, -5.1549e-01,  4.2069e-01,\n",
            "         2.1010e-01,  2.6070e-01, -1.9646e-01, -2.0973e-01, -3.6546e-01,\n",
            "        -2.0635e-01,  6.6203e-02,  2.3397e-01,  2.9810e-02, -2.5236e-01,\n",
            "        -1.1457e-01,  1.9035e-01,  1.5963e-01,  9.9991e-02, -1.0846e-01,\n",
            "         7.7550e-03, -5.3352e-02, -8.3883e-01, -2.2676e-01,  1.5019e-01,\n",
            "        -6.2022e-02, -6.5022e-01, -4.7411e-01,  5.2256e-01,  2.9087e-01,\n",
            "         4.3617e-01,  1.2351e-01, -1.2202e-02, -4.4329e-02,  1.4813e-01,\n",
            "        -2.4624e-02, -1.0743e-02, -3.5133e-01,  3.3932e-01, -2.4901e-01,\n",
            "         5.2962e-01, -3.7202e-01, -1.8926e-01,  1.4788e-01, -2.8546e-02,\n",
            "         6.8788e-01, -1.3929e-01,  4.5095e-01,  2.1428e-01,  4.2709e-03,\n",
            "        -3.6338e-01, -4.0906e-01,  3.1759e-02, -5.2010e-01, -1.4079e-01,\n",
            "         1.7391e-01,  4.8397e-02, -2.6914e-01,  5.1058e-01, -8.3128e-01,\n",
            "         5.9865e-02, -3.3916e-01, -9.5532e-02, -2.4822e-02, -7.2304e-02,\n",
            "         1.4991e-02, -3.1075e-01,  2.0771e-01, -7.9039e-02,  3.3314e-01,\n",
            "        -2.6971e-01,  4.4979e-01, -2.5114e-01, -1.1941e-01,  1.4560e-01,\n",
            "         6.6226e-01,  5.3148e-01,  3.6326e-02, -2.1482e-01,  1.5212e-01]), 'outputs': tensor([ 7.2683,  7.2778, -7.0258,  7.3182, -7.0776,  7.3485, -7.8109, -7.7147,\n",
            "         7.2999, -6.9208, -7.5714, -7.7541, -6.9477, -8.0255,  7.3350,  7.2702,\n",
            "        -7.4246,  5.7398, -7.7130, -6.8472,  7.1375, -7.1912, -7.0511, -6.9834,\n",
            "         7.1658, -7.1436, -7.5435,  7.3468,  7.2589,  7.2887, -6.8296,  7.2763,\n",
            "        -8.0829, -7.9503,  7.2931,  7.2987, -6.9177,  7.2912, -7.0211, -7.0273,\n",
            "         6.2449,  7.3194,  7.3179,  6.9796, -8.0984,  7.2841,  7.2950, -7.7064,\n",
            "        -6.8966, -7.0056,  7.3608, -7.1519,  7.2895, -7.3125,  7.0247,  7.2641,\n",
            "         7.2431,  6.2779,  7.2554,  7.3261,  7.3416, -7.0720,  6.4456,  7.3131,\n",
            "         7.2680,  6.6897,  7.3271,  7.1545, -7.8846,  7.3506,  7.2823,  7.2588,\n",
            "         6.5007,  7.2639, -7.4880, -7.4900,  7.2944,  7.2974, -7.3942,  7.2859,\n",
            "         7.2920,  6.2866,  7.2330, -7.5608, -7.9311, -7.4484, -7.6113, -7.8929,\n",
            "        -7.3141, -6.9970, -7.0564,  7.2876,  6.9509,  7.3073, -7.1535,  7.2626,\n",
            "         6.1406, -7.9761,  6.3714,  7.2825,  7.3295, -8.0122, -7.5707,  7.2929,\n",
            "        -7.5079,  7.2981,  7.2563,  7.2935, -7.5559,  7.2914,  7.2701, -7.6274,\n",
            "         7.2942,  6.1465, -7.9455, -7.2456, -7.8506,  6.6846, -7.4158, -7.7563,\n",
            "         6.9059, -7.7373,  7.3202, -6.8930, -7.4994, -7.9655,  6.2618,  7.2524,\n",
            "        -6.9323,  6.6709, -7.8649, -7.5816,  7.2735, -7.2522,  6.9519, -7.3634,\n",
            "        -7.4896,  6.5401, -6.8019,  7.3121, -7.2110,  6.8523, -8.0012,  7.2838,\n",
            "        -6.9600, -7.7618,  7.2756, -7.8473, -8.1324, -7.2913,  7.3081,  7.3017,\n",
            "         7.3453, -6.8679, -7.6124, -7.9901, -7.2358,  7.3261,  7.2864,  7.3140,\n",
            "         6.5504,  7.3002, -7.0651, -7.5333, -8.1040, -7.2882,  7.3237, -8.1667,\n",
            "        -7.9683, -6.9032,  7.1464,  7.2931, -7.6354, -7.6173,  7.2562, -7.7142,\n",
            "        -6.8998,  7.3471, -7.1189, -7.5762,  7.2428,  7.3428,  7.2527,  6.9637,\n",
            "         7.3266, -7.5072,  7.3159, -8.0604,  7.2659,  7.3340,  7.2499,  7.2864,\n",
            "         7.2868,  7.3533, -7.1936,  5.9088,  7.2437, -7.9113,  5.7118,  7.2636,\n",
            "        -7.0826, -7.5241, -7.3631, -7.0294,  6.6719,  7.2521, -7.1268,  7.3353,\n",
            "        -7.5740,  7.2856,  7.2947, -7.5073,  7.3531, -7.8100, -7.9650,  7.2885,\n",
            "        -6.9530, -7.5592, -7.3351, -7.8925,  7.3178, -6.9931,  7.3033,  7.3062,\n",
            "        -7.8058, -8.1286,  6.3531,  7.3051, -7.4268, -8.0652, -7.0084,  7.3431,\n",
            "         7.2587, -7.6470, -7.4044,  6.6302, -7.6965, -7.4848,  6.4243, -7.2704,\n",
            "        -7.0117,  7.2648,  7.2755,  6.7097, -7.6803,  6.1047,  7.2667, -7.7239,\n",
            "         7.2912, -7.0309, -7.3964,  5.9548,  7.3131,  6.7363, -7.3673, -7.5349,\n",
            "        -7.5164,  7.3299,  6.1367,  7.3027,  7.2722,  7.2801, -7.4770, -7.0526,\n",
            "        -6.7134,  7.2805,  7.2972,  7.3174,  7.3179,  6.3387, -7.7306,  5.9556,\n",
            "         7.2917, -7.1876, -7.6964,  7.3431,  7.3187,  7.3077,  7.3518, -6.9309,\n",
            "        -7.7764,  5.8247, -7.8645, -7.8859, -6.7734, -7.3881, -8.1610,  7.3015,\n",
            "        -6.9606, -7.7772,  7.2848, -7.1455, -6.9474, -7.4576, -7.1830, -8.0503,\n",
            "        -7.2097,  7.2713, -7.9033,  7.2832, -6.9489, -7.3255, -7.5480,  7.3011,\n",
            "         7.2702,  7.2559,  7.2821,  7.3146,  7.2596,  7.2920,  7.3539,  6.7710,\n",
            "        -7.6331, -7.5908, -7.2828, -7.1766, -6.9195, -7.1324,  6.5194,  7.3263,\n",
            "         7.2879,  7.3570, -7.2215,  7.3628,  7.3404, -7.5390,  7.2876, -7.6205,\n",
            "         7.2857, -7.6175, -6.9676,  7.3473, -7.7726, -7.3136, -7.6972,  7.2985,\n",
            "        -7.1248, -7.6062,  7.2937, -7.4075, -7.9346,  6.1656, -7.8380,  7.2928,\n",
            "        -7.7222,  7.3555, -7.4724, -7.0219,  7.2690, -7.5969, -6.9401,  7.2637,\n",
            "        -6.9139,  7.2461, -7.5844, -7.5684,  6.3629, -8.0641, -6.6707,  7.3011,\n",
            "        -7.9434, -7.0408,  7.3458,  6.5084, -7.0320, -7.6929,  7.3446, -7.2617,\n",
            "        -7.1632, -8.1649,  7.2567,  7.2566, -7.4221, -7.4965, -7.9977,  7.2984,\n",
            "        -6.6984,  7.3580, -7.6672, -7.9285,  7.1898, -7.3931, -7.3511,  7.3693,\n",
            "        -7.5490, -7.6964,  7.2942,  7.1720, -7.7200,  7.2918, -7.7991, -7.7185,\n",
            "         7.2968,  7.3153,  7.2604, -6.7940, -7.2128, -7.8246, -7.8523, -7.8451,\n",
            "         6.6579, -7.3590, -7.3226,  7.2732,  7.3023, -7.5792,  7.3399, -7.9792,\n",
            "        -8.0157, -7.8015, -7.8807,  7.2648, -7.1174, -7.4731, -7.1510, -8.0198,\n",
            "        -6.9418, -7.6720,  7.2601,  7.3294, -7.4833,  6.3512,  6.5989, -7.5085,\n",
            "        -7.0588,  7.2495,  7.2553,  7.2794,  6.8324, -7.5960, -7.9002,  7.3005,\n",
            "         7.3015,  7.2888,  6.1696,  7.2651, -7.2796,  7.2958, -7.2365, -7.5385,\n",
            "         7.2545, -8.1639,  7.2810, -7.8650,  7.2559,  7.3126, -6.9499, -7.0600,\n",
            "        -7.6096, -6.9128, -7.7387,  7.2894,  6.6068, -7.8550,  6.5596, -7.0466,\n",
            "        -7.3739,  7.2636, -8.1270, -7.2380,  7.2968,  7.3022,  7.2919, -7.9017,\n",
            "         7.2958,  7.3277,  7.2619, -7.2644, -7.8717, -7.5888,  6.5245,  6.4636,\n",
            "        -6.9533, -7.8151, -7.9240, -7.0768,  7.2647,  7.3636,  6.3304,  7.2642,\n",
            "         5.6927,  7.3137,  7.3678, -7.7233, -7.7278, -7.3220, -7.9106,  7.2729,\n",
            "         7.3592,  7.2908,  6.7637,  7.2802,  7.2710, -7.3598,  7.2577,  7.3142,\n",
            "         7.2895,  7.2568, -7.0677,  7.2921,  7.2807,  7.2711,  6.1122,  6.6549,\n",
            "         7.2483,  7.3273, -7.6250, -8.0894,  6.5032, -6.8428, -7.3351,  7.2948,\n",
            "         7.3590,  7.3710,  7.2530,  7.3159,  6.5610,  7.3316,  7.2713, -8.0972,\n",
            "         7.2914, -8.3693, -6.8500, -7.8838,  7.2821,  7.3499, -7.0531,  7.2750,\n",
            "         7.2947, -7.9827,  7.3470,  7.2420,  7.2562, -6.8329,  7.2490,  7.2436,\n",
            "        -7.4828,  6.4778,  7.1095, -7.5198,  7.2620, -7.8250, -8.1596, -7.0350,\n",
            "         7.2825,  7.2660, -7.1769, -7.7802, -6.8808,  7.3629, -7.0888, -7.2827,\n",
            "         7.2523, -7.6163,  6.1533,  7.3423, -7.2654,  7.2880,  7.2928,  7.2749,\n",
            "         7.2637,  7.2307,  7.2916,  6.5924, -7.0655,  7.3270,  7.3286,  5.7534,\n",
            "         7.0772, -8.2831,  5.7462,  6.5885,  7.2525,  7.3371, -7.4131, -7.4785,\n",
            "         7.2599,  6.1081, -7.8675,  7.3233, -8.1416, -7.0809, -7.7970,  7.2587,\n",
            "        -7.2983,  7.2716,  7.2775, -7.8056, -7.3991, -6.9190, -7.7594,  7.2405,\n",
            "        -7.5756, -7.9078, -8.0353, -7.6489, -8.0426, -7.3584,  7.2878,  7.2759,\n",
            "        -7.5015, -7.5973, -7.1046,  7.2967,  7.3379, -8.1687, -7.4785, -7.6110,\n",
            "         7.2777, -8.1249, -7.7867,  7.3044,  7.3574,  7.2877,  7.2993,  7.2896,\n",
            "        -7.8382,  7.2272,  7.3307, -7.0043, -7.0840,  7.3053, -6.9453, -7.0237,\n",
            "         7.1472,  7.2505, -6.8858,  6.7975, -7.2273,  7.3320,  7.2931, -8.0001,\n",
            "        -7.8873, -7.7282,  6.3702,  7.3085, -7.3465,  6.6457, -7.5087, -7.8279,\n",
            "         7.2861, -7.4665,  6.4855,  7.3579,  6.6785, -6.9101,  7.3287,  7.3055,\n",
            "         7.2797, -7.2219,  7.2841,  6.6187, -7.7176, -7.0113,  7.3005, -7.7165,\n",
            "         7.2933, -7.5581, -7.3651,  5.9131,  6.3661, -7.7833,  6.1636,  7.2724,\n",
            "         7.2828,  7.2612, -6.9043,  7.3284, -7.9099, -7.9999,  7.2279, -7.2435,\n",
            "        -7.0947,  7.3224,  6.5436,  7.3236, -7.5780, -7.5631, -6.9675,  7.2942,\n",
            "         7.3671, -7.4636,  7.3079,  7.2982,  7.1047,  7.3283, -7.8949,  7.3498,\n",
            "        -6.9160,  7.3217,  7.1342,  7.2655, -7.5070, -7.1432, -6.9421,  7.2625,\n",
            "         7.2889, -7.9258,  7.2860,  7.2625, -7.1831, -7.9173, -7.8271,  6.6250,\n",
            "        -7.5505, -7.5433,  7.3006, -7.8712, -6.9854, -8.2259, -7.1997, -7.5111,\n",
            "        -7.6742,  7.3002,  7.2708,  6.6240, -7.9192, -7.0400, -7.7266,  7.3578,\n",
            "         6.5121,  7.2939, -7.0467,  7.3315,  7.3053, -7.8744, -7.5399,  7.2557,\n",
            "        -7.3751, -7.6671, -7.6659,  7.3408,  7.3038,  6.9456,  5.5965, -7.8341,\n",
            "        -7.5132, -7.1370, -7.1134,  6.8690,  7.2899,  6.1458, -7.1454, -7.1161,\n",
            "         7.2570, -7.2941,  5.5875,  6.2389,  7.2874,  7.3163,  7.3607, -7.6469,\n",
            "         7.2051, -7.0275, -7.6207,  7.3271, -7.8834, -7.4556, -7.4007, -7.2437,\n",
            "        -7.3654,  7.2778, -7.4225, -7.5583,  6.0478,  7.3407,  7.2717,  7.2944,\n",
            "         7.2768,  7.2868,  7.2857, -7.7627,  7.3477,  6.1028,  7.2620, -7.7702,\n",
            "         7.3143,  5.9369,  7.2742,  7.3234, -7.7231,  7.3218,  7.3121, -7.0306,\n",
            "         7.3429,  7.2812,  6.3666,  5.6759, -7.8427,  7.3024, -7.7806, -6.8158,\n",
            "         7.2930, -6.8437,  7.2669,  7.2982, -8.2745, -7.5651, -7.6616,  7.2552,\n",
            "         6.7565, -7.9359, -8.0138,  7.3066,  7.2399, -7.3904,  7.2839, -7.5322,\n",
            "        -7.7332, -7.7464,  7.3318,  7.3028,  7.3243,  7.2644,  7.3602, -7.6303,\n",
            "        -7.9407, -7.4593, -6.9424,  6.6419, -7.7853,  7.2367, -7.2308, -7.5992,\n",
            "         7.2949,  7.3559,  7.2993, -7.6446,  7.2410,  7.3080,  7.3387,  7.2487,\n",
            "        -7.4185, -7.8076, -7.5370,  7.2919, -7.4407, -7.9337, -6.9742,  7.2788,\n",
            "        -7.2926, -7.9291, -7.2654, -8.0086, -7.2433,  7.3318, -7.8257, -8.0592,\n",
            "        -7.4120,  7.2934,  7.2824,  7.3438, -7.8177, -8.1212,  7.2720,  7.3146,\n",
            "        -7.0984,  7.3617,  6.2802,  6.8235,  7.2796, -6.7909,  7.0677,  7.3033,\n",
            "         7.2845, -7.7796,  6.7663, -7.8881,  7.2688, -6.9003, -6.9584, -7.1706,\n",
            "         7.2841, -7.0485,  7.2643,  7.2843,  6.7674, -6.7181, -7.8095,  7.2888,\n",
            "         7.3537, -7.4719,  7.2895, -7.7793, -7.8898, -6.8154, -7.7861,  7.3203,\n",
            "        -7.8978,  7.2592,  7.2837, -7.0710,  7.3476,  7.2950,  7.3677, -8.1177,\n",
            "        -7.6584, -7.2318,  7.2538, -7.8357,  7.2821,  7.0266,  7.3513, -7.0222,\n",
            "        -6.8765, -6.7935,  7.3675, -6.9463, -7.7463,  7.3002, -7.4801,  7.2989,\n",
            "         7.3006,  7.2620, -7.0310, -7.6606, -8.0565,  7.2793, -6.9158, -7.2237,\n",
            "         7.2773,  6.4015, -7.7820, -7.9931, -7.4223, -7.6233, -7.0867, -7.7016,\n",
            "         7.2847,  6.4269, -7.2321,  7.2928, -7.4710, -7.4952,  7.2638, -7.8688,\n",
            "        -7.6060, -8.1093, -7.7878, -7.3075, -8.0034, -7.7131, -8.0779,  7.3710,\n",
            "         7.2666, -7.4028,  7.3112,  7.2457, -7.4990,  7.3407,  7.3006,  7.2802,\n",
            "        -7.6765, -7.0647,  7.3449,  7.2357, -7.0778, -7.4687,  7.2735,  7.2676,\n",
            "        -7.3968,  7.2870, -7.9321, -6.9513, -6.9173,  7.3781, -7.7059, -6.9973,\n",
            "         6.4582, -7.6623,  7.2545, -6.8307, -7.4471, -7.7385,  5.9444, -7.5729,\n",
            "         7.2626, -7.5919,  7.2631,  7.3115, -7.4695, -7.7570, -7.8578,  7.2796,\n",
            "         7.2985, -7.5455, -7.6343,  5.8588, -7.6254, -6.8047, -7.6808, -6.9243,\n",
            "         7.2874, -7.5815,  7.2747,  7.3591,  7.2997, -7.2848,  7.2809, -7.1928]), 'labels': tensor([ 1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1., -1.,\n",
            "         1.,  1., -1.,  1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,\n",
            "         1.,  1., -1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1.,\n",
            "         1.,  1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,  1.,  1.,\n",
            "         1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
            "         1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,\n",
            "        -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "         1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "         1.,  1., -1., -1., -1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1.,\n",
            "         1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,\n",
            "        -1.,  1., -1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,\n",
            "        -1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
            "        -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1.,\n",
            "         1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
            "         1., -1.,  1.,  1., -1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,\n",
            "         1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1.,  1.,  1.,\n",
            "        -1., -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.,\n",
            "         1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1., -1.,  1.,\n",
            "         1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,\n",
            "         1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1.,\n",
            "        -1.,  1., -1., -1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1.,\n",
            "        -1., -1., -1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
            "         1.,  1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,\n",
            "        -1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1., -1.,  1.,\n",
            "        -1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,\n",
            "        -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1., -1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,\n",
            "        -1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,\n",
            "         1.,  1.,  1., -1., -1., -1., -1., -1.,  1., -1., -1.,  1.,  1., -1.,\n",
            "         1., -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
            "        -1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,\n",
            "         1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,  1., -1., -1.,\n",
            "        -1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,  1.,\n",
            "         1., -1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1.,\n",
            "         1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.,  1.,\n",
            "         1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
            "         1.,  1., -1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
            "         1., -1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,\n",
            "         1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,\n",
            "        -1., -1., -1.,  1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
            "         1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
            "        -1., -1.,  1.,  1., -1.,  1., -1., -1., -1.,  1., -1.,  1.,  1., -1.,\n",
            "        -1., -1., -1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1.,\n",
            "        -1.,  1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
            "        -1.,  1.,  1., -1., -1.,  1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,\n",
            "         1., -1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,\n",
            "         1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1., -1.,\n",
            "        -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,\n",
            "        -1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
            "        -1.,  1., -1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1.,\n",
            "        -1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1., -1., -1.,  1.,\n",
            "         1.,  1., -1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,\n",
            "        -1., -1., -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1.,\n",
            "        -1., -1., -1., -1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
            "         1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,\n",
            "         1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1., -1., -1.,\n",
            "        -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,\n",
            "         1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1., -1.,  1.,  1.,\n",
            "         1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1., -1., -1., -1.,  1.,\n",
            "        -1., -1., -1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,\n",
            "         1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1.,\n",
            "         1., -1., -1., -1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.,\n",
            "         1., -1., -1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1.,\n",
            "        -1., -1.,  1., -1.,  1.,  1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,\n",
            "        -1.,  1.,  1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,  1., -1., -1.,\n",
            "        -1., -1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1., -1., -1.,\n",
            "        -1., -1., -1., -1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
            "        -1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,\n",
            "        -1., -1.,  1., -1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,  1.,  1.,\n",
            "        -1., -1., -1.,  1.,  1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
            "         1.,  1.,  1., -1.,  1., -1.])}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(io.BytesIO(b))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def load_dynamics_data(file_path):\n",
        "    \"\"\"\n",
        "    Load dynamics data from a pickle file.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path: The path to the pickle file containing the data.\n",
        "\n",
        "    Returns:\n",
        "    - A list of dynamics data entries.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'rb') as f:\n",
        "        # Skip the HyperParams object\n",
        "        pickle.load(f)\n",
        "\n",
        "        # Read the remaining results\n",
        "        results = []\n",
        "        while True:\n",
        "            try:\n",
        "                result = pickle.load(f)\n",
        "                results.append(result)\n",
        "            except EOFError:\n",
        "                break\n",
        "\n",
        "    # Extract 'dynamics' from the results\n",
        "    for result in results:\n",
        "        if 'regular' in result and 'dynamics' in result['regular']:\n",
        "            return result['regular']['dynamics']\n",
        "\n",
        "    raise ValueError(\"No dynamics data found in the provided file.\")\n",
        "\n",
        "def plot_losses_and_errors(dynamics):\n",
        "    \"\"\"\n",
        "    Plot training and test losses and errors from dynamics data.\n",
        "\n",
        "    Parameters:\n",
        "    - dynamics: A list of dynamics data entries.\n",
        "    \"\"\"\n",
        "    if not dynamics:\n",
        "        print(\"No dynamics data available.\")\n",
        "        return\n",
        "\n",
        "    # Print the first and last entries\n",
        "    print(\"First entry:\", dynamics[0])\n",
        "    print(\"Last entry:\", dynamics[-1])\n",
        "\n",
        "    steps = []\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    train_errors = []\n",
        "    test_errors = []\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for entry in dynamics:\n",
        "        steps.append(entry['step'])\n",
        "\n",
        "        # Extract loss and error values\n",
        "        if 'train' in entry:\n",
        "            train_losses.append(entry['train'].get('loss', None))\n",
        "            train_errors.append(entry['train'].get('err', None))\n",
        "            # Calculate accuracy from error\n",
        "            train_accuracies.append(1 - entry['train'].get('err', 0))\n",
        "\n",
        "        if 'test' in entry:\n",
        "            test_losses.append(entry['test'].get('loss', None))\n",
        "            test_errors.append(entry['test'].get('err', None))\n",
        "            # Calculate accuracy from error\n",
        "            test_accuracies.append(1 - entry['test'].get('err', 0))\n",
        "\n",
        "    # Plot training and test losses\n",
        "    plt.figure(figsize=(8, 3))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(steps, train_losses, label='Train Loss', color='blue')\n",
        "    plt.plot(steps, test_losses, label='Test Loss', color='red')\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('Steps')\n",
        "    # plt.xlim(10 ** 2, np.max(steps))\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Test Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    print(f\"step len\", len(steps))\n",
        "\n",
        "    # Plot training and test accuracies\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(steps, train_accuracies, label='Train Accuracy', color='blue')\n",
        "    plt.plot(steps, test_accuracies, label='Test Accuracy', color='red')\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('Steps')\n",
        "    # plt.xlim(10 ** 2, np.max(steps))\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Test Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "file_path = 'C10k3Lsp_adam/C10k3Lsp_adam.pickle'\n",
        "dynamics_data = load_dynamics_data(file_path)\n",
        "plot_losses_and_errors(dynamics_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "mvSQSJs5DXZx",
        "outputId": "c3779524-afd1-4b5c-f4d4-e869e818a00f"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First entry: {'step': 0, 'wall': 0.01184870100041735, 'batch_loss': 0.7968352136670752, 'norm': 80.4820279441315, 'dnorm': 0.49106236378435025, 'train': {'loss': 0.7097318003053928, 'aloss': 0.7097318003053928, 'aaloss': 0.7097318003053928, 'err': 0.49714285135269165, 'nd': 0, 'dfnorm': tensor(0.3723), 'fnorm': tensor(0.3723)}, 'test': {'loss': 0.7035889364935375, 'aloss': 0.7035889364935375, 'aaloss': 0.7035889364935375, 'err': 0.4814285635948181, 'nd': 0, 'dfnorm': tensor(0.3714), 'fnorm': tensor(0.3714)}, 'wnorm': [39.05115856885033, 49.396493420512364, 49.82594658141633, 5.45159479580254], 'dwnorm': [0.23234094148568268, 0.29996399522477873, 0.2999099302735704, 0.04242559785271834]}\n",
            "Last entry: {'step': 9640, 'wall': 32.14731873200071, 'batch_loss': 0.0051474246183460005, 'norm': 28.6719981629486, 'dnorm': 83.6087781083084, 'train': {'loss': 0.0006983444486166438, 'aloss': 0.0006983444486166438, 'aaloss': 0.0006983444486166438, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.3493), 'fnorm': tensor(7.3493)}, 'test': {'loss': 0.0007777651094558019, 'aloss': 0.0007777651094558019, 'aaloss': 0.0007777651094558019, 'err': 0.0, 'nd': 0, 'dfnorm': tensor(7.2731), 'fnorm': tensor(7.2731)}, 'wnorm': [14.296173972360116, 13.98695149736319, 13.054147941477735, 14.643111141787887], 'dwnorm': [40.36475024883542, 50.77078123018841, 51.07932993080728, 11.709310286253592]}\n",
            "step len 84\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUhgup4z2x/1U2MInL4FAl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kodai-utsunomiya/memorization-and-generalization/blob/main/numerical_experiments1_time_scale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $(k, d)$-Sparse Parity Task\n",
        "\n",
        "$d$-個の数字の内の $k$ 個の数字のパリティを計算する（$k \\le d$）という問題\n",
        "\n",
        "## データセット\n",
        "\n",
        "---\n",
        "\n",
        "- $\\mathcal{D}_{k, d} = \\{ (\\boldsymbol{x}_i , y_i) \\}_{i=1}^n$\n",
        "    - $n$ 個の学習データ\n",
        "    - $\\boldsymbol{x}_i \\in \\{ 0,1 \\}^d$：バイナリーベクトル．$\\boldsymbol{x}_i \\sim \\text{Unif} \\left( \\{0,1\\}^d \\right)$\n",
        "    - $y_i = \\left(\\sum_{i}^k x^{(i)} \\right) \\text{mod}  2$ ：最初の $k$  個の数字（clean digits）のパリティ\n",
        "\n",
        "- $\\boldsymbol{x}_i$ の残りの $d-k$ 個の数字（noisy digits）は $y_i$ とは無関係\n",
        "\n",
        "- 例：\n",
        "    - $(3, 30)$-sparse parity dataset\n",
        "        - $\\boldsymbol{x}_1$：$\\color{blue}000\\color{black}110010110001010111001001011$，  $y_1 = 0$\n",
        "        - $\\boldsymbol{x}_2$：$\\color{blue}010\\color{black}110010110001010111001001011$，  $y_2 = 1$\n",
        "            \n",
        "            $\\; \\vdots$\n",
        "            \n",
        "\n",
        "- 学習データをまとめて，$\\mathcal{X} = \\left[ \\boldsymbol{x}_1, \\ldots, \\boldsymbol{x}_n \\right] \\in \\mathbb{R}^{n \\times d}$，$\\mathcal{Y} = \\left[ y_1 \\ldots, y_n \\right] \\in \\mathbb{R}^n$ と行列表記"
      ],
      "metadata": {
        "id": "vnvu-UoTZ-Rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"green\"> $f(\\boldsymbol{x})$ の定義 </font>\n",
        "\n",
        "### ネットワークの構造\n",
        "\n",
        "1. **入力層**: 次元数 $d$ の入力を受け取る．\n",
        "2. **隠れ層**: 層数 $L$ の隠れ層があり，各隠れ層のユニット数は $h$．\n",
        "3. **出力層**: 最終層は出力がスカラー値である 1 次元のベクトルを生成．\n",
        "\n",
        "<br>\n",
        "\n",
        "### 層ごとの計算\n",
        "\n",
        "1. **初期化**:\n",
        "   - 隠れ層 $i$ の重み行列 $W_i$ は，次のように初期化：\n",
        "     \n",
        "     $\n",
        "     W_i \\sim \\mathcal{N}(0, 1)\n",
        "     $\n",
        "\n",
        "     ここで，$W_i$ のサイズは $ h \\times \\text{hh}_{i}$ ．\n",
        "     \n",
        "     $\\text{hh}_{i} $ は前の層の出力ユニット数．\n",
        "\n",
        "   - メモリ効率を考慮し，重み行列を分割：\n",
        "     \n",
        "     \\begin{aligned} W_i =  \\begin{bmatrix}\n",
        "        W_i^{(0)} \\\\\n",
        "        W_i^{(1)} \\\\\n",
        "        \\vdots \\\\\n",
        "        W_i^{(n-1)}\n",
        "        \\end{bmatrix}  \\end{aligned}\n",
        "     \n",
        "     各部分行列 $W_i^{(j)}$ はサイズ $m \\times \\text{hh}_{i}$．ここで，$m$ は分割サイズ．\n",
        "\n",
        "<br>\n",
        "\n",
        "2. **順伝播計算**:\n",
        "   - 入力テンソル $x$ は，初期の隠れ層で次のように変換：\n",
        "     \n",
        "     $\n",
        "     x^{(0)} = x W_0^T / \\sqrt{d}\n",
        "     $\n",
        "\n",
        "     ここで，$W_0$ は最初の隠れ層の重み行列．バイアス項がある場合，次のように加算：\n",
        "     \n",
        "     $\n",
        "     x^{(0)} = x^{(0)} + b_0\n",
        "     $\n",
        "\n",
        "     その後，活性化関数 $ \\sigma $ を適用：\n",
        "     \n",
        "     $\n",
        "     x^{(1)} = \\sigma(x^{(0)})\n",
        "     $\n",
        "\n",
        "   - 次の隠れ層も同様に計算．一般的に，隠れ層 $i$ の計算は次のようになる：\n",
        "     \n",
        "     $\n",
        "     x^{(i)} = x^{(i-1)} W_i^T / \\sqrt{h}\n",
        "     $\n",
        "\n",
        "     ここで，$W_i$ は現在の層の重み行列．バイアス項がある場合，次のように加算：\n",
        "\n",
        "     $\n",
        "     x^{(i)} = x^{(i)} + b_i\n",
        "     $\n",
        "\n",
        "     そして，活性化関数を適用：\n",
        "     \n",
        "     $\n",
        "     x^{(i+1)} = \\sigma(x^{(i)})\n",
        "     $\n",
        "\n",
        "   - 最終層では，次のように計算：\n",
        "     \n",
        "     $\n",
        "     x^{(L)} = x^{(L-1)} W_L^T / h + b_L\n",
        "     $\n",
        "     \n",
        "     ここで，$W_L$ は最終層の重み行列．出力テンソル $x$ を 1 次元に変換して返す：\n",
        "\n",
        "     $\n",
        "     x^{(L)} = x^{(L)} \\text{view}(-1)\n",
        "     $"
      ],
      "metadata": {
        "id": "zTfCKOJKWt-E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jNvFCjVAUR0R"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\"\"\"\n",
        "全結合ネットワーク（Fully Connected Network, FC）のクラスを定義．\n",
        "任意の層数 L を持ち，各層のユニット数は h で指定．\n",
        "活性化関数 act は任意に指定可能で，バイアス項の有無も指定可能．\n",
        "\"\"\"\n",
        "\n",
        "class FC(nn.Module):\n",
        "    def __init__(self, d, h, L, act, bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # ネットワークの初期化\n",
        "        hh = d  # 入力の次元数\n",
        "        for i in range(L):\n",
        "            # 隠れ層の重み行列を正規分布で初期化\n",
        "            W = torch.randn(h, hh)\n",
        "\n",
        "            # メモリ効率を考慮し，重み行列を部分行列に分割して ParameterList に格納\n",
        "            # next two line are here to avoid memory issue when computing the kerne\n",
        "            n = max(1, 128 * 256 // hh)  # 分割サイズを計算\n",
        "            W = nn.ParameterList([nn.Parameter(W[j: j+n]) for j in range(0, len(W), n)])\n",
        "\n",
        "            # 分割した重み行列をレイヤーとして登録\n",
        "            setattr(self, \"W{}\".format(i), W)\n",
        "\n",
        "            # バイアス項が指定されている場合は，それをゼロで初期化して登録\n",
        "            if bias:\n",
        "                self.register_parameter(\"B{}\".format(i), nn.Parameter(torch.zeros(h)))\n",
        "\n",
        "            # 次のレイヤーの入力次元は現在の隠れ層のユニット数になる\n",
        "            hh = h\n",
        "\n",
        "        # 最終層の重み行列を初期化（出力がスカラー値なので次元は (1, h)）\n",
        "        self.register_parameter(\"W{}\".format(L), nn.Parameter(torch.randn(1, hh)))\n",
        "\n",
        "        # バイアス項が指定されている場合は，最終層のバイアスをゼロで初期化\n",
        "        if bias:\n",
        "            self.register_parameter(\"B{}\".format(L), nn.Parameter(torch.zeros(1)))\n",
        "\n",
        "        # クラス変数としてレイヤー数，活性化関数，バイアスの有無を保持\n",
        "        self.L = L\n",
        "        self.act = act\n",
        "        self.bias = bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 順伝播計算\n",
        "        for i in range(self.L + 1):\n",
        "            # i 番目の層の重み行列を取得\n",
        "            W = getattr(self, \"W{}\".format(i))\n",
        "\n",
        "            # ParameterList 形式の重み行列をフルの行列に結合\n",
        "            if isinstance(W, nn.ParameterList):\n",
        "                W = torch.cat(list(W))\n",
        "\n",
        "            # バイアス項が指定されている場合は，バイアスを取得\n",
        "            if self.bias:\n",
        "                B = self.bias * getattr(self, \"B{}\".format(i))\n",
        "            else:\n",
        "                B = 0\n",
        "\n",
        "            # 現在の入力の次元数を取得\n",
        "            h = x.size(1)\n",
        "\n",
        "            if i < self.L:\n",
        "                # 隠れ層での線形変換とスケーリング，そして活性化関数の適用\n",
        "                x = x @ (W.t() / h ** 0.5)  # 重み行列との積（次元スケーリング）\n",
        "                x = self.act(x + B)  # バイアス項を加えた後，活性化関数を適用\n",
        "            else:\n",
        "                # 最終層での線形変換（出力はスカラー値）\n",
        "                x = x @ (W.t() / h) + B  # スカラー出力\n",
        "\n",
        "        # 出力を 1 次元のテンソルに変換して返す\n",
        "        return x.view(-1)"
      ]
    }
  ]
}